<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png"><link rel="icon" href="/img/fluid.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="Zhou1317fe5"><meta name="keywords" content=""><meta name="description" content="吴恩达深度学习笔记 第一门课神经网络和深度学习 第三周浅层神经网络"><meta property="og:type" content="article"><meta property="og:title" content="深度学习-Course1_Week3浅层神经网络"><meta property="og:url" content="https://zhou1317fe5.github.io/2024/02/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course1-Week3%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html"><meta property="og:site_name" content="Zhou1317fe5"><meta property="og:description" content="吴恩达深度学习笔记 第一门课神经网络和深度学习 第三周浅层神经网络"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course1_Week3/3%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_1.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course1_Week3/3%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_2.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course1_Week3/3%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_3.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course1_Week3/3%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_4.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course1_Week3/3%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_5.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course1_Week3/3%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_8.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course1_Week3/3%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_7.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course1_Week3/3%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_9.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course1_Week3/3%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_11.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course1_Week3/3%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_12.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course1_Week3/3%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_10.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course1_Week3/3%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_13.png"><meta property="article:published_time" content="2024-02-07T01:51:16.000Z"><meta property="article:modified_time" content="2024-02-07T02:29:00.158Z"><meta property="article:author" content="Zhou1317fe5"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="深度学习"><meta property="article:tag" content="神经网络"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://zhou1317fe5.github.io/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course1_Week3/3%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_1.png"><meta name="referrer" content="no-referrer-when-downgrade"><title>深度学习-Course1_Week3浅层神经网络 - Zhou1317fe5</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="/css/iconfont_csdn/iconfont.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"zhou1317fe5.github.io",root:"/",version:"1.9.4",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1}},search_path:"/local-search.xml"};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><meta name="generator" content="Hexo 6.3.0"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>Zhou1317fe5</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/Post_banner_img.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="深度学习-Course1_Week3浅层神经网络"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2024-02-07 09:51" pubdate>2024年2月7日 上午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 3.9k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 33 分钟 </span><span id="busuanzi_container_page_pv" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="busuanzi_value_page_pv"></span> 次</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 style="display:none">深度学习-Course1_Week3浅层神经网络</h1><div class="markdown-body"><h1 id="神经网络概述">1 神经网络概述</h1><p>神经网络的结构与逻辑回归类似，只是神经网络的层数比逻辑回归多一层，多出来的中间那层称为隐藏层。 <img src="/img/深度学习-Course1_Week3/3浅层神经网络_1.png" srcset="/img/loading.gif" lazyload></p><p>最右边这一层叫做<strong>输出层</strong>（output layer），因为最后一层输出的是整个神经网络的预测值。最左边一层叫<strong>输入层</strong>（input layer）。在输入层和输出层中间的所有层都叫<strong>隐藏层</strong>，在神经网络中输入层和输出层都有明确的数值，而中间的层并没有告诉确切的数值，是未知的，所以叫隐藏层。</p><p>该神经网络为两层神经网络，一般我们不把输入层纳入神经网络的层数中，除输入层以外的所有层数总数为网络的总层数。</p><h1 id="浅层神经网络">2 浅层神经网络</h1><p>如下图所示，单隐藏层神经网络就是典型的浅层（shallow）神经网络，因为网络层数交少（浅）。</p><p><img src="/img/深度学习-Course1_Week3/3浅层神经网络_2.png" srcset="/img/loading.gif" lazyload> 结构上，从左到右，可以分成三层：输入层（Input layer），隐藏层（Hidden layer）和输出层（Output layer）。</p><h1 id="计算神经网络输出">3 计算神经网络输出</h1><p>接下来我们开始详细推导神经网络的计算过程。</p><p>两层神经网络可以看成是计算完第一层的逻辑回归后再重复计算一次逻辑回归。如下图所示，逻辑回归的正向计算可以分解成计算z和a的两部分：</p><p><img src="/img/深度学习-Course1_Week3/3浅层神经网络_3.png" srcset="/img/loading.gif" lazyload></p><p>对于两层神经网络，从输入层到隐藏层对应一次逻辑回归运算；从隐藏层到输出层对应一次逻辑回归运算。每层计算时，要注意对应的上标和下标，一般我们记上标方括号表示layer，下标表示第几个神经元。例如<span class="math inline">\(a_i^{[l]}\)</span>表示第l层的第i个神经 元。注意，i从1开始，l从0开始。</p><p>下面，我们将从输入层到输出层的计算公式列出来： <span class="math display">\[\begin{gathered} z_{1}^{[1]} =w_1^{[1]T}x+b_1^{[1]},\mathrm{~}a_1^{[1]}=\sigma(z_1^{[1]}) \\ z_2^{[1]} =w_2^{[1]T}x+b_2^{[1]},a_2^{[1]}=\sigma(z_2^{[1]}) \\ z_3^{[1]} =w_3^{[1]T}x+b_3^{[1]},~a_3^{[1]}=\sigma(z_3^{[1]}) \\ z_4^{[1]} =w_4^{[1]T}x+b_4^{[1]},a_4^{[1]}=\sigma(z_4^{[1]}) \end{gathered}\]</span></p><p>然后，从隐藏层到输出层的计算公式为： <span class="math display">\[ z_1^{[2]}=w_1^{[2]T}a^{[1]}+b_1^{[2]},\mathrm{~}a_1^{[2]}=\sigma(z_1^{[2]}) \]</span> 其中<span class="math inline">\(a^{[1]}\)</span>为： <span class="math display">\[a^{[1]}=\begin{bmatrix}a_1^{[1]}\\a_2^{[1]}\\a_3^{[1]}\\a_4^{[1]}\end{bmatrix}\]</span> 上述每个节点的计算都对应着一次逻辑运算的过程，分别由计算z和a两部分组成。</p><p>同样的，为了提高程序运算速度，我们引入向量化和矩阵运算的思想，将上述表达式转换成矩阵运算的形式： <img src="/img/深度学习-Course1_Week3/3浅层神经网络_4.png" srcset="/img/loading.gif" lazyload></p><h1 id="矩阵运算">4 矩阵运算</h1><p>对于m个训练样本的运算，不使用for循环，利用矩阵运算的思想，输入矩阵X的维度为（<span class="math inline">\(n_x\)</span>,m）。这样，我们可以将for循环写成矩阵运算的形式： <img src="/img/深度学习-Course1_Week3/3浅层神经网络_5.png" srcset="/img/loading.gif" lazyload> 其中，<span class="math inline">\(Z^{[1]}\)</span>的维度是(4,m),4是隐藏层神经元的个数；<span class="math inline">\(A^{[1]}\)</span>的维度与<span class="math inline">\(Z^{[1]}\)</span>相同；<span class="math inline">\(Z^{[2]}\)</span>和<span class="math inline">\(A^{[2]}\)</span>的维度均为(1,m)。对上面这四个矩阵来说，均可以这样来理解：行表示神经元个数，列表示样本书目m</p><h1 id="非线性激活函数">5 非线性激活函数</h1><p>原先的四种激活函数都是非线性（non-linear）的。那是否可以使用线性激活函数呢？答案是不行！</p><p>如果使用线性激活函数，使用神经网络与直接使用线性模型的效果并没有什么两样。即便是包含多层隐藏层的神经网络，如果使用线性函数作为激活函数最终的输出仍然是输入X的线性模型。这样的话神经网络就没有任何作用了。因此，隐藏层的激活函数必须要是非线性的。</p><p>另外，如果所有的隐藏层全部使用线性激活函数，只有输出层使用非线性激活函数，那么整个神经网络的结构就类似于一个简单的逻辑回归模型，而失去了神经网络模型本身的优势和价值。</p><p>但是，如果是预测问题而不是分类问题，并且输出y是连续的情况下，输出层的激活函数可以使用线性函数。如果输出y恒为正值，则也可以使用ReLU激活函数，具体情况，具体分析。</p><h1 id="激活函数的导数">6激活函数的导数</h1><ol type="1"><li><strong>sigmoid activation function</strong> <img src="/img/深度学习-Course1_Week3/3浅层神经网络_8.png" srcset="/img/loading.gif" lazyload></li></ol><p><span class="math inline">\(\frac{d}{dz}g(z)=\frac{1}{1+e^{-z}}(1-\frac{1}{1+e^{-z}})=g(z)(1-g(z))\)</span> 当<span class="math inline">\(z=10\)</span>或<span class="math inline">\(z=-10\)</span> ;<span class="math inline">\({\frac{d}{dz}}g(z)\approx0\)</span> 当<span class="math inline">\(z=0\)</span> <span class="math inline">\(\frac{d}{dz}g(z)=g(z)(1-g(z))=1/4\)</span></p><p>在神经网络中<span class="math inline">\(\begin{aligned}a=g(z);g(z)^{&#39;}=\frac{d}{dz}g(z)=a(1-a)\end{aligned}\)</span></p><ol start="2" type="1"><li><strong>Tanh activation function</strong> <img src="/img/深度学习-Course1_Week3/3浅层神经网络_7.png" srcset="/img/loading.gif" lazyload></li></ol><p><span class="math inline">\(g(z)=tanh(z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}\)</span> <span class="math inline">\(\frac d{dz}g(z)=1-(tanh(z))^2\)</span></p><p>当<span class="math inline">\(z=10\)</span>或<span class="math inline">\(z=-10\:\frac{d}{dz}g(z)\approx0\)</span> 当<span class="math inline">\(z=0,\:\frac{d}{dz}g(z)\text{=1-}(0)\text{=1}\)</span></p><ol start="3" type="1"><li><p><strong>Rectified Linear Unit (ReLU)</strong> <img src="/img/深度学习-Course1_Week3/3浅层神经网络_9.png" srcset="/img/loading.gif" lazyload> <span class="math inline">\(g(z)=max(0,z)\)</span> <span class="math display">\[\left.g(z)^{&#39;}=\left\{\begin{array}{ll}0&amp;\text{if z&lt;0}\\1&amp;\text{if z&gt;0}\\undefined&amp;\text{if z=0}\end{array}\right.\right.\]</span> 注：通常在<span class="math inline">\(z= 0\)</span>的时候给定其导数1,0；当然<span class="math inline">\(z=0\)</span>的情况很少很少</p></li><li><p><strong>Leaky linear unit (Leaky ReLU)</strong> <span class="math inline">\(g(z)=\max(0.01z,z)\)</span> <span class="math display">\[\left.g(z)^{&#39;}=\left\{\begin{array}{ll}0.01&amp;\text{if z&lt;0}\\1&amp;\text{if z&gt;0}\\undefined&amp;\text{if z=0}\end{array}\right.\right.\]</span></p></li></ol><h1 id="反向传播">7 反向传播</h1><p>浅层神经网络：</p><ul><li>参数： W<span class="math inline">\(^{[1]},\mathrm{b}^{[1]},\mathrm{W}^{[2]},\mathrm{b}^{[2]};\)</span></li><li>输入层特征向量个数： <span class="math inline">\(\mathrm{n_x= n^{[ 0] }; }\)</span></li><li>隐藏层神经元个数： n<span class="math inline">\(^{[1]}\)</span>,</li><li>输出层神经元个数： n<span class="math inline">\(^{[2]}=1;\)</span></li><li>W<span class="math inline">\(^{[1]}\)</span>的维度为(n<span class="math inline">\(^{[1]},\mathrm{n}^{[0]})\)</span>, b<span class="math inline">\(^{[1]}\)</span>的维度为(n<span class="math inline">\(^{[1]},1);\)</span></li><li>W<span class="math inline">\(^{[2]}\)</span>的维度为(n<span class="math inline">\(^{[2]},\mathrm{n}^{[1]})\)</span>, b<span class="math inline">\(^{[2]}\)</span>的维度为(n<span class="math inline">\(^{[2]},1);\)</span></li></ul><p>我们仍然使用计算图的方式来推导神经网络反向传播过程。 <img src="/img/深度学习-Course1_Week3/3浅层神经网络_11.png" srcset="/img/loading.gif" lazyload> 由于多了一个隐藏层，神经网络的计算图要比逻辑回归的复杂一些。对于单个训练样本，正向过程很容易，反向过程可以根据梯度计算方法逐一推导。 <img src="/img/深度学习-Course1_Week3/3浅层神经网络_12.png" srcset="/img/loading.gif" lazyload></p><p>总结一下，浅层神经网络（包含一个隐藏层），m个训练样本的正向传播过程和反向传播过程分别包含了6个表达式，下图为神经网络反向传播公式（左）和其代码向量化（右）： <img src="/img/深度学习-Course1_Week3/3浅层神经网络_10.png" srcset="/img/loading.gif" lazyload></p><h1 id="随即初始化">8 随即初始化</h1><p>当你训练神经网络时，如果你把权重w都初始化为0，那么梯度下降将不会起作用。</p><p><img src="/img/深度学习-Course1_Week3/3浅层神经网络_13.png" srcset="/img/loading.gif" lazyload></p><p>举个简单的例子，一个浅层神经网络包含两个输 入，隐藏层包含两个神经元。如果权重<span class="math inline">\(W^{[1]}\)</span>和 入 <span class="math inline">\(W^{[2]}\)</span>都初始化为零,即 <span class="math display">\[\begin{aligned}W^{[1]}&amp;=\begin{bmatrix}0&amp;0\\0&amp;0\end{bmatrix}\\W^{[2]}&amp;=\begin{bmatrix}0&amp;0\end{bmatrix}\end{aligned}\]</span></p><p>这样使得隐藏层第一个神经元的输出等于第二个神经元的输出，即<span class="math inline">\(a_1^{[1]}=a_2^{[1]}\)</span>。经过推导得到<span class="math inline">\(dz_1^{[1]}=dz_2^{[1]}\)</span>,以及<span class="math inline">\(dW_1^{[1]}=dW_2^{[1]}\)</span>。因此，这样的结果是隐藏层两个神经元对应的权重行向量<span class="math inline">\(W_1^{[1]}\)</span>和<span class="math inline">\(W_2^{[1]}\)</span>每次迭代更新都会得到完全相同的结果，<span class="math inline">\(W_1^{[1]}\)</span>始终等于<span class="math inline">\(W_2^{[1]}\)</span>,完全对称。这样隐藏层设置多个神经元就没有任何意义了。但是，参数b可以全部初始化为零，并不会影响神经网络训练效果</p><p>在初始化的时候，w参数要进行随机初始化，b则不存在对称性的问题它可以设置为0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">W = np.random.rand((<span class="hljs-number">2</span>,<span class="hljs-number">2</span>))* <span class="hljs-number">0.01</span><br>b = np.zero((<span class="hljs-number">2</span>,<span class="hljs-number">1</span>))<br><br></code></pre></td></tr></table></figure><p>这里我们将W的值乘以0.01是为了尽可能使得权重W初始化为较小的值，这是因为如果使用sigmoid函数或者tanh函数作为激活函数时，W比较小，则<span class="math inline">\(Z = W X + b\)</span>所得的值也比较小，处在0的附近，0点区域的附近梯度较大，能够大大提高算法的更新速度。而如果W设置的太大的话，得到的梯度较小，训练过程因此会变得很慢。</p><p>ReLU和Leaky ReLU作为激活函数时，不存在这种问题，因为在大于0的时候，梯度均为1。</p></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="category-chain-item">深度学习</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">#机器学习</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">#深度学习</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">#神经网络</a></div></div><div class="license-box my-3"><div class="license-title"><div>深度学习-Course1_Week3浅层神经网络</div><div>https://zhou1317fe5.github.io/2024/02/07/深度学习-Course1-Week3浅层神经网络/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>Zhou1317fe5</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2024年2月7日</div></div><div class="license-meta-item"><div>许可协议</div><div><a target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2024/02/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course1-Week4%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="深度学习-Course1_Week4深层神经网络"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">深度学习-Course1_Week4深层神经网络</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2024/02/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course1-Week2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" title="深度学习-Course1_Week2神经网络基础知识"><span class="hidden-mobile">深度学习-Course1_Week2神经网络基础知识</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div><div class="statistics"><span id="busuanzi_container_site_pv" style="display:none">总访问量 <span id="busuanzi_value_site_pv"></span> 次 </span><span id="busuanzi_container_site_uv" style="display:none">总访客数 <span id="busuanzi_value_site_uv"></span> 人</span></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script>window.MathJax?(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise()):window.MathJax={tex:{inlineMath:{"[+]":[["$","$"]]}},loader:{load:["ui/lazy"]},options:{renderActions:{insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(t=>{let e=t.parentNode;"li"===e.nodeName.toLowerCase()&&e.parentNode.classList.add("has-jax")})},"",!1]}}},Fluid.events.registerRefreshCallback((function(){"MathJax"in window&&MathJax.startup.document&&"function"==typeof MathJax.startup.document.state&&(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise())}))</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script src="/js/local-search.js"></script><script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>