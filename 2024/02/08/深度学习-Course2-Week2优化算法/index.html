<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png"><link rel="icon" href="/img/fluid.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="Zhou1317fe5"><meta name="keywords" content=""><meta name="description" content="吴恩达深度学习笔记 第二门课改善深层神经网络：超参数调试、正则化以及优化 第二周优化算法"><meta property="og:type" content="article"><meta property="og:title" content="深度学习-Course2-Week2优化算法"><meta property="og:url" content="https://zhou1317fe5.github.io/2024/02/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week2%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/index.html"><meta property="og:site_name" content="Zhou1317fe5"><meta property="og:description" content="吴恩达深度学习笔记 第二门课改善深层神经网络：超参数调试、正则化以及优化 第二周优化算法"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week2/2%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week2/2%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95_1.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week2/2%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95_2.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week2/2%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95_3.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week2/2%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95_5.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week2/2%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95_6.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week2/2%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95_7.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week2/2%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95_8.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week2/2%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95_13.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week2/2%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95_14.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week2/2%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95_15.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week2/2%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95_11.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week2/2%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95_12.png"><meta property="article:published_time" content="2024-02-08T09:11:00.000Z"><meta property="article:modified_time" content="2024-02-08T09:17:50.336Z"><meta property="article:author" content="Zhou1317fe5"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="深度学习"><meta property="article:tag" content="神经网络"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://zhou1317fe5.github.io/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week2/2%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95.png"><meta name="referrer" content="no-referrer-when-downgrade"><title>深度学习-Course2-Week2优化算法 - Zhou1317fe5</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="/css/iconfont_csdn/iconfont.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"zhou1317fe5.github.io",root:"/",version:"1.9.4",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1}},search_path:"/local-search.xml"};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><meta name="generator" content="Hexo 6.3.0"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>Zhou1317fe5</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/Post_banner_img.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="深度学习-Course2-Week2优化算法"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2024-02-08 17:11" pubdate>2024年2月8日 下午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 7.6k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 64 分钟 </span><span id="busuanzi_container_page_pv" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="busuanzi_value_page_pv"></span> 次</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 style="display:none">深度学习-Course2-Week2优化算法</h1><div class="markdown-body"><h1 id="mini-batch梯度下降">1 Mini-batch梯度下降</h1><p>之前我们介绍的神经网络训练过程是对所有m个样本，称为batch，通过向量化计算方式，同时进行的。</p><p>如果m很大，例如达到百万数量级，训练速度往往会很慢，因为每次迭代都要对所有样本进行求和运算和矩阵运算。我们将这种梯度下降算法称为Batch Gradient Descent。</p><p>为了解决这一问题，我们可以把m个训练样本分成若干个子集，称为mini-batches，这样每个子集包含的数据量就小了，例如只有1000，然后每次在单一子集上进行神经网络训练，速度就会大大提高。这种梯度下降算法叫做Mini-batch Gradient Descent。</p><p>假设总的训练样本个数m=5000000,其维度为<span class="math inline">\((n_x,m)\)</span>。将其分成5000个子集，每个mini-batch含有1000个样本。我们将每个mini-batch记为<span class="math inline">\(X^{\{t\}}\)</span>,其维度为<span class="math inline">\((n_x,1000)\)</span>。相应的每个mini-batch的输出记为<span class="math inline">\(Y^{\{t\}}\)</span>,其 维 度 为 (1,1000)，且<span class="math inline">\(t=1,2,\cdots,5000\)</span>。</p><p>这里顺便总结一下我们遇到的神经网络中几类字母的上标含义：</p><p><span class="math inline">\(X^{(i)}:\)</span> 第i个样本 <span class="math inline">\(Z^{[l]}:\)</span> 神经网络第<span class="math inline">\(l\)</span>层网络的线性输出 <span class="math inline">\(X^{\{t\}},Y^{\{t\}}:\)</span> 第t组mini-batch</p><p>Mini-batches Gradient Descent的实现过程是先将总的训练样本分成T个子集（mini-batches），然后对每个mini-batch进行神经网络训练，包括向前传播，计算Cost Function，反向传播，循环至T个mini-batch都训练完毕。</p><figure class="highlight excel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs excel">for  <span class="hljs-built_in">t</span>=<span class="hljs-number">1</span>,⋯,<span class="hljs-built_in">T</span> &#123;<br>    Forward Propagation<br>    ComputeCostFunction<br>    BackwardPropagation<br>    <span class="hljs-symbol">W:</span>=W−α⋅dW<br>    <span class="hljs-symbol">b:</span>=b−α⋅<span class="hljs-built_in">db</span><br>&#125;<br></code></pre></td></tr></table></figure><p>经过T次循环之后，所有m个训练样本都进行了梯度下降计算。这个过程，我们称之为经历了一个epoch。</p><p>对于Batch Gradient Descent而言，一个epoch只进行一次梯度下降算法；而Mini-Batches Gradient Descent，一个epoch会进行T次梯度下降算法。</p><p>值得一提的是，对于Mini-Batches Gradient Descent，可以进行多次epoch训练。而且，每次epoch，最好是将总体训练数据重新打乱、重新分成T组mini-batches，这样有利于训练出最佳的神经网络模型。</p><p>Batch gradient descent和Mini-batch gradient descent的cost曲线如下图所示：</p><p><img src="/img/深度学习-Course2-Week2/2优化算法.png" srcset="/img/loading.gif" lazyload></p><p>对于一般的神经网络模型，使用Batch gradient descent，随着迭代次数增加，cost是不断减小的。然而，使用Mini-batch gradient descent，随着在不同的mini-batch上迭代训练，其cost不是单调下降，而是受类似noise的影响，出现振荡。但整体的趋势是下降的，最终也能得到较低的cost值。</p><p>之所以出现细微振荡的原因是不同的mini-batch之间是有差异的。例如可能第一个子集<span class="math inline">\((X^{\{1\}},Y^{\{1\}})\)</span>是好的子集，而第二个子集<span class="math inline">\((X^{\{2\}},Y^{\{2\}})\)</span>包含了一些噪声noise。出现细微振荡是正常的。</p><p>如何选择每个mini-batch的大小，即包含的样本个数呢？有两个极端：如果mini-batch size=m ,即为Batch gradient descent ,只包含一个子集 为<span class="math inline">\((X^{\{1\}},Y^{\{1\}})=(X,Y)\)</span> ;如果mini-batch size=1 , 即为Stachastic gradient descent,每个样本就是一个子集<span class="math inline">\((X^{\{1\}},Y^{\{1\}})=(x^{(i)},y^{(i)})\)</span> ,共有m个子集。</p><p>我们来比较一下Batch gradient descent和Stachastic gradient descent的梯度下降曲线。 <img src="/img/深度学习-Course2-Week2/2优化算法_1.png" srcset="/img/loading.gif" lazyload></p><p>如图所示，蓝色的线代表Batch gradient descent，紫色的线代表Stachastic gradient descent。</p><p>Batch gradient descent会比较平稳地接近全局最小值，但是因为使用了所有m个样本，每次前进的速度有些慢。</p><p>Stachastic gradient descent每次前进速度很快，但是路线曲折，有较大的振荡，最终会在最小值附近来回波动，难以真正达到最小值处。而且在数值处理上就不能使用向量化的方法来提高运算速度。</p><p>实际使用中，mini-batch size不能设置得太大（Batch gradient descent），也不能设置得太小（Stachastic gradient descent）。</p><p>这样，相当于结合了Batch gradient descent和Stachastic gradient descent各自的优点，既能使用向量化优化算法，又能较快速地找到最小值。</p><p>mini-batch gradient descent的梯度下降曲线如下图绿色所示，每次前进速度较快，且振荡较小，基本能接近全局最小值。 <img src="/img/深度学习-Course2-Week2/2优化算法_2.png" srcset="/img/loading.gif" lazyload> 一般来说，如果总体样本数量m不太大时，例如m≤2000m≤2000，建议直接使用Batch gradient descent。</p><p>如果总体样本数量m很大时，建议将样本分成许多mini-batches。推荐常用的mini-batch size为64,128,256,512。这些都是2的幂。</p><p>之所以这样设置的原因是计算机存储数据一般是2的幂，这样设置可以提高运算速度。</p><h1 id="指数加权平均">2 指数加权平均</h1><h2 id="指数加权平均概念">指数加权平均概念</h2><p>该部分我们将介绍指数加权平均（Exponentially weighted averages）的概念。</p><p>我们记录半年内伦敦市的气温变化，并在二维平面上绘制出来，如下图所示：</p><p><img src="/img/深度学习-Course2-Week2/2优化算法_3.png" srcset="/img/loading.gif" lazyload> 如果我们希望看到半年内气温的整体变化趋势，可以通过移动平均（moving average）的方法来对每天气温进行平滑处</p><p>例如我们可以设V0=0，当成第0天的气温值。</p><p>第一天的气温与第0天的气温有关： <span class="math display">\[V_1=0.9V_0+0.1\theta_1\]</span></p><p>第二天的气温与第一天的气温有关： <span class="math display">\[V_2=0.9V_1+0.1\theta_2\]</span></p><p>第三天的气温与第二天的气温有关： <span class="math display">\[V_3=0.9V_2+0.1\theta_3\]</span></p><p>即第t天与第t-1天的气温迭代关系为： <span class="math display">\[V_t=0.9V_{t-1}+0.1\theta_t\]</span></p><p>经过移动平均处理得到的气温如下图红色曲线所示： <img src="/img/深度学习-Course2-Week2/2优化算法_5.png" srcset="/img/loading.gif" lazyload> 这种滑动平均算法称为指数加权平均（exponentially weighted average）。根据之前的推导公式，其一般形式为： <span class="math display">\[V_t=\beta V_{t-1}+(1-\beta)\theta_t\]</span></p><p>上面的例子中，β=0.9。β值决定了指数加权平均的天数，近似表示为<span class="math inline">\(\frac1{1-\beta}\)</span></p><p>例如，当<span class="math inline">\(\beta=0.9\)</span>,则<span class="math inline">\(\frac{1}{1-\beta}=10\)</span>,表示将前10天进行指数加权平均。当<span class="math inline">\(\beta=0.98\)</span>,则<span class="math inline">\(\frac{1}{1-\beta}=50\)</span>,表示将前50天进行指数加权平均。<span class="math inline">\(\beta\)</span>值越大，则指数加权平均的天数越多，平均后的趋势线就越平缓，但是同时也会向右平移。下图绿色曲线和黄色曲线分别表示了<span class="math inline">\(\beta=0.98\)</span>和<span class="math inline">\(\beta=0.5\)</span>时，指数加权平均的结果。 <img src="/img/深度学习-Course2-Week2/2优化算法_6.png" srcset="/img/loading.gif" lazyload></p><p>这里简单解释一下公式1/(1−β)是怎么来的。准确来说，指数加权平均算法跟之前所有天的数值都有关系，根据之前的推导公式就能看出。</p><p>但是指数是衰减的，一般认为衰减到1/e就可以忽略不计了。因此，根据之前的推导公式，我们只要证明下式就好了： <span class="math display">\[ \beta^{\frac1{1-\beta}}=\frac1e \]</span> 令<span class="math inline">\(\frac{1}{1-\beta}=N\)</span> ,<span class="math inline">\(N&gt;0\)</span> ,则<span class="math inline">\(\beta=1-\frac{1}{N}\)</span> ,<span class="math inline">\(\frac{1}{N}&lt;1\)</span>。即证明转化为： <span class="math display">\[(1-\frac1N)^N=\frac1e\]</span></p><p>显然，当N&gt;&gt;0时，上述等式是近似成立的。</p><p>至此，简单解释了为什么指数加权平均的天数的计算公式为1/(1−β)。</p><h2 id="偏差修正">偏差修正</h2><p>上文中提到当β=0.98时，指数加权平均结果如下图绿色曲线所示。但是实际上，真实曲线如紫色曲线所示。 <img src="/img/深度学习-Course2-Week2/2优化算法_7.png" srcset="/img/loading.gif" lazyload> 我们注意到，紫色曲线与绿色曲线的区别是，紫色曲线开始的时候相对较低一些。这是因为开始时我们设置V0=0，所以初始值会相对小一些，直到后面受前面的影响渐渐变小，趋于正常。</p><p>修正这种问题的方法是进行偏移校正（bias correction），即在每次计算完Vt后，对Vt进行下式处理：<span class="math inline">\(\frac{V_t}{1-\beta^t}\)</span></p><p>在刚开始的时候，t比较小，<span class="math inline">\((1-\beta^t)&lt;1\)</span>,这样就将<span class="math inline">\(V_t\)</span>修正得更大一些，效果是把紫色曲线开始部分向上提升一些，与绿色曲线接近重合。随着t增大<span class="math inline">\((1-\beta^t)\approx1\)</span>，<span class="math inline">\(V_t\)</span>基本不变，紫色曲线与绿色曲线依然重合。这样就实现了简单的偏移校正，得到我们希望的绿色曲线。</p><p>值得一提的是，机器学习中，偏移校正并不是必须的。因为，在迭代一次次后（t较大），Vt受初始值影响微乎其微，紫色曲线与绿色曲线基本重合。所以，一般可以忽略初始迭代过程，等到一定迭代之后再取值，这样就不需要进行偏移校正了。</p><h1 id="动量梯度下降算法">3 动量梯度下降算法</h1><p>该部分将介绍动量梯度下降算法，其速度要比传统的梯度下降算法快很多。做法是在每次训练时，对梯度进行指数加权平均处理，然后用得到的梯度值更新权重W和常数项b。下面介绍具体的实现过程。 <img src="/img/深度学习-Course2-Week2/2优化算法_8.png" srcset="/img/loading.gif" lazyload> 原始的梯度下降算法如上图蓝色折线所示。在梯度下降过程中，梯度下降的振荡较大，尤其对于W、b之间数值范围差别较大的情况。此时每一点处的梯度只与当前方向有关，产生类似折线的效果，前进缓慢。</p><p>而如果对梯度进行指数加权平均，这样使当前梯度不仅与当前方向有关，还与之前的方向有关，这样处理让梯度前进方向更加平滑，减少振荡，能够更快地到达最小值处。</p><p>这是因为之前的梯度方向可以保证是大致指向极小值方向的，而当前的可能是也可能不是，因此如果是，则用之前的梯度加强其收敛速度，否则用之前的去抵消以减弱收敛速度，这种方法的问题即容易积累动量从而越过极小值。</p><p>权重W和常数项b的指数加权平均表达式如下： <span class="math display">\[\begin{aligned}V_{dW}&amp;=\beta\cdot V_{dW}+(1-\beta)\cdot dW\\V_{db}&amp;=\beta\cdot V_{db}+(1-\beta)\cdot db\end{aligned}\]</span> 初始时，令Vdw=0,Vdb=0。一般设置β=0.9，即指数加权平均前10天的数据，实际应用效果较好。</p><p>另外，关于偏移校正，可以不使用。因为经过10次迭代后，随着滑动平均的过程，偏移情况会逐渐消失。</p><h1 id="rmsprop">4 RMSprop</h1><p>RMSprop是另外一种优化梯度下降速度的算法。每次迭代训练过程中，其权重W和常数项b的更新表达式为： <span class="math display">\[\begin{gathered} S_W=\beta S_{dW}+(1-\beta)dW^2 \\ S_b=\beta S_{db}+(1-\beta)db^2 \\ W:=W-\alpha\frac{dW}{\sqrt{S_{W}}},b:=b-\alpha\frac{db}{\sqrt{S_{b}}} \end{gathered}\]</span></p><p>下面简单解释一下RMSprop算法的原理，仍然以下图为例，为了便于分析，令水平方向为W的方向，垂直方向为b的方向。 <img src="/img/深度学习-Course2-Week2/2优化算法_13.png" srcset="/img/loading.gif" lazyload></p><p>从图中可以看出，梯度下降(蓝色折线)在垂直方向 (b)上振荡较大，在水平方向(W)上振荡较小，表示在b方向上梯度较大，即<span class="math inline">\(db\)</span>较大，而在W方向上梯度较小，即<span class="math inline">\(dW\)</span>较小。因此，上述表达式中<span class="math inline">\(S_{b}\)</span>较大，而<span class="math inline">\(S_W\)</span>较小。在更新W和b的表达式中，变化值 <span class="math inline">\(\frac{dW}{\sqrt{Sw}}\)</span>较大，而<span class="math inline">\(\frac{db}{\sqrt{S_b}}\)</span>较小。也就使得W变化得多一些，b变化得少一些。即加快了W方向的速度，减小了b方向的速度，减小振荡，实现快速梯度下降算法，其梯度下降过程如绿色折线所示。总得来说，就是如果哪个方向振荡大，就减小该方向的更新速度， 从而减小振荡。</p><p>还有一点需要注意的是为了避免RMSprop算法中分母为零，通常可以在分母增加一个极小的常数ε： <span class="math display">\[W:=W-\alpha\frac{dW}{\sqrt{S_W}+\varepsilon},\mathrm{~}b:=b-\alpha\frac{db}{\sqrt{S_b}+\varepsilon}\]</span> 其中，ε=10^−8，或者其它较小值。</p><h1 id="adam优化算法">5 Adam优化算法</h1><p>Adam（Adaptive Moment Estimation）算法结合了动量梯度下降算法和RMSprop算法。其算法流程为： <span class="math display">\[\begin{aligned} &amp;V_{dW}=0,S_{dW},V_{db}=0,S_{db}=0 \\ &amp;\text{On iteration t:} \\ &amp;&amp;&amp;\textit{Cimpute d}W,db \\ &amp;&amp;&amp;\begin{aligned}V_{dW}=\beta_1V_{dW}+(1-\beta_1)dW,V_{db}=\beta_1V_{db}+(1-\beta_1)db\end{aligned} \\ &amp;&amp;&amp;\begin{aligned}S_{dW}=\beta_2S_{dW}+(1-\beta_2)dW^2,S_{db}=\beta_2S_{db}+(1-\beta_2)db^2\end{aligned} \\ &amp;&amp;&amp;V_{dW}^{corrected}=\frac{V_{dW}}{1-\beta_1^t},V_{db}^{corrected}=\frac{V_{db}}{1-\beta_1^t} \\ &amp;&amp;&amp;S_{dW}^{corrected}=\frac{S_{dW}}{1-\beta_2^t},S_{db}^{corrected}=\frac{S_{db}}{1-\beta_2^t} \\ &amp;&amp;&amp;W:=W-\alpha\frac{V_{dW}^{corrected}}{\sqrt{S_{dW}^{corrected}}+\varepsilon},b:=b-\alpha\frac{V_{db}^{corrected}}{\sqrt{S_{db}^{corrected}}+\varepsilon} \end{aligned}\]</span></p><p>Adam 算法包含了几个超参数，分别是：<span class="math inline">\(\alpha,\beta_1,\beta_2,\varepsilon\)</span>。其中，<span class="math inline">\(\beta_1\)</span>通常设置为0.9，<span class="math inline">\(\beta_2\)</span>通常设置为0.999,<span class="math inline">\(\varepsilon\)</span>通常设置为<span class="math inline">\(10^{-8}\)</span>。一般只需要对<span class="math inline">\(\beta_{1}\)</span>和<span class="math inline">\(\beta_{2}\)</span>进行调试。</p><p>实际应用中，Adam算法结合了动量梯度下降和RMSprop各自的优点，使得神经网络训练速度大大提高。</p><h1 id="学习率衰减">6 学习率衰减</h1><p>减小学习因子α也能有效提高神经网络训练速度，这种方法被称为learning rate decay。</p><p>学习率衰减就是随着迭代次数增加，学习因子α逐渐减小。</p><p>下面用图示的方式来解释这样做的好处。下图中，蓝色折线表示使用恒定的学习因子α，由于每次训练α相同，步进长度不变，在接近最优值处的振荡也大，在最优值附近较大范围内振荡，与最优值距离就比较远。 <img src="/img/深度学习-Course2-Week2/2优化算法_14.png" srcset="/img/loading.gif" lazyload> 绿色折线表示使用不断减小的α，随着训练次数增加，α逐渐减小，步进长度减小，使得能够在最优值处较小范围内微弱振荡，不断逼近最优值。相比较恒定的α来说，learning rate decay更接近最优值。</p><p>Learning rate decay中对α可由下列公式得到： <span class="math display">\[\alpha=\frac1{1+\textit{deca}y\_rate*epoch}\alpha_0\]</span> 其中，deacy_rate是参数（可调），epoch是训练完所有样本的次数。随着epoch增加，α会不断变小。</p><p>除了上面计算α的公式之外，还有其它可供选择的计算公式： <span class="math display">\[\begin{gathered}\alpha=0.95^{epoch}\cdot\alpha_0\\\alpha=\frac k{\sqrt{epoch}}\cdot\alpha_0\quad or\quad\frac k{\sqrt{t}}\cdot\alpha_0\end{gathered}\]</span></p><p>其中，k为可调参数，t为mini-bach number。</p><p>除此之外，还可以设置α为关于t的离散值，随着t增加，α呈阶梯式减小。 <img src="/img/深度学习-Course2-Week2/2优化算法_15.png" srcset="/img/loading.gif" lazyload></p><p>也可以根据训练情况手动调整当前的α值，但会比较耗时间。</p><h1 id="局部最优问题">7 局部最优问题</h1><p>在使用梯度下降算法不断减小cost function时，可能会得到局部最优解（local optima）而不是全局最优解（global optima）。</p><p>之前我们对局部最优解的理解是形如碗状的凹槽，如下图左边所示。但是在神经网络中，局部最优解的概念发生了变化。 <img src="/img/深度学习-Course2-Week2/2优化算法_11.png" srcset="/img/loading.gif" lazyload> 准确地来说，大部分梯度为零的“最优点”并不是这些凹槽处，而是形如右边所示的马鞍状，称为saddle point。</p><p>也就是说，梯度为零并不能保证都是convex（极小值），也有可能是concave（极大值）。</p><p>特别是在神经网络中参数很多的情况下，所有参数梯度为零的点很可能都是右边所示的马鞍状的saddle point，而不是左边那样的local optimum。</p><p>类似马鞍状的plateaus会降低神经网络学习速度。Plateaus是梯度接近于零的平缓区域，如下图所示。在plateaus上梯度很小，前进缓慢，到达saddle point需要很长时间。</p><p>到达saddle point后，由于随机扰动，梯度一般能够沿着图中绿色箭头，离开saddle point，继续前进，只是在plateaus上花费了太多时间。 <img src="/img/深度学习-Course2-Week2/2优化算法_12.png" srcset="/img/loading.gif" lazyload></p><p>总的来说，关于local optima，有两点总结：</p><ul><li><p>只要选择合理的强大的神经网络，一般不太可能陷入local optima</p></li><li><p>Plateaus可能会使梯度下降变慢，降低学习速度</p></li></ul><p>值得一提的是，上文介绍的动量梯度下降，RMSprop，Adam算法都能有效解决plateaus下降过慢的问题，大大提高神经网络的学习速度。</p></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="category-chain-item">深度学习</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">#机器学习</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">#深度学习</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">#神经网络</a></div></div><div class="license-box my-3"><div class="license-title"><div>深度学习-Course2-Week2优化算法</div><div>https://zhou1317fe5.github.io/2024/02/08/深度学习-Course2-Week2优化算法/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>Zhou1317fe5</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2024年2月8日</div></div><div class="license-meta-item"><div>许可协议</div><div><a target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2024/02/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week3%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95-Batch%E6%AD%A3%E5%88%99%E5%8C%96/" title="深度学习-Course2-Week3超参数调试 Batch正则化"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">深度学习-Course2-Week3超参数调试 Batch正则化</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2024/02/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week1%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2/" title="深度学习-Course2-Week1深度学习实用层面"><span class="hidden-mobile">深度学习-Course2-Week1深度学习实用层面</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div><div class="statistics"><span id="busuanzi_container_site_pv" style="display:none">总访问量 <span id="busuanzi_value_site_pv"></span> 次 </span><span id="busuanzi_container_site_uv" style="display:none">总访客数 <span id="busuanzi_value_site_uv"></span> 人</span></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script>window.MathJax?(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise()):window.MathJax={tex:{inlineMath:{"[+]":[["$","$"]]}},loader:{load:["ui/lazy"]},options:{renderActions:{insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(t=>{let e=t.parentNode;"li"===e.nodeName.toLowerCase()&&e.parentNode.classList.add("has-jax")})},"",!1]}}},Fluid.events.registerRefreshCallback((function(){"MathJax"in window&&MathJax.startup.document&&"function"==typeof MathJax.startup.document.state&&(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise())}))</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script src="/js/local-search.js"></script><script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>