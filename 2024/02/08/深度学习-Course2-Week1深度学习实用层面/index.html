<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png"><link rel="icon" href="/img/fluid.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="Zhou1317fe5"><meta name="keywords" content=""><meta name="description" content="吴恩达深度学习笔记 第二门课改善深层神经网络：超参数调试、正则化以及优化 第一周深度学习实用层面"><meta property="og:type" content="article"><meta property="og:title" content="深度学习-Course2-Week1深度学习实用层面"><meta property="og:url" content="http://zhou1317fe5.link/2024/02/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week1%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2/index.html"><meta property="og:site_name" content="Zhou1317fe5"><meta property="og:description" content="吴恩达深度学习笔记 第二门课改善深层神经网络：超参数调试、正则化以及优化 第一周深度学习实用层面"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://zhou1317fe5.link/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week1/1%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2.png"><meta property="og:image" content="http://zhou1317fe5.link/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week1/1%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2_1.png"><meta property="og:image" content="http://zhou1317fe5.link/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week1/1%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2_2.png"><meta property="og:image" content="http://zhou1317fe5.link/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week1/1%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2_3.png"><meta property="og:image" content="http://zhou1317fe5.link/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week1/1%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2_4.png"><meta property="og:image" content="http://zhou1317fe5.link/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week1/1%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2_6.png"><meta property="og:image" content="http://zhou1317fe5.link/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week1/1%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2_7.png"><meta property="og:image" content="http://zhou1317fe5.link/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week1/1%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2_8.png"><meta property="og:image" content="http://zhou1317fe5.link/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week1/1%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2_9.png"><meta property="og:image" content="http://zhou1317fe5.link/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week1/1%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2_10.png"><meta property="og:image" content="http://zhou1317fe5.link/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week1/1%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2_11.png"><meta property="og:image" content="http://zhou1317fe5.link/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week1/1%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2_12.png"><meta property="og:image" content="http://zhou1317fe5.link/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week1/1%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2_13.png"><meta property="og:image" content="http://zhou1317fe5.link/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week1/1%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2_14.png"><meta property="article:published_time" content="2024-02-08T09:10:44.000Z"><meta property="article:modified_time" content="2024-02-08T09:15:05.110Z"><meta property="article:author" content="Zhou1317fe5"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="深度学习"><meta property="article:tag" content="神经网络"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="http://zhou1317fe5.link/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week1/1%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2.png"><meta name="referrer" content="no-referrer-when-downgrade"><title>深度学习-Course2-Week1深度学习实用层面 - Zhou1317fe5</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="/css/iconfont_csdn/iconfont.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"zhou1317fe5.link",root:"/",version:"1.9.4",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1}},search_path:"/local-search.xml"};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><meta name="generator" content="Hexo 6.3.0"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>Zhou1317fe5</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/Post_banner_img.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="深度学习-Course2-Week1深度学习实用层面"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2024-02-08 17:10" pubdate>2024年2月8日 下午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 11k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 92 分钟 </span><span id="busuanzi_container_page_pv" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="busuanzi_value_page_pv"></span> 次</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 style="display:none">深度学习-Course2-Week1深度学习实用层面</h1><div class="markdown-body"><h1 id="训练验证测试集train-dev-test-sets">1.1 训练，验证，测试集（Train / Dev / Test sets）</h1><h2 id="数据集划分">数据集划分</h2><p>一般我们将所有的样本数据分成三个部分：Train/Dev/Test sets。</p><ul><li>训练集（Train sets）：用于训练深度学习模型。</li><li>验证集（Dev sets）：用于验证不同算法的表现，并选择最佳的算法模型。</li><li>测试集（Test sets）：用于测试最佳算法的实际表现，提供无偏估计。</li></ul><h2 id="数据集比例设定">数据集比例设定</h2><ul><li><p>通常设置Train sets和Test sets的数量比例为70%和30%。如果有Dev sets，则设置比例为60%、20%、20%，分别对应Train/Dev/Test sets。</p></li><li><p>对于大数据样本，可将 Dev sets 和 Test sets 的比例设置得更低，如 1% 或 0.5%。例如，对于 100 万样本，可以采用 98%/1%/1% 或 99%/0.5%/0.5% 的比例分配。</p></li></ul><h2 id="超参数优化">超参数优化</h2><ul><li>构建神经网络时需设置多个超参数，如层数、每个隐藏层神经元个数、学习速率、激活函数等。</li><li>通过反复迭代更新来获得最佳参数值。循环迭代过程包括：提出想法（Idea）、选择初始参数值、构建神经网络模型结构、通过代码实现神经网络、验证参数对应的神经网络性能。</li><li>通过多次循环调整参数并选择最佳参数值来优化神经网络性能。</li></ul><p><img src="/img/深度学习-Course2-Week1/1深度学习的实用层面.png" srcset="/img/loading.gif" lazyload></p><h2 id="训练效率提升">训练效率提升</h2><ul><li>应用深度学习是一个反复迭代的过程，需要通过多次循环训练得到最优化参数。</li><li>循环训练的关键是单次循环所需时间，单次循环越快，训练过程越快。合适的 Train/Dev/Test sets 数量能有效提高训练效率。</li></ul><h1 id="偏差方差bias-variance">1.2 偏差，方差（Bias /Variance）</h1><h2 id="偏差和方差的概念"><strong>偏差和方差的概念</strong></h2><p>在传统的机器学习算法中，偏差(Bias)和方差(Variance)是对立的，分别对应着欠拟合和过拟合，我们常常需要在Bias和Variance之间进行权衡。</p><p>而在深度学习中，我们可以同时减小Bias和Variance，构建最佳神经网络模型。</p><h2 id="偏差和方差的权衡"><strong>偏差和方差的权衡</strong></h2><p><img src="/img/深度学习-Course2-Week1/1深度学习的实用层面_1.png" srcset="/img/loading.gif" lazyload></p><ul><li>针对数据集，当使用简单模型（如逻辑回归）无法很好地拟合数据时，出现高偏差（high bias），即“欠拟合”（underfitting）。</li><li>相反，当使用过于复杂的模型（如深度神经网络）能够完美拟合数据，但泛化能力较差时，出现高方差（high variance），即“过拟合”（overfitting）。</li><li>介于过度拟合和欠拟合之间，存在一些适度拟合的模型，这种数据拟合看起来更加合理。</li></ul><p>对于二维数据集，可以通过绘制数据和可视化分割边界来观察偏差和方差的情况。在多维空间数据中，虽无法直接绘制数据，但可以通过训练集误差和验证集误差研究偏差和方差。</p><h2 id="训练集误差和验证集误差"><strong>训练集误差和验证集误差</strong></h2><ul><li>关键数据包括训练集误差（Train set error）和验证集误差（Dev set error）。</li><li>通过比较训练集误差和验证集误差，可以诊断算法是否具有高方差或高偏差。</li></ul><h2 id="诊断算法的高偏差和高方差"><strong>诊断算法的高偏差和高方差</strong></h2><p>我们以识别猫狗为例，人类都能正确识别所有猫类图片 即base error为0。base error不同，相应的Train set error和Dev set error会有所变化，但没有相对变化。，对于此种情况：</p><p><img src="/img/深度学习-Course2-Week1/1深度学习的实用层面_2.png" srcset="/img/loading.gif" lazyload></p><ul><li><p><strong>高方差</strong>：假设Train set error为1%，而Dev set error为11%，即该算法模型对训练样本的识别很好，但是对验证集的识别却不太好。这说明了该模型对训练样本可能存在过拟合，模型泛化能力不强，导致验证集识别率低。这恰恰是high variance的表现。<strong>训练集误差较低，但验证集误差较高，导致过度拟合。</strong></p></li><li><p><strong>高偏差</strong>：假设Train set error为15%，而Dev set error为16%，虽然二者error接近，即该算法模型对训练样本和验证集的识别都不是太好。这说明了该模型对训练样本存在欠拟合。这恰恰是high bias的表现。<strong>训练集误差和验证集误差都较高，导致欠拟合。</strong></p></li><li><p><strong>高偏差&amp;高方差</strong>： 假设Train set error为15%，而Dev set error为30%，说明了该模型既存在high bias也存在high variance（深度学习中最坏的情况）。</p></li><li><p><strong>低偏差&amp;低方差</strong>： 再假设Train set error为0.5%，而Dev set error为1%，即low bias和low variance，是最好的情况。</p></li></ul><h2 id="高偏差高方差的情况"><strong>高偏差&amp;高方差的情况</strong></h2><p>一般来说，Train set error体现了是否出现bias，Dev set error体现了是否出现variance（正确地说，应该是Dev set error与Train set error的相对差值）。</p><p>我们已经通过二维平面展示了high bias或者high variance的模型，下图展示了high bias and high variance的模型： <img src="/img/深度学习-Course2-Week1/1深度学习的实用层面_3.png" srcset="/img/loading.gif" lazyload> 模型既存在high bias也存在high variance，可以理解成某段区域是欠拟合的，某段区域是过拟合的。</p><h2 id="如何减小high-bias和high-variance">如何减小high bias和high variance</h2><ul><li><p>减少high bias ：增加神经网络的隐藏层个数、神经元个数，训练时间延长，选择其它更复杂的NN模型等。</p></li><li><p>减少high variance ：增加训练样本数据，进行正则化Regularization，选择其他更复杂的NN模型等</p></li></ul><h1 id="机器学习基础basic-recipe-for-machine-learning">1.3 机器学习基础（Basic Recipe for Machine Learning）</h1><p><img src="/img/深度学习-Course2-Week1/1深度学习的实用层面_4.png" srcset="/img/loading.gif" lazyload></p><p>在训练神经网络时，我们需要考虑偏差（bias）和方差（variance）的问题。初始模型训练完成后，我们首先要评估算法的偏差水平。如果偏差较高，意味着模型无法很好地拟合训练集数据，可以选择一个新的网络架构，例如增加更多的隐藏层或隐藏单元，这样通常可以提升模型的拟合能力。另外，可以花费更多时间来训练网络，或者尝试使用更先进的优化算法。</p><p>一旦偏差降低到可接受的水平，我们需要检查方差的问题。如果方差较高，解决方案之一是增加更多的数据。然而，有时候我们无法获得更多的数据，这时我们可以尝试使用正则化方法来减少过拟合。</p><p>有时候，我们不得不反复尝试不同的方法。但是，如果能找到更合适的神经网络架构，有时候它可以同时减少偏差和方差。</p><h1 id="正则化regularization">1.4 正则化（Regularization）</h1><p>如果出现了过拟合，可以通过扩大训练样本数量来减小方差，但是通常获得更多训练样本的成本太高，比较困难。所以，更可行有效的办法就是使用正则化（regularization）来解决。</p><p>常见正则化类型有L1正则化和L2正则化，其中L2正则化是最常用的。</p><h2 id="l2-regularization">L2 regularization</h2><p>L2正则化采用矩阵范数的平方来衡量模型复杂度，将矩阵中所有元素的平方和作为正则化项，并乘以正则化参数λ和系数1/2m，加入到损失函数中，从而减小参数的值。将之前的Logistic regression采用L2 regularization 为： <span class="math display">\[\begin{gathered}J(w,b)=\frac1m\sum_{i=1}^mL(\hat{y}^{(i)},y^{(i)})+\frac\lambda{2m}||w||_2^2\\||w||_2^2=\sum_{j=1}^{n_x}w_j^2=w^Tw\end{gathered}\]</span></p><p>在神经网络中应用L2正则化： <span class="math display">\[\begin{gathered}\begin{aligned}J(w^{[1]},b^{[1]},\cdots,w^{[L]},b^{[L]})&amp;=\frac1m\sum_{i=1}^mL(\hat{y}^{(i)},y^{(i)})+\frac\lambda{2m}\sum_{l=1}^L||w^{[l]}||^2\\\\||w^{[l]}||^2&amp;=\sum_{i=1}^{n^{[l]}}\sum_{j=1}^{n^{[l-1]}}(w_{ij}^{[l]})^2\end{aligned}\end{gathered}\]</span></p><p>神经网络含有一个成本函数，该函数包含所有参数，其中正则项为<span class="math inline">\(\frac\lambda{2m}\sum_{l=1}^L||w^{[l]}||^2\)</span>，其中<span class="math inline">\(L\)</span>是神经网络所含的层数。</p><p>由于加入了正则化项，梯度下降算法中的计算表达式需要做如下修改： <span class="math display">\[\begin{gathered}dw^{[l]}=dw_{before}^{[l]}+\frac\lambda mw^{[l]}\\\\w^{[l]}:=w^{[l]}-\alpha\cdot dw^{[l]}\end{gathered}\]</span> L2正则化也被称为“权重衰减(weight decay)”，因为它乘以一个系数<span class="math inline">\((1 - \alpha\frac{\lambda}{m})\)</span>来减小权重指标。不断迭代更新，不断地减小。 <span class="math display">\[\begin{aligned} w^{[l]}&amp; :=w^{[l]}-\alpha\cdot dw^{[l]} \\ &amp;=w^{[l]}-\alpha\cdot(dw_{before}^{[l]}+\frac\lambda mw^{[l]}) \\ &amp;=(1-\alpha\frac\lambda m)w^{[l]}-\alpha\cdot dw_{before}^{[l]} \end{aligned}\]</span></p><h2 id="l1-regularization">L1 regularization</h2><p><span class="math display">\[\begin{gathered}J(w,b)=\frac1m\sum_{i=1}^mL(\hat{y}^{(i)},y^{(i)})+\frac\lambda{2m}||w||_1\\||w||_1=\sum_{j=1}^{nx}|w_j|\end{gathered}\]</span> 使用L1正则化，正则项为<span class="math inline">\(\frac{\lambda}{m}\)</span>乘以<span class="math inline">\(\sum_{j= 1}^{n_{x}}{|w|}\)</span>，其中<span class="math inline">\(\sum_{j =1}^{n_{x}}{|w|}\)</span>也被称为参数<span class="math inline">\(w\)</span>向量的L1范数。使用L1正则化后，<span class="math inline">\(w\)</span>最终会是稀疏的，即<span class="math inline">\(w\)</span>向量中有很多0，其优点是能够降低存储内存。</p><p>然而，实际上L1 regularization在解决high variance方面比L2 regularization并不更具优势。而且，L1的在微分求导方面比较复杂。所以，一般L2 regularization更加常用。</p><h2 id="总结">总结</h2><p>正则化能够通过减小参数的值来降低模型的复杂度，从而防止过拟合的出现。 正则化的目的就是让权重w在一个约束范围内梯度下降并寻找最优解</p><h1 id="为什么正则化有利于预防过拟合呢why-regularization-reduces-overfitting">1.5 为什么正则化有利于预防过拟合呢？（Why regularization reduces overfitting?）</h1><p><img src="/img/深度学习-Course2-Week1/1深度学习的实用层面_6.png" srcset="/img/loading.gif" lazyload></p><p>假如我们选择了非常复杂的神经网络模型，如上图左上角所示。在未使用正则化的情况下，我们得到的分类超平面可能是类似上图右侧的过拟合。但是，如果使用L2 regularization,当<span class="math inline">\(\lambda\)</span>很大时，<span class="math inline">\(w^{[l]}\approx0\)</span>。<span class="math inline">\(w^{[l]}\)</span>近似为零，意味着该神经网络模型中的某些神经元实际的作用很小，可以忽略。</p><p><img src="/img/深度学习-Course2-Week1/1深度学习的实用层面_7.png" srcset="/img/loading.gif" lazyload></p><p>从效果上来看，其实是将某些神经元给忽略掉了。这样原本过于复杂的神经网络模型就变得不那么复杂了，而变得非常简单化了。如上图所示，整个简化的神经网络模型变成了一个逻辑回归模型。问题就从high variance变成了high bias 了。</p><p>还有另外一个直观的例子来解释为什么正则化能够避免发生过拟合。假设激活函数是tanh函数。tanh函数的特点是在z接近零的区域，函数近似是线性的，而当|z|很大的时候，函数非线性且变化缓慢。当使用正则化，<span class="math inline">\(\lambda\)</span>较大，即对权重<span class="math inline">\(w^{[l]}\)</span>的惩罚较大，<span class="math inline">\(w^{[l]}\)</span>减小。因为<span class="math inline">\(z^{[l]}=w^{[l]}a^{[l]}+b^{[l]}\)</span>，当<span class="math inline">\(w^{[l]}\)</span>减小的时候，<span class="math inline">\(z^{[l]}\)</span>也会减小。则此时的<span class="math inline">\(z^{[l]}\)</span>分布在tanh函数的近似线性区域（下图红色部分）。那么这个神经元起的作用就相当于是linear regression。如果每个神经元对应的权重<span class="math inline">\(w^{[l]}\)</span>都比较小，那么整个神经网络模型相当于是多个linear regression的组合，即可看成一个linear network。得到的分类超平面就会比较简单，不会出现过拟合现象。 <img src="/img/深度学习-Course2-Week1/1深度学习的实用层面_8.png" srcset="/img/loading.gif" lazyload></p><h1 id="dropoutdropout">1.6 Dropout（Dropout）</h1><p>除了L2 regularization之外，还有另外一种防止过拟合的有效方法：Dropout。</p><p>Dropout是指在深度学习网络的训练过程中，对于每层的神经元，按照一定的概率将其暂时从网络中丢弃。也就是说，每次训练时，每一层都有部分神经元不工作，起到简化复杂网络模型的效果，从而避免发生过拟合。</p><p><img src="/img/深度学习-Course2-Week1/1深度学习的实用层面_9.png" srcset="/img/loading.gif" lazyload></p><p>Dropout有不同的实现方法，接下来介绍一种常用的方法：Inverted dropout。</p><p>（1）假设对于第l层神经元，设定保留神经元比例概率keep_prob=0.8，即该层有20%的神经元停止工作。</p><p>（2）dl为dropout向量，设置dl为随机vector，其中80%的元素为1，20%的元素为0。在python中可以使用如下语句生成dropout vector：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">dl = np.random.rand(al.shape[<span class="hljs-number">0</span>],al.shape[<span class="hljs-number">1</span>])&lt;keep_prob<br></code></pre></td></tr></table></figure><p>（3）然后，第l层经过dropout，随机删减20%的神经元，只保留80%的神经元，其输出为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">al = np.multiply(al,dl)<br></code></pre></td></tr></table></figure><p>（4）最后，还要对al进行scale up处理，即：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">al /= keep_prob<br></code></pre></td></tr></table></figure><p>以上就是Inverted dropout的方法。之所以要对al进行scale up是为了保证在经过dropout后，al作为下一层神经元的输入值尽量保持不变。</p><p>假设第l层有50个神经元，经过dropout后，有10个神经元停止工作，这样只有40个神经元有作用。</p><p>那么得到的al只相当于原来的80%，期望值小了20%。scale up后，能够尽可能保持al的期望值相比之前没有大的变化，不影响整体。</p><p>Inverted dropout的另外一个好处就是在对该dropout后的神经网络进行测试时能够减少scaling问题。</p><p>因为在训练时，使用scale up保证al的期望值没有大的变化，测试时就不需要再对样本数据进行类似的尺度伸缩操作了。</p><h1 id="理解-dropoutunderstanding-dropout">1.7 理解 dropout（Understanding Dropout）</h1><h2 id="dropout基本概念">Dropout基本概念</h2><p>在每次迭代训练时，随机删除掉隐藏层一定数量的神经元；然后，在删除后的剩下的神经元上进行正向和反向更新权重w和常数项b；接着，下一次迭代中，再恢复之前删除的神经元，重新随机删除一定数量的神经元，进行正向和反向更新w和b。不断重复上述过程，直至迭代训练完成。</p><p>值得注意的是，使用dropout训练结束后，==在测试和实际应用模型时，不需要进行dropout和随机删减神经元==，所有的神经元都在工作。</p><h2 id="dropout防止过拟合的原理">Dropout防止过拟合的原理</h2><p>Dropout通过每次迭代训练时，随机选择不同的神经元，相当于每次都在不同的神经网络上进行训练，类似机器学习中Bagging的方法，能够防止过拟合。</p><h2 id="从权重w的角度解释防止过拟合的原理">从权重w的角度解释防止过拟合的原理</h2><p>对于某个神经元来说，某次训练时，它的某些输入在dropout的作用被过滤了。而在下一次训练时，又有不同的某些输入被过滤。经过多次训练后，某些输入被过滤，某些输入被保留。</p><p>这样，该神经元就不会受某个输入非常大的影响，影响被均匀化了。也就是说，对应的权重w不会很大。</p><p>这从效果上来说，与L2 regularization是类似的，都是对权重w进行“惩罚”，减小了w的值。</p><p>总的来说，对于同一组训练数据，利用不同的神经网络训练之后，求其输出的平均值可以减少过拟合。</p><p>Dropout就是利用这个原理，每次丢掉一定数量的隐藏层神经元，相当于在不同的神经网络上进行训练，这样就减少了神经元之间的依赖性，即每个神经元不能依赖于某几个其他的神经元（指层与层之间相连接的神经元），使神经网络更加能学习到与其他神经元之间的更加健壮robust的特征。</p><h2 id="注意事项">注意事项</h2><p>首先，不同隐藏层的dropout系数keep_prob可以不同。一般来说，神经元越多的隐藏层，keep_out可以设置得小一些，例如0.5；神经元越少的隐藏层，keep_out可以设置的大一些，例如0.8，设置是1。</p><p>另外，实际应用中，不建议对输入层进行dropout，如果输入层维度很大，例如图片，那么可以设置dropout，但keep_out应设置的大一些，例如0.8，0.9。</p><p>总体来说，就是越容易出现overfitting的隐藏层，其keep_prob就设置的相对小一些。没有准确固定的做法，通常可以根据validation进行选择。</p><p>Dropout在电脑视觉CV领域应用比较广泛，因为输入层维度较大，而且没有足够多的样本数量。</p><p>值得注意的是dropout是一种regularization技巧，用来防止过拟合的，最好只在需要regularization的时候使用dropout。</p><p>使用dropout的时候，可以通过绘制cost function来进行debug，看看dropout是否正确执行。一般做法是，将所有层的keep_prob全设置为1，再绘制cost function，即涵盖所有神经元，看J是否单调下降。下一次迭代训练时，再将keep_prob设置为其它值。</p><h1 id="其他正则化方法other-regularization-methods">1.8 其他正则化方法（Other regularization methods）</h1><h2 id="数据增强">数据增强</h2><p>除了L2 regularization和dropout regularization之外，还有其它减少过拟合的方法。</p><p>其中一种方法是增加训练样本数量。但是通常成本较高，难以获得额外的训练样本。但是，我们可以对已有的训练样本进行一些处理来“制造”出更多的样本，称为data augmentation。</p><p>例如图片识别问题中，可以对已有的图片进行水平翻转、任意角度旋转、缩放或扩大等等。 <img src="/img/深度学习-Course2-Week1/1深度学习的实用层面_10.png" srcset="/img/loading.gif" lazyload></p><p>这些处理都能“制造”出新的训练样本。虽然这些是基于原有样本的，但是对增大训练样本数量还是有很有帮助的，不需要增加额外成本，却能起到防止过拟合的效果。</p><h2 id="early-stopping">early stopping</h2><p>一个神经网络模型随着迭代训练次数增加，train set error一般是单调减小的，而dev set error 先减小，之后又增大。</p><p>也就是说训练次数过多时，模型会对训练样本拟合的越来越好，但是对验证集拟合效果逐渐变差，即发生了过拟合。</p><p>因此，迭代训练次数不是越多越好，可以通过train set error和dev set error随着迭代次数的变化趋势，选择合适的迭代次数，即early stopping。 <img src="/img/深度学习-Course2-Week1/1深度学习的实用层面_11.png" srcset="/img/loading.gif" lazyload></p><p>然而，Early stopping有其自身缺点。通常来说，机器学习训练模型有两个目标：一是优化cost function，尽量减小J；二是防止过拟合。</p><p>这两个目标彼此对立的，即减小J的同时可能会造成过拟合，反之亦然。我们把这二者之间的关系称为正交化orthogonalization。</p><p>在深度学习中，我们可以同时减小Bias和Variance，构建最佳神经网络模型。</p><p>但是，Early stopping的做法通过减少迭代次数来防止过拟合，这样J就不会足够小。也就是说，early stopping将上述两个目标融合在一起，同时优化，但可能没有“分而治之”的效果好。</p><p>与early stopping相比，L2 regularization可以实现“分而治之”的效果：迭代训练足够多，可以减小J，而且也能有效防止过拟合。</p><p>而L2 regularization的缺点之一是最优的正则化参数λ的选择比较复杂。</p><p>对这一点来说，early stopping比较简单。总的来说，L2 regularization更加常用一些。</p><h1 id="归一化输入normalizing-inputs">1.9 归一化输入（Normalizing inputs）</h1><p>在训练神经网络时，标准化输入可以提高训练的速度。标准化输入就是对训练数据集进行归一化的操作，即将原始数据减去其均值μ后，再除以其方差σ^2： <span class="math display">\[\begin{gathered} \mu=\frac{1}{m}\sum_{i=1}^{m}X^{(i)} \\ \sigma^{2}=\frac1m\sum_{i=1}^{m}(X^{(i)})^{2} \\ \begin{aligned}X:=\frac{X-\mu}{\sigma^2}\end{aligned} \end{gathered}\]</span> 以二维平面为例，下图展示了其归一化过程：</p><p><img src="/img/深度学习-Course2-Week1/1深度学习的实用层面_12.png" srcset="/img/loading.gif" lazyload></p><p>值得注意的是，由于训练集进行了标准化处理，那么对于测试集或在实际应用时，应该使用同样的μ和σ^2对其进行标准化处理。这样保证了训练集合测试集的标准化操作一致。</p><p>之所以要对输入进行标准化操作，主要是为了让所有输入归一化同样的尺度上，方便进行梯度下降算法时能够更快更准确地找到全局最优解。(原理可参考机器学习笔记-&gt;0监督学习-&gt;特征缩放)</p><h1 id="梯度消失梯度爆炸vanishing-exploding-gradients">1.10 梯度消失/梯度爆炸（Vanishing / Exploding gradients）</h1><p>在神经网络尤其是深度神经网络中存在可能存在这样一个问题：梯度消失和梯度爆炸。</p><p>意思是当训练一个层数非常多的神经网络时，计算得到的梯度可能非常小或非常大，甚至是指数级别的减小或增大。这样会让训练过程变得非常困难。</p><p><img src="/img/深度学习-Course2-Week1/1深度学习的实用层面_13.png" srcset="/img/loading.gif" lazyload></p><p>为了简化复杂度，便于分析，我们令各层的激活函数为线性函数，即<span class="math inline">\(g(Z)=Z\)</span>。且忽略各层常数项b的影响，令b全部为零。那么，该网络的预测输出<span class="math inline">\(\hat{Y}\)</span>为：</p><p><span class="math inline">\(\hat{Y}=W^{[L]}W^{[L-1]}W^{[L-2]}\ldots W^{[3]}W^{[2]}W^{[1]}X\)</span> 如果各层权重<span class="math inline">\(W^{[l]}\)</span>的元素都稍大于1，例如1.5, 则预测输出<span class="math inline">\(\hat{Y}\)</span>将正比于<span class="math inline">\(1.5^L\)</span>。L越大，<span class="math inline">\(\hat{Y}\)</span>越大，且呈指数型增长。我们称之为数值爆炸。 相反，如果各层权重<span class="math inline">\(W^{[l]}\)</span>的元素都稍小于1，例如0.5,则预测输出<span class="math inline">\(\hat{Y}\)</span>将正比于<span class="math inline">\(0.5^L\)</span>。网络层数L越多，<span class="math inline">\(\hat{Y}\)</span>呈指数型减小。我们称之为数值消失。</p><p>也就是说，如果各层权重<span class="math inline">\(W^{[l]}\)</span>都大于1或者都小于1，那么各层激活函数的输出将随着层数<span class="math inline">\(l\)</span>的增加，呈指数型增大或减小。当层数很大时，出现数值爆炸或消失。同样，这种情况也会引起梯度呈现同样的指数型增大或减小的变化。L非常大时，例如L=150,则梯度会非常大或非常小，引起每次更新的步进长度过大或者过小，这让训练变得十分困难。</p><h1 id="神经网络的权重初始化weight-initialization-for-deep-networksvanishing-exploding-gradients">1.11 神经网络的权重初始化（Weight Initialization for Deep NetworksVanishing /Exploding gradients）</h1><p>下面介绍如何改善梯度消失/梯度爆炸这类问题，方法是对权重w进行一些初始化处理。</p><p>如果激活函数是tanh，在初始化w时，令其方差为1/n。相应的python伪代码为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">w[l] = np.random.randn(n[l],n[l-<span class="hljs-number">1</span>])*np.sqrt(<span class="hljs-number">1</span>/n[l-<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><p>如果激活函数是ReLU，权重w的初始化一般令其方差为2/n：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">w[l] = np.random.randn(n[l],n[l-<span class="hljs-number">1</span>])*np.sqrt(<span class="hljs-number">2</span>/n[l-<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><p>除此之外，Yoshua Bengio提出了另外一种初始Bengio提出了另外一种初始化w的方法,令其方差为：<span class="math inline">\(\frac2{n^{[l-1]}n^{[l]}}\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">w[l] = np.random.randn(n[l],n[l-<span class="hljs-number">1</span>])*np.sqrt(<span class="hljs-number">2</span>/n[l-<span class="hljs-number">1</span>]*n[l])<br></code></pre></td></tr></table></figure><h1 id="梯度检验gradient-checking">1.12 梯度检验（Gradient checking）</h1><p>Back Propagation神经网络有一项重要的测试是梯度检查（gradient checking）。其目的是检查验证反向传播过程中梯度下降算法是否正确。</p><h2 id="梯度的数值逼近">梯度的数值逼近</h2><p><img src="/img/深度学习-Course2-Week1/1深度学习的实用层面_14.png" srcset="/img/loading.gif" lazyload></p><p>利用微分思想（中值定理<span class="math inline">\(f(x)=(f(b)-f(a))/(b-a)\)</span>），函数f在点θ处的梯度可以表示成： <span class="math display">\[g(\theta)=\frac{f(\theta+\varepsilon)-f(\theta-\varepsilon)}{2\varepsilon}\]</span> 其中，ε&gt;0，且足够小。</p><p>函数f在点θ处的梯度近似等于<span class="math inline">\(g(\theta)\)</span>的值。</p><h2 id="梯度检验">梯度检验</h2><p>用函数值去估计梯度，然后和记录下来的dw或者db比较，看是不是算对了</p><p>梯度检查首先要做的是分别将<span class="math inline">\(W^{[1]},b^{[1]},\cdots,W^{[L]},b^{[L]}\)</span>这些矩阵构造成一维向量，然后将这些一维向量组合起来构成一个更大的一维 向 量 <span class="math inline">\(\theta\)</span>。这 样 cost function <span class="math inline">\(J(W^{[1]},b^{[1]},\ldots,W^{[L]},b^{[L]})\)</span> 就 可以表示成<span class="math inline">\(J(\theta)\)</span>。</p><p>然后将反向传播过程通过梯度下降算法得到的 <span class="math inline">\(dW^{[1]},db^{[1]},\cdots,dW^{[L]},db^{[L]}\)</span>按照一样的顺序构造成一个一维向量<span class="math inline">\(d\theta\)</span>。<span class="math inline">\(d\theta\)</span>的维度与<span class="math inline">\(\theta\)</span>一致。</p><p>接着利用<span class="math inline">\(J(\theta)\)</span>对每个<span class="math inline">\(\theta_i\)</span>计算近似梯度，其值与反 向传播算法得到的<span class="math inline">\(d\theta_i\)</span>相比较，检查是否一致。例如，对于第i个元素，近似梯度为： <span class="math display">\[d\theta_{approx}[i]=\frac{J(\theta_1,\theta_2,\cdots,\theta_i+\varepsilon,\cdots)-J(\theta_1,\theta_2,\cdots,\theta_i-\varepsilon,\cdots)}{2\varepsilon}\]</span></p><p>计算完所有<span class="math inline">\(\theta_i\)</span>的近似梯度后，可以计算<span class="math inline">\(d\theta_{approx}\)</span>与<span class="math inline">\(d\theta\)</span>的欧氏(Euclidean) 距离来比较二者的相似度。公式如下：</p><p><span class="math display">\[\frac{||d\theta_{approx}-d\theta||_2}{||d\theta_{approx}||_2+||d\theta||_2}\]</span></p><p>一般来说，如果欧氏距离越小，例如<span class="math inline">\(10^{-7}\)</span>,甚至更小，则表明<span class="math inline">\(d\theta_{approx}\)</span>与<span class="math inline">\(d\theta\)</span>越接近，即反向梯度计算是正确的，没有bugs。</p><p>如果欧氏距离较大， 例如<span class="math inline">\(10^{-5}\)</span>,则表明梯度计算可能出现问题，需要再次检查是否有bugs存在。</p><p>如果欧氏距离很大，例如<span class="math inline">\(10^{-3}\)</span>,甚至更大，则表明<span class="math inline">\(d\theta_{approx}\)</span>与<span class="math inline">\(d\theta\)</span>差别很大，梯度下降计算过程有bugs，需要仔细检查。</p><h2 id="梯度检验应用的注意事项">梯度检验应用的注意事项</h2><ul><li><p>不要在整个训练过程中都进行梯度检查，仅仅作为debug使用。</p></li><li><p>如果梯度检查出现错误，找到对应出错的梯度，检查其推导是否出现错误。</p></li><li><p>注意不要忽略正则化项，计算近似梯度的时候要包括进去。</p></li><li><p>梯度检查时关闭dropout，检查完毕后再打开dropout。</p></li><li><p>随机初始化时运行梯度检查，经过一些训练后再进行梯度检查（不常用）。</p></li></ul></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="category-chain-item">深度学习</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">#机器学习</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">#深度学习</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">#神经网络</a></div></div><div class="license-box my-3"><div class="license-title"><div>深度学习-Course2-Week1深度学习实用层面</div><div>http://zhou1317fe5.link/2024/02/08/深度学习-Course2-Week1深度学习实用层面/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>Zhou1317fe5</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2024年2月8日</div></div><div class="license-meta-item"><div>许可协议</div><div><a target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2024/02/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course2-Week2%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/" title="深度学习-Course2-Week2优化算法"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">深度学习-Course2-Week2优化算法</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2024/02/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course1-Week4%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="深度学习-Course1_Week4深层神经网络"><span class="hidden-mobile">深度学习-Course1_Week4深层神经网络</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div><div class="statistics"><span id="busuanzi_container_site_pv" style="display:none">总访问量 <span id="busuanzi_value_site_pv"></span> 次 </span><span id="busuanzi_container_site_uv" style="display:none">总访客数 <span id="busuanzi_value_site_uv"></span> 人</span></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script>window.MathJax?(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise()):window.MathJax={tex:{inlineMath:{"[+]":[["$","$"]]}},loader:{load:["ui/lazy"]},options:{renderActions:{insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(t=>{let e=t.parentNode;"li"===e.nodeName.toLowerCase()&&e.parentNode.classList.add("has-jax")})},"",!1]}}},Fluid.events.registerRefreshCallback((function(){"MathJax"in window&&MathJax.startup.document&&"function"==typeof MathJax.startup.document.state&&(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise())}))</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script src="/js/local-search.js"></script><script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>