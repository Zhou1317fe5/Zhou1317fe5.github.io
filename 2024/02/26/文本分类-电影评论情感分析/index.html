<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png"><link rel="icon" href="/img/fluid.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="Zhou1317fe5"><meta name="keywords" content=""><meta name="description" content="基于深度学习的文本分类,Kaggle代码LSTM 实现详解"><meta property="og:type" content="article"><meta property="og:title" content="文本分类-电影评论情感分析"><meta property="og:url" content="http://zhou1317fe5.link/2024/02/26/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB-%E7%94%B5%E5%BD%B1%E8%AF%84%E8%AE%BA%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/index.html"><meta property="og:site_name" content="Zhou1317fe5"><meta property="og:description" content="基于深度学习的文本分类,Kaggle代码LSTM 实现详解"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://zhou1317fe5.link/img/%E7%94%B5%E5%BD%B1%E8%AF%84%E8%AE%BA%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/output_19_0.png"><meta property="og:image" content="http://zhou1317fe5.link/img/%E7%94%B5%E5%BD%B1%E8%AF%84%E8%AE%BA%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/output_19_1.png"><meta property="article:published_time" content="2024-02-26T02:23:24.000Z"><meta property="article:modified_time" content="2024-02-26T02:28:33.541Z"><meta property="article:author" content="Zhou1317fe5"><meta property="article:tag" content="深度学习"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="http://zhou1317fe5.link/img/%E7%94%B5%E5%BD%B1%E8%AF%84%E8%AE%BA%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/output_19_0.png"><meta name="referrer" content="no-referrer-when-downgrade"><title>文本分类-电影评论情感分析 - Zhou1317fe5</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="/css/iconfont_csdn/iconfont.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"zhou1317fe5.link",root:"/",version:"1.9.4",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1}},search_path:"/local-search.xml"};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><meta name="generator" content="Hexo 6.3.0"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>Zhou1317fe5</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/Post_banner_img.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="文本分类-电影评论情感分析"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2024-02-26 10:23" pubdate>2024年2月26日 上午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 16k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 136 分钟 </span><span id="busuanzi_container_page_pv" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="busuanzi_value_page_pv"></span> 次</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 style="display:none">文本分类-电影评论情感分析</h1><div class="markdown-body"><p>基于深度学习的文本分类,Kaggle代码<a target="_blank" rel="noopener" href="https://www.kaggle.com/code/hanjoonchoe/movie-sentimental-analysis-lstm-pytorch">LSTM 实现</a>详解。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np <br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd <br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> Counter<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader,Dataset<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">train = pd.read_csv(<span class="hljs-string">&#x27;./data/train.tsv&#x27;</span>,sep=<span class="hljs-string">&#x27;\t&#x27;</span>)<br>test = pd.read_csv(<span class="hljs-string">&#x27;./data/test.tsv&#x27;</span>,sep=<span class="hljs-string">&#x27;\t&#x27;</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train.head()<br></code></pre></td></tr></table></figure><div><style scoped>.dataframe tbody tr th:only-of-type{vertical-align:middle}.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</style><table border="1" class="dataframe"><thead><tr style="text-align:right"><th></th><th>PhraseId</th><th>SentenceId</th><th>Phrase</th><th>Sentiment</th></tr></thead><tbody><tr><th>0</th><td>1</td><td>1</td><td>A series of escapades demonstrating the adage ...</td><td>1</td></tr><tr><th>1</th><td>2</td><td>1</td><td>A series of escapades demonstrating the adage ...</td><td>2</td></tr><tr><th>2</th><td>3</td><td>1</td><td>A series</td><td>2</td></tr><tr><th>3</th><td>4</td><td>1</td><td>A</td><td>2</td></tr><tr><th>4</th><td>5</td><td>1</td><td>series</td><td>2</td></tr></tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">test.head()<br></code></pre></td></tr></table></figure><div><style scoped>.dataframe tbody tr th:only-of-type{vertical-align:middle}.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</style><table border="1" class="dataframe"><thead><tr style="text-align:right"><th></th><th>PhraseId</th><th>SentenceId</th><th>Phrase</th></tr></thead><tbody><tr><th>0</th><td>156061</td><td>8545</td><td>An intermittently pleasing but mostly routine ...</td></tr><tr><th>1</th><td>156062</td><td>8545</td><td>An intermittently pleasing but mostly routine ...</td></tr><tr><th>2</th><td>156063</td><td>8545</td><td>An</td></tr><tr><th>3</th><td>156064</td><td>8545</td><td>intermittently pleasing but mostly routine effort</td></tr><tr><th>4</th><td>156065</td><td>8545</td><td>intermittently pleasing but mostly routine</td></tr></tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">Corpus_Extr</span>(<span class="hljs-params">df</span>):<br>    <span class="hljs-comment"># 构建语料库</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Construct Corpus...&#x27;</span>)<br>    corpus = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(df))):<br>        corpus.append(df.Phrase[i].lower().split())<br>    <br>    <span class="hljs-comment"># 统计词频</span><br>    corpus = Counter(np.hstack(corpus))<br>    corpus = corpus<br><br>    <span class="hljs-comment"># 排序语料库</span><br>    corpus2 = <span class="hljs-built_in">sorted</span>(corpus,key=corpus.get,reverse=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-comment"># 将语料库中的单词转换为整数</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Convert Corpus to Integers&#x27;</span>)<br>    vocab_to_int = &#123;word: idx <span class="hljs-keyword">for</span> idx,word <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(corpus2,<span class="hljs-number">1</span>)&#125;<br><br>    <span class="hljs-comment"># 将短语转换为整数序列</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Convert Phrase to Integers&#x27;</span>)<br>    phrase_to_int = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(df))):<br>        phrase_to_int.append([vocab_to_int[word] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> df.Phrase.values[i].lower().split()])<br>    <span class="hljs-keyword">return</span> corpus,vocab_to_int,phrase_to_int<br><br>corpus,vocab_to_int,phrase_to_int = Corpus_Extr(train)<br></code></pre></td></tr></table></figure><pre><code class="hljs">Construct Corpus...


100%|██████████| 156060/156060 [00:01&lt;00:00, 106095.64it/s]


Convert Corpus to Integers
Convert Phrase to Integers


100%|██████████| 156060/156060 [00:01&lt;00:00, 128755.83it/s]</code></pre><h4 id="代码详解">代码详解</h4><p><strong>整体解释：</strong></p><p>这段代码定义了一个名为<code>Corpus_Extr</code>的函数，用于从数据框<code>df</code>中构建语料库，并将语料库中的单词及短语转换为整数表示。下面是对函数中各步骤的详细解释：</p><ol type="1"><li><strong>构建语料库：</strong><ul><li>首先打印出<code>'Construct Corpus...'</code>提示开始构建语料库。</li><li>通过迭代数据框<code>df</code>中的每一行来构建一个单词列表<code>corpus</code>。对于<code>df</code>中的每个"Phrase"项，都将其转换为小写并分割成单词，然后加入到<code>corpus</code>列表中。</li></ul></li><li><strong>统计词频：</strong><ul><li>使用<code>np.hstack</code>将<code>corpus</code>中的所有单词列表连接成一个大的一维数组。</li><li>使用<code>Counter</code>统计这个数组中每个单词的出现次数，即得到一个词频字典<code>corpus</code>。</li></ul></li><li><strong>排序语料库：</strong><ul><li>使用<code>sorted</code>函数对<code>corpus</code>按词频降序排序，生成新的列表<code>corpus2</code>。</li></ul></li><li><strong>将语料库中的单词转换为整数：</strong><ul><li>打印出<code>'Convert Corpus to Integers'</code>提示开始转换。</li><li>通过<code>enumerate</code>函数为排序后的语料库<code>corpus2</code>中的每个单词分配一个独一无二的索引（整数类型），从1开始编号（因为1作为参数传递给了<code>enumerate</code>），创建一个<code>vocab_to_int</code>字典。 例如，<code>A</code>的索引为<code>3</code>，对应的编号为<code>3</code>，,<code>series</code>的索引为<code>315</code>，则编号为<code>315</code>。</li></ul></li><li><strong>将短语转换为整数序列：</strong><ul><li>打印出<code>'Convert Phrase to Integers'</code>提示开始转换。</li><li>再次迭代数据框<code>df</code>的每一行，并使用<code>vocab_to_int</code>字典将每个短语中的单词转换为其对应的整数（索引），生成<code>phrase_to_int</code>列表。这个列表包含了转换为整数序列的短语。例如，短语为<code>A series</code> ，则转换为<code>[3, 315]</code></li></ul></li><li><strong>返回结果：</strong><ul><li>函数返回三个对象：<code>corpus</code>（词频字典），<code>vocab_to_int</code>（单词到整数的字典），和<code>phrase_to_int</code>（短语到整数序列的列表）。</li></ul></li></ol><p>在函数调用的最后一行，<code>Corpus_Extr</code>函数被用于数据框<code>train</code>，并且将返回的对象赋值给变量<code>corpus</code>，<code>vocab_to_int</code>，和<code>phrase_to_int</code>。</p><p><strong>细节解释：</strong></p><ol type="1"><li><code>corpus = Counter(np.hstack(corpus))</code></li></ol><p><code>np.hstack()</code>函数用于将两个或多个数组连接成一个数组，这里将<code>corpus</code>列表中的所有单词连接成一个数组，然后使用<code>Counter()</code>函数计算每个单词的计数。</p><p><code>Counter()</code>函数用于计算序列中元素的频率。它接受一个可迭代对象（如列表、元组等）作为参数，并返回一个字典，其中键是序列中的元素，值是对应的元素出现的次数。</p><p>例如，如果<code>corpus</code>列表中包含以下单词：</p><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scheme">[<span class="hljs-symbol">&#x27;apple</span>&#x27;, <span class="hljs-symbol">&#x27;banana</span>&#x27;, <span class="hljs-symbol">&#x27;apple</span>&#x27;, <span class="hljs-symbol">&#x27;orange</span>&#x27;, <span class="hljs-symbol">&#x27;banana</span>&#x27;, <span class="hljs-symbol">&#x27;apple</span>&#x27;]<br></code></pre></td></tr></table></figure><p>则<code>corpus</code>列表中的单词计数将如下所示：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">Counter</span><span class="hljs-params">(&#123;<span class="hljs-string">&#x27;apple&#x27;</span>: <span class="hljs-number">3</span>, <span class="hljs-string">&#x27;banana&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;orange&#x27;</span>: <span class="hljs-number">1</span>&#125;)</span></span><br></code></pre></td></tr></table></figure><p>其中，<code>'apple'</code>出现了3次，<code>'banana'</code>和<code>'orange'</code>各出现了2次。</p><hr><ol start="2" type="1"><li><code>vocab_to_int = &#123;word: idx for idx,word in enumerate(corpus2,1)&#125;</code></li></ol><p>创建了一个字典vocab_to_int，用于将corpus2中的单词映射到整数索引。</p><ul><li><p><code>&#123;&#125;</code>：这是字典的创建方法，表示创建一个空字典。</p></li><li><p><code>word: idx</code>：这是字典的键值对，其中word是键，idx是值。</p></li><li><p><code>for idx, word in enumerate(corpus2, 1)</code>：这是一个for循环，用于迭代corpus2中的元素。enumerate函数将corpus2中的每个元素与它的索引配对。idx是索引，word是corpus2中的元素。</p></li><li><p><code>enumerate(corpus2, 1)</code>：enumerate函数用于返回一个枚举对象，它生成一个包含索引和值的元组。参数1指定开始索引的值，这里设置为1。</p></li><li><p><code>word: idx for idx, word in enumerate(corpus2, 1)</code>：这是一个字典推导式，它使用enumerate函数生成的元组创建字典的键值对。</p></li></ul><p>代码行<code>vocab_to_int = &#123;word: idx for idx,word in enumerate(corpus2,1)&#125;</code>将corpus2中的单词作为键，索引作为值，创建了一个映射字典vocab_to_int。索引的起始值为1。 这个字典可以在后续代码中用来将单词转换为整数索引。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 填充序列</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">Pad_sequences</span>(<span class="hljs-params">phrase_to_int,seq_length</span>):<br>    <span class="hljs-comment"># 初始化填充序列的矩阵</span><br>    pad_sequences = np.zeros((<span class="hljs-built_in">len</span>(phrase_to_int), seq_length),dtype=<span class="hljs-built_in">int</span>)<br>    <span class="hljs-comment"># 填充或截断序列</span><br>    <span class="hljs-keyword">for</span> idx,row <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">enumerate</span>(phrase_to_int),total=<span class="hljs-built_in">len</span>(phrase_to_int)):<br>        pad_sequences[idx, :<span class="hljs-built_in">len</span>(row)] = np.array(row)[:seq_length]<br>    <span class="hljs-keyword">return</span> pad_sequences<br></code></pre></td></tr></table></figure><h4 id="代码解释">代码解释</h4><p><strong>整体解释：</strong></p><p>这段代码定义了一个名为<code>Pad_sequences</code>的函数，其功能是将整数序列列表<code>phrase_to_int</code>填充（或截断）到一个统一的长度<code>seq_length</code>。让我们逐步了解这个函数的工作流程：</p><ol type="1"><li><strong>初始化填充序列的矩阵：</strong><ul><li>创建一个名为<code>pad_sequences</code>的矩阵，其形状为<code>(len(phrase_to_int), seq_length)</code>，即行数等于整数序列列表中短语的数量，列数等于指定的序列长度<code>seq_length</code>。</li><li>该矩阵被初始化为零，并且数据类型设为整数。</li></ul></li><li><strong>填充或截断序列：</strong><ul><li>通过迭代<code>phrase_to_int</code>中的每个整数序列，<code>enumerate</code>函数提供当前序列的索引<code>idx</code>和序列的内容<code>row</code>。</li><li><code>tqdm</code>被用于包装迭代器，显示进度条，<code>total</code>参数设定为<code>phrase_to_int</code>的长度，确保进度条反映整体进度。</li><li>对于每一行，将行索引<code>idx</code>对应的<code>pad_sequences</code>的前n个元素设置为<code>row</code>的内容，其中n是<code>row</code>的长度或<code>seq_length</code>中较小的一个，这保证了不会超过指定的序列长度。</li><li>如果<code>row</code>的长度小于<code>seq_length</code>，则保持剩余元素为0（即填充）；如果<code>row</code>的长度大于<code>seq_length</code>，则<code>row</code>将被截断到<code>seq_length</code>。</li></ul></li><li><strong>返回填充后的矩阵：</strong><ul><li>函数最终返回<code>pad_sequences</code>矩阵，其包含了填充（或截断）后的整数序列。</li></ul></li></ol><p>例如，如果我们有以下整数序列列表<code>[[1, 2, 3], [4, 5]]</code>和序列长度<code>seq_length = 5</code>，使用<code>Pad_sequences</code>函数后，我们会得到一个矩阵：</p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs inform7"><span class="hljs-comment">[</span><br><span class="hljs-comment"> <span class="hljs-comment">[1, 2, 3, 0, 0]</span>,</span><br><span class="hljs-comment"> <span class="hljs-comment">[4, 5, 0, 0, 0]</span></span><br><span class="hljs-comment">]</span><br></code></pre></td></tr></table></figure><p>填充技术常用于自然语言处理中，尤其是在准备训练模型的数据时，因为模型通常需要固定长度的输入序列。填充是增加额外的“无意义”数据来达到这个长度，而截断是丢弲末尾的数据以缩短序列长度。</p><p><strong>细节解释：</strong></p><ul><li><code>pad_sequences[idx, :len(row)] = np.array(row)[:seq_length]</code></li></ul><p>这行代码是<code>Pad_sequences</code>函数中用来对单个短语的整数序列进行填充或截断的关键操作。让我们分解这个操作：</p><ol type="1"><li><code>pad_sequences[idx, :len(row)]</code>:<ul><li><code>pad_sequences</code>是一个二维numpy数组，其大小为<code>(len(phrase_to_int), seq_length)</code>，其中<code>len(phrase_to_int)</code>是短语的总数，<code>seq_length</code>是填充后序列的固定长度。</li><li><code>idx</code>是当前正在处理的短语的索引。</li><li><code>:len(row)</code>是一个切片操作，用来指定<code>pad_sequences</code>的第<code>idx</code>行的从第一个元素到<code>len(row)</code>个元素。这意味着只有当前短语长度内的部分将被新值替换，超出当前短语长度的部分（即填充部分）仍然保持初始化时的零值。</li><li>当<code>len(row)</code> &lt; = <code>seq_length</code>时，<code>pad_sequences[idx, :len(row)]</code>表示<code>pad_sequences</code>前<code>len(row)</code>个值替换为<code>row</code>(短语的整数序列)。</li><li>当<code>len(row)</code> &gt; <code>seq_length</code>时，<code>pad_sequences</code>只能切片到<code>seq_length</code>的长度，即<code>pad_sequences</code>前<code>seq_length</code>个值替换为<code>row</code>(短语的整数序列)的前<code>seq_length</code>个值。</li></ul></li><li><code>np.array(row)[:seq_length]</code>:<ul><li><code>row</code>表示当前短语的整数序列。</li><li><code>np.array(row)</code>将这个列表转换为numpy数组，以使用numpy的切片功能。</li><li><code>[:seq_length]</code>是一个切片操作，用来选取从开头到<code>seq_length</code>位置的元素。这里的作用是确保即使<code>row</code>的长度大于<code>seq_length</code>，也只选取前<code>seq_length</code>个元素进行操作。</li></ul></li></ol><p>综合来看，<code>pad_sequences[idx, :len(row)] = np.array(row)[:seq_length]</code>这行代码的作用是：</p><ul><li>如果当前整数序列<code>row</code>的长度小于或等于<code>seq_length</code>，那么<code>row</code>中的全部元素将被复制到<code>pad_sequences</code>的第<code>idx</code>行，且不会有任何改变（<code>row</code>长度小于<code>seq_length</code>时，剩余部分保持为零）。</li><li>如果当前整数序列<code>row</code>的长度大于<code>seq_length</code>，那么<code>row</code>将被截断，只有前<code>seq_length</code>个元素被复制到<code>pad_sequences</code>的第<code>idx</code>行。</li></ul><p>这种操作允许在不改变原有语料顺序的基础上，统一短语的长度，以便它们能被用于需要固定大小输入的机器学习模型中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">pad_sequences = Pad_sequences(phrase_to_int,<span class="hljs-number">30</span>)<br></code></pre></td></tr></table></figure><pre><code class="hljs">100%|██████████| 156060/156060 [00:00&lt;00:00, 586561.27it/s]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train.sample(<span class="hljs-number">50</span>)<br></code></pre></td></tr></table></figure><div><style scoped>.dataframe tbody tr th:only-of-type{vertical-align:middle}.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</style><table border="1" class="dataframe"><thead><tr style="text-align:right"><th></th><th>PhraseId</th><th>SentenceId</th><th>Phrase</th><th>Sentiment</th></tr></thead><tbody><tr><th>146370</th><td>146371</td><td>7961</td><td>Ever see one of those comedies that just seem ...</td><td>1</td></tr><tr><th>109634</th><td>109635</td><td>5809</td><td>The slapstick is labored , and the bigger setp...</td><td>0</td></tr><tr><th>38716</th><td>38717</td><td>1845</td><td>this turd</td><td>0</td></tr><tr><th>155765</th><td>155766</td><td>8526</td><td>The Santa Clause 2 is a barely adequate babysi...</td><td>1</td></tr><tr><th>35762</th><td>35763</td><td>1687</td><td>be able to look away for a second</td><td>3</td></tr><tr><th>154282</th><td>154283</td><td>8431</td><td>been lost in the translation this time</td><td>1</td></tr><tr><th>28948</th><td>28949</td><td>1341</td><td>is very , very far</td><td>1</td></tr><tr><th>18779</th><td>18780</td><td>825</td><td>averting</td><td>2</td></tr><tr><th>75864</th><td>75865</td><td>3891</td><td>I could not stand them</td><td>0</td></tr><tr><th>14023</th><td>14024</td><td>603</td><td>playful respite</td><td>3</td></tr><tr><th>3146</th><td>3147</td><td>118</td><td>sumptuous</td><td>3</td></tr><tr><th>21437</th><td>21438</td><td>959</td><td>real star</td><td>3</td></tr><tr><th>100844</th><td>100845</td><td>5296</td><td>about black urban professionals</td><td>2</td></tr><tr><th>15019</th><td>15020</td><td>646</td><td>benevolent deception , which , while it may no...</td><td>3</td></tr><tr><th>127717</th><td>127718</td><td>6869</td><td>'s also a -- dare I say it twice -- delightful...</td><td>4</td></tr><tr><th>149868</th><td>149869</td><td>8160</td><td>a good ear for dialogue ,</td><td>3</td></tr><tr><th>51768</th><td>51769</td><td>2553</td><td>accessible and</td><td>3</td></tr><tr><th>132644</th><td>132645</td><td>7152</td><td>a waste of De Niro , McDormand and the other g...</td><td>1</td></tr><tr><th>69389</th><td>69390</td><td>3527</td><td>solid sci-fi thriller</td><td>3</td></tr><tr><th>66970</th><td>66971</td><td>3399</td><td>Close Encounters</td><td>2</td></tr><tr><th>106267</th><td>106268</td><td>5610</td><td>does a flip-flop</td><td>2</td></tr><tr><th>74661</th><td>74662</td><td>3828</td><td>made about an otherwise appalling , and downri...</td><td>2</td></tr><tr><th>136015</th><td>136016</td><td>7348</td><td>at its most</td><td>2</td></tr><tr><th>92024</th><td>92025</td><td>4788</td><td>given up on in favor of sentimental war movies...</td><td>2</td></tr><tr><th>148394</th><td>148395</td><td>8074</td><td>guns , drugs , avarice and damaged dreams</td><td>2</td></tr><tr><th>97725</th><td>97726</td><td>5116</td><td>baseball-playing</td><td>2</td></tr><tr><th>43334</th><td>43335</td><td>2094</td><td>and , through it all , human</td><td>3</td></tr><tr><th>99334</th><td>99335</td><td>5209</td><td>represents an engaging and intimate first feat...</td><td>3</td></tr><tr><th>74235</th><td>74236</td><td>3802</td><td>wise men</td><td>3</td></tr><tr><th>19568</th><td>19569</td><td>859</td><td>bear suits</td><td>2</td></tr><tr><th>85740</th><td>85741</td><td>4435</td><td>a bore</td><td>1</td></tr><tr><th>78392</th><td>78393</td><td>4032</td><td>'re most likely to find on the next inevitable...</td><td>1</td></tr><tr><th>64844</th><td>64845</td><td>3283</td><td>a bumbling American in Europe</td><td>2</td></tr><tr><th>65549</th><td>65550</td><td>3320</td><td>who face arrest 15 years after their crime</td><td>2</td></tr><tr><th>3660</th><td>3661</td><td>138</td><td>does n't end up having much that is fresh to s...</td><td>1</td></tr><tr><th>20350</th><td>20351</td><td>909</td><td>it is all awkward , static , and lifeless rumb...</td><td>2</td></tr><tr><th>81036</th><td>81037</td><td>4176</td><td>of hell so shattering it</td><td>1</td></tr><tr><th>87753</th><td>87754</td><td>4556</td><td>real visual charge</td><td>3</td></tr><tr><th>40372</th><td>40373</td><td>1932</td><td>speculative effort</td><td>3</td></tr><tr><th>87707</th><td>87708</td><td>4554</td><td>compensate for them</td><td>2</td></tr><tr><th>45668</th><td>45669</td><td>2219</td><td>me want to bolt the theater in the first 10 mi...</td><td>0</td></tr><tr><th>118321</th><td>118322</td><td>6321</td><td>tweaked up</td><td>2</td></tr><tr><th>31405</th><td>31406</td><td>1468</td><td>just zings along with vibrance and warmth .</td><td>4</td></tr><tr><th>49093</th><td>49094</td><td>2398</td><td>romantic thriller</td><td>3</td></tr><tr><th>32154</th><td>32155</td><td>1506</td><td>und drung , but explains its characters ' deci...</td><td>1</td></tr><tr><th>20466</th><td>20467</td><td>913</td><td>the second half</td><td>2</td></tr><tr><th>56965</th><td>56966</td><td>2866</td><td>one big laugh , three or four mild giggles , and</td><td>3</td></tr><tr><th>60104</th><td>60105</td><td>3032</td><td>Galinsky</td><td>2</td></tr><tr><th>109183</th><td>109184</td><td>5781</td><td>true emotions</td><td>3</td></tr><tr><th>73647</th><td>73648</td><td>3766</td><td>is how so many talented people were convinced ...</td><td>0</td></tr></tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">PhraseDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,df,pad_sequences</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.df = df<br>        self.pad_sequences = pad_sequences<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.df)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self,idx</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;Sentiment&#x27;</span> <span class="hljs-keyword">in</span> self.df.columns:<br>            label = self.df[<span class="hljs-string">&#x27;Sentiment&#x27;</span>].values[idx]<br>            item = self.pad_sequences[idx]<br>            <span class="hljs-keyword">return</span> item,label<br>        <span class="hljs-keyword">else</span>:<br>            item = self.pad_sequences[idx]<br>            <span class="hljs-keyword">return</span> item<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SentimentRNN</span>(nn.Module):<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,corpus_size,output_size,embedd_dim,hidden_dim,n_layers</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.output_size = output_size<br>        self.n_layers = n_layers<br>        self.hidden_dim = hidden_dim<br>        <br>        self.embedding = nn.Embedding(corpus_size,embedd_dim)<br>        self.lstm = nn.LSTM(embedd_dim, hidden_dim,n_layers,dropout=<span class="hljs-number">0.5</span>, batch_first=<span class="hljs-literal">True</span>)<br>        self.dropout = nn.Dropout(<span class="hljs-number">0.3</span>)<br>        self.fc = nn.Linear(hidden_dim,output_size)<br>        self.act = nn.Sigmoid()<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x,hidden</span>):<br>        batch_size = x.size(<span class="hljs-number">0</span>)<br>        embeds = self.embedding(x)<br>        lstm_out, hidden = self.lstm(embeds,hidden)<br>        lstm_out = lstm_out.contiguous().view(-<span class="hljs-number">1</span>,self.hidden_dim)<br>        out = self.dropout(lstm_out)<br>        out = self.fc(out)<br>        out = self.act(out)<br>        out = out.view(batch_size,-<span class="hljs-number">1</span>)<br>        out = out[:,-<span class="hljs-number">5</span>:]<br>        <span class="hljs-keyword">return</span> out, hidden<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_hidden</span>(<span class="hljs-params">self,batch_size</span>):<br>        weight = <span class="hljs-built_in">next</span>(self.parameters()).data<br>        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),<br>                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())<br>        <span class="hljs-keyword">return</span> hidden<br></code></pre></td></tr></table></figure><h4 id="代码解释-1">代码解释</h4><p>这段代码定义了一个名为<code>SentimentRNN</code>的类，它是一个继承自<code>nn.Module</code>的PyTorch神经网络模型，用于情感分析。下面是详细解释每个部分：</p><p><strong>类初始化方法 (<code>__init__</code>):</strong></p><ul><li><code>self.output_size</code>: 输出层的大小。</li><li><code>self.n_layers</code>: LSTM层的数量。</li><li><code>self.hidden_dim</code>: LSTM层中隐藏状态的特征维度。</li><li><code>self.embedding</code>: 嵌入层，使用<code>nn.Embedding</code>根据语料库大小和词嵌入维度<code>embedd_dim</code>创建了一个查找表，用于将单词整数映射转成嵌入向量。</li><li><code>self.lstm</code>: LSTM层，定义了LSTM网络的结构，包括输入数据的维度<code>embedd_dim</code>、隐藏层维度<code>hidden_dim</code>、层数<code>n_layers</code>、以及dropout比率为0.5，<code>batch_first=True</code>指明输入数据的第一个维度是批次大小。</li><li><code>self.dropout</code>: 丢弃层，使用<code>nn.Dropout</code>定义丢弃率为0.3以减少过拟合。</li><li><code>self.fc</code>: 全连接层，使用<code>nn.Linear</code>将LSTM的输出映射到输出层的大小<code>output_size</code>。</li><li><code>self.act</code>: 激活层，使用<code>nn.Sigmoid</code>函数，该函数的输出通常用于二分类情感分析（输出一个概率值表示正面情感的概率）。</li></ul><p><strong>前向传播方法 (<code>forward</code>):</strong></p><ul><li><code>batch_size</code>: 通过获取输入<code>x</code>的第一个维度大小来确定批次大小。</li><li><code>embeds</code>: 使用嵌入层将输入的单词整数序列转换为嵌入向量。</li><li><code>lstm_out</code>, <code>hidden</code>: LSTM层的输出和隐藏状态。</li><li><code>lstm_out = lstm_out.contiguous().view(-1,self.hidden_dim)</code>: 调整LSTM层输出的形状以匹配全连接层的输入要求。</li><li><code>self.dropout(lstm_out)</code>: 应用丢弃层。</li><li><code>self.fc(out)</code>: 将丢弃层的输出通过全连接层。</li><li><code>self.act(out)</code>: 使用Sigmoid激活函数。</li></ul><p>接着将处理过的输出调整回批次格式，并通过<code>out[:,-5:]</code>得到每个序列的最后五个时间步（短语）的结果。这假定情感的输出可能取决于序列的最后几个单词。</p><p><strong>初始化隐藏状态方法 (<code>init_hidden</code>):</strong></p><ul><li>用于初始化一个包含两个全零张量的隐藏状态元组。(一个用于LSTM的隐藏状态，另一个用于LSTM的细胞状态)。</li><li><code>batch_size</code>: 批次大小，决定了隐藏状态的第二维的大小。</li><li><code>weight.new</code>: 创建一个与模型参数同类型的张量，<code>self.parameters()</code>是一个迭代器，包含模型所有参数。</li><li><code>zero_()</code>: 将张量内的所有元素置为0。</li></ul><p>总体来说，这个<code>SentimentRNN</code>是一个用于情感分析的循环神经网络，它会输出每个输入序列的情感倾向，这是通过查看序列最后五个元素的输出来判断的。这个网络在初始化时需要定义一系列的参数，并在之后可以接收输入数据和相应的隐藏状态进行前向传播以及在训练前初始化隐藏状态。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">vocab_size = <span class="hljs-built_in">len</span>(vocab_to_int) <span class="hljs-comment"># 语料库大小</span><br>output_size = <span class="hljs-number">5</span> <span class="hljs-comment"># 输出维度</span><br>embedding_dim = <span class="hljs-number">400</span> <span class="hljs-comment"># 词嵌入维度</span><br>hidden_dim = <span class="hljs-number">256</span> <span class="hljs-comment"># 隐藏层维度</span><br>n_layers = <span class="hljs-number">2</span> <span class="hljs-comment"># 隐藏层数</span><br>net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim,n_layers)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(net)<br></code></pre></td></tr></table></figure><pre><code class="hljs">SentimentRNN(
  (embedding): Embedding(16531, 400)
  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)
  (dropout): Dropout(p=0.3, inplace=False)
  (fc): Linear(in_features=256, out_features=5, bias=True)
  (act): Sigmoid()
)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> gc<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs python">net.train()<br>clip=<span class="hljs-number">5</span><br>epochs = <span class="hljs-number">200</span><br>counter = <span class="hljs-number">0</span><br>print_every = <span class="hljs-number">20</span><br>lr=<span class="hljs-number">0.01</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">criterion</span>(<span class="hljs-params"><span class="hljs-built_in">input</span>, target, size_average=<span class="hljs-literal">True</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Categorical cross-entropy with logits input and one-hot target&quot;&quot;&quot;</span><br>    l = -(target * torch.log(F.softmax(<span class="hljs-built_in">input</span>, dim=<span class="hljs-number">1</span>) + <span class="hljs-number">1e-10</span>)).<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">if</span> size_average:<br>        l = l.mean()<br>    <span class="hljs-keyword">else</span>:<br>        l = l.<span class="hljs-built_in">sum</span>()<br>    <span class="hljs-keyword">return</span> l<br>optimizer = torch.optim.Adam(net.parameters(), lr=lr)<br><br>batch_size=<span class="hljs-number">32</span><br>losses = []<br>accs=[]<br><span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>    a = np.random.choice(<span class="hljs-built_in">len</span>(train)-<span class="hljs-number">1</span>, <span class="hljs-number">1000</span>)<br>    train_set = PhraseDataset(train.loc[train.index.isin(np.sort(a))],pad_sequences[a])<br>    train_loader = DataLoader(train_set,batch_size=<span class="hljs-number">32</span>,shuffle=<span class="hljs-literal">True</span>)<br>    <span class="hljs-comment"># initialize hidden state</span><br>    h = net.init_hidden(<span class="hljs-number">32</span>)<br>    running_loss = <span class="hljs-number">0.0</span><br>    running_acc = <span class="hljs-number">0.0</span><br>    <span class="hljs-comment"># batch loop</span><br>    <span class="hljs-keyword">for</span> idx,(inputs, labels) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader):<br>        counter += <span class="hljs-number">1</span><br>        gc.collect()<br>        <span class="hljs-comment"># Creating new variables for the hidden state, otherwise</span><br>        <span class="hljs-comment"># we&#x27;d backprop through the entire training history</span><br>        h = <span class="hljs-built_in">tuple</span>([each.data <span class="hljs-keyword">for</span> each <span class="hljs-keyword">in</span> h])<br><br>        <span class="hljs-comment"># zero accumulated gradients</span><br>        optimizer.zero_grad()<br>        <span class="hljs-keyword">if</span> inputs.shape[<span class="hljs-number">0</span>] != batch_size:<br>            <span class="hljs-keyword">break</span><br>        <span class="hljs-comment"># get the output from the model</span><br>        output, h = net(inputs, h)<br>        labels=torch.nn.functional.one_hot(labels, num_classes=<span class="hljs-number">5</span>)<br>        <span class="hljs-comment"># calculate the loss and perform backprop</span><br>        loss = criterion(output, labels)<br>        loss.backward()<br>        running_loss += loss.cpu().detach().numpy()<br>        running_acc += (output.argmax(dim=<span class="hljs-number">1</span>) == labels.argmax(dim=<span class="hljs-number">1</span>)).<span class="hljs-built_in">float</span>().mean()<br>        <span class="hljs-comment"># `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.</span><br>        nn.utils.clip_grad_norm_(net.parameters(), clip)<br>        optimizer.step()<br>        <span class="hljs-keyword">if</span> idx%print_every == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Epoch: &#123;&#125;/&#123;&#125;...&quot;</span>.<span class="hljs-built_in">format</span>(e+<span class="hljs-number">1</span>, epochs),<br>                  <span class="hljs-string">&quot;Step: &#123;&#125;...&quot;</span>.<span class="hljs-built_in">format</span>(counter),<br>                  <span class="hljs-string">&quot;Loss: &#123;:.6f&#125;...&quot;</span>.<span class="hljs-built_in">format</span>((running_loss/(idx+<span class="hljs-number">1</span>))))<br>            losses.append(<span class="hljs-built_in">float</span>(running_loss/(idx+<span class="hljs-number">1</span>)))<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;acc:<span class="hljs-subst">&#123;running_acc/(idx+<span class="hljs-number">1</span>)&#125;</span>&#x27;</span>)<br>            accs.append(running_acc/(idx+<span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure><pre><code class="hljs">Epoch: 1/200... Step: 1... Loss: 1.412927...
acc:0.46875
Epoch: 1/200... Step: 11... Loss: 1.417399...
acc:0.47159090638160706
Epoch: 1/200... Step: 21... Loss: 1.408894...
acc:0.480654776096344
Epoch: 1/200... Step: 31... Loss: 1.404616...
acc:0.4868951737880707
Epoch: 2/200... Step: 33... Loss: 1.388400...
acc:0.5
Epoch: 2/200... Step: 43... Loss: 1.389541...
acc:0.5
Epoch: 2/200... Step: 53... Loss: 1.389531...
acc:0.5104166865348816</code></pre><h4 id="代码解释-2">代码解释</h4><p>这段代码是一个典型的深度学习训练循环，用于训练一个名为<code>net</code>的神经网络。代码的主要组成部分是设置训练参数、定义损失函数、配置优化器以及执行训练循环。这里是每个部分的详细解释：</p><ol type="1"><li><p><strong>初始化训练参数：</strong></p><ul><li><code>net.train()</code>：确保处于训练模式，特别是对于包含dropout或批归一化层的网络。</li><li><code>clip=5</code>：设置梯度裁剪的最大阈值，防止梯度爆炸问题。</li><li><code>epochs = 200</code>：将网络训练200个时代（epochs）。</li><li><code>counter = 0</code>：用以计数训练步骤总数。</li><li><code>print_every = 100</code>：每训练100个batch，打印一次训练信息。</li><li><code>lr=0.01</code>：设置优化器的学习率为0.01。</li></ul></li><li><p><strong>定义自定义损失函数：</strong></p><p>在上述代码中，自定义的损失函数<code>criterion</code>用于计算分类任务中的交叉熵损失。这个函数接受两个参数：<code>input</code>和<code>target</code>，以及一个可选的参数<code>size_average</code>，详细解释如下：</p><p>（1） 参数：</p><ul><li><code>input</code>：模型的原始输出（logits），在多分类问题中通常是一个二维张量，形状为 <code>(batch_size, num_classes)</code>，表示批量数据的类别未归一化的预测分数。</li><li><code>target</code>：实际标签的one-hot表示，形状与<code>input</code>相同，每一行对应于一个样本的类别标签的one-hot编码。</li><li><code>size_average</code>：布尔值，决定最终损失是否要在样本上取平均。默认值<code>True</code>表示计算损失的平均值，而<code>False</code>表示损失的总和。</li></ul><p>（2） 损失计算流程：</p><pre><code class="hljs">  1. `F.softmax(input, dim=1)`: 对`input`应用Softmax函数，以每个样本的预测分数转换成概率分布。Softmax作用在第一个维度上（`dim=1`），即将每一个样本的logits归一化成概率值。
  2. `+ 1e-10`: 在应用对数函数前，加上一个小的常数`1e-10`，防止数值不稳定问题，尤其是防止当概率值为零时对数无法计算的情况。
  3. `torch.log(...)`: 对Softmax的结果取对数。因为交叉熵损失涉及到概率的对数，所以需要这一步。
  4. `-(target * ...)`: 将目标one-hot向量与对数概率相乘。在one-hot编码中，正确标签的位置为1，其余位置为0，因此这一操作将选出每个样本正确类别的对数概率。
  5. `.sum(1)`: 对每个样本的结果按类别求和，由于正确标签的位置为1其他为0，因此每个样本只计算了正确类别的负对数似然。
  6. `if size_average ... else ...`: 根据`size_average`参数的值，决定是计算损失的平均值（`.mean()`）还是总和（`.sum()`）。
  7. `return l`: 返回计算后的损失值。</code></pre></li><li><p><strong>配置优化器：</strong></p><ul><li><code>optimizer</code>：一个使用Adam优化算法和指定学习率来更新<code>net</code>参数的优化器。</li></ul></li><li><p><strong>训练过程：</strong></p></li></ol><ul><li>初始化和构建训练数据：</li></ul><ol type="1"><li><p><code>a = np.random.choice(len(train)-1, 1000)</code>: 从<code>train</code>数据集中随机选择1000个样本（不包括最后一个）。<code>len(train)-1</code>确保在选择时不会超出索引范围。</p></li><li><p><code>train_set = PhraseDataset(train.loc[train.index.isin(np.sort(a))], pad_sequences[a])</code>: 使用选中的样本索引来创建<code>train_set</code>，一个PyTorch数据集。<code>PhraseDataset</code>是一个自定义数据集类，用于存储训练数据和对应的序列化表示（<code>pad_sequences[a]</code>）。</p></li><li><p><code>train_loader = DataLoader(train_set, batch_size=32, shuffle=True)</code>: 利用<code>DataLoader</code>将<code>train_set</code>封装成一个可迭代的数据加载器，每次迭代返回一批数据。参数<code>batch_size=32</code>指明每批次包含32个样本，<code>shuffle=True</code>表示在每个epoch开始时，数据将被打乱。</p></li></ol><ul><li>训练循环：</li></ul><p>对于每一个epoch（一个epoch表示遍历一次完整的数据集）：</p><ol start="4" type="1"><li><p><code>h = net.init_hidden(32)</code>: 初始化网络的隐藏状态。这对于训练RNNs（递归神经网络）或LSTMs（长短期记忆网络）是必要的。</p></li><li><p><code>running_loss = 0.0</code> 和 <code>running_acc = 0.0</code>: 初始化变量用于累计一个epoch中所有批次的损失和准确率。</p></li></ol><p>接下来，对于每一个批次：</p><ol start="6" type="1"><li><code>gc.collect()</code>: Python的垃圾收集器，用来尝试释放内存中未使用的对象。</li></ol><p>每个batch循环开始：</p><ol start="7" type="1"><li><p><code>h = tuple([each.data for each in h])</code>: 分离隐藏状态，以免在整个训练历史中进行反向传播。</p></li><li><p><code>optimizer.zero_grad()</code>: 清零之前的梯度，否则梯度将会累加到已有的梯度上。</p></li><li><p><code>if inputs.shape[0] != batch_size: break</code>: 如果当前批次的大小不等于预设的batch_size，则跳过该批次。</p></li><li><p><code>output, h = net(inputs, h)</code>: 通过模型进行前向传播，得到预测输出和新的隐藏状态。</p></li><li><p><code>labels = torch.nn.functional.one_hot(labels, num_classes=5)</code>: 将标签转换为one-hot编码，总共有5个类别。</p></li><li><p><code>loss = criterion(output, labels)</code>: 计算模型输出和实际标签之间的损失。</p></li><li><p><code>loss.backward()</code>: 进行反向传播，计算损失对模型参数的梯度。</p></li><li><p><code>running_loss += loss.cpu().detach().numpy()</code>: 将当前batch的损失累加到总损失上。</p></li><li><p><code>running_acc += (output.argmax(dim=1) == labels.argmax(dim=1)).float().mean()</code>: 计算并累加当前batch的准确率。</p></li><li><p><code>nn.utils.clip_grad_norm_(net.parameters(), clip)</code>: 对梯度进行裁剪，以防止梯度爆炸。</p></li><li><p><code>optimizer.step()</code>: 更新网络参数。</p></li></ol><ul><li>打印和记录训练进度：</li></ul><ol start="18" type="1"><li><p><code>if idx % print_every == 0</code>: 满足条件时，即经过<code>print_every</code>设置的数目的批次后，打印当前训练的状态。</p></li><li><p>打印包含当前epoch、步数、损失的字符串。</p></li><li><p><code>losses.append(float(running_loss/(idx+1)))</code> 和 <code>accs.append(running_acc/(idx+1))</code>: 将平均损失和平均准确率记录到列表中，用于之后分析模型训练过程的性能。</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>plt.plot(losses)<br>plt.show()<br>plt.plot(accs)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/img/电影评论情感分析/output_19_0.png" srcset="/img/loading.gif" lazyload></p><p><img src="/img/电影评论情感分析/output_19_1.png" srcset="/img/loading.gif" lazyload></p></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E5%85%A5%E9%97%A8/" class="category-chain-item">深度学习实战入门</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">#深度学习</a></div></div><div class="license-box my-3"><div class="license-title"><div>文本分类-电影评论情感分析</div><div>http://zhou1317fe5.link/2024/02/26/文本分类-电影评论情感分析/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>Zhou1317fe5</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2024年2月26日</div></div><div class="license-meta-item"><div>许可协议</div><div><a target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"></article><article class="post-next col-6"><a href="/2024/02/12/Jupyter-Notebook-%E6%97%A0%E6%B3%95%E8%B7%B3%E8%BD%AC%E7%BD%91%E9%A1%B5%E7%9A%84%E9%97%AE%E9%A2%98/" title="Jupyter Notebook 无法跳转网页的问题"><span class="hidden-mobile">Jupyter Notebook 无法跳转网页的问题</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div><div class="statistics"><span id="busuanzi_container_site_pv" style="display:none">总访问量 <span id="busuanzi_value_site_pv"></span> 次 </span><span id="busuanzi_container_site_uv" style="display:none">总访客数 <span id="busuanzi_value_site_uv"></span> 人</span></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script>window.MathJax?(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise()):window.MathJax={tex:{inlineMath:{"[+]":[["$","$"]]}},loader:{load:["ui/lazy"]},options:{renderActions:{insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(t=>{let e=t.parentNode;"li"===e.nodeName.toLowerCase()&&e.parentNode.classList.add("has-jax")})},"",!1]}}},Fluid.events.registerRefreshCallback((function(){"MathJax"in window&&MathJax.startup.document&&"function"==typeof MathJax.startup.document.state&&(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise())}))</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script src="/js/local-search.js"></script><script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>