<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png"><link rel="icon" href="/img/fluid.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="Zhou1317fe5"><meta name="keywords" content=""><meta name="description" content="吴恩达深度学习笔记 第五门课序列模型 第一周序列模型"><meta property="og:type" content="article"><meta property="og:title" content="深度学习-Course5-Week1循环序列模型"><meta property="og:url" content="http://zhou1317fe5.link/2024/02/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course5-Week1%E5%BE%AA%E7%8E%AF%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/index.html"><meta property="og:site_name" content="Zhou1317fe5"><meta property="og:description" content="吴恩达深度学习笔记 第五门课序列模型 第一周序列模型"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://zhou1317fe5.link/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course5-Week1/1%E5%BE%AA%E7%8E%AF%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B.png"><meta property="og:image" content="http://zhou1317fe5.link/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course5-Week1/1%E5%BE%AA%E7%8E%AF%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B_1.png"><meta property="og:image" content="http://zhou1317fe5.link/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course5-Week1/1%E5%BE%AA%E7%8E%AF%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B_2.png"><meta property="og:image" content="http://zhou1317fe5.link/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course5-Week1/1%E5%BE%AA%E7%8E%AF%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B_3.png"><meta property="og:image" content="http://zhou1317fe5.link/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course5-Week1/rnn_step_forward_figure2_v3a.png"><meta property="og:image" content="http://zhou1317fe5.link/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course5-Week1/1%E5%BE%AA%E7%8E%AF%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B_4.png"><meta property="og:image" content="http://zhou1317fe5.link/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course5-Week1/1%E5%BE%AA%E7%8E%AF%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B_5.png"><meta property="og:image" content="http://zhou1317fe5.link/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course5-Week1/1%E5%BE%AA%E7%8E%AF%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B_5.png"><meta property="og:image" content="http://zhou1317fe5.link/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course5-Week1/1%E5%BE%AA%E7%8E%AF%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B_6.png"><meta property="og:image" content="http://zhou1317fe5.link/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course5-Week1/1%E5%BE%AA%E7%8E%AF%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B_7.png"><meta property="og:image" content="http://zhou1317fe5.link/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course5-Week1/1%E5%BE%AA%E7%8E%AF%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B_8.png"><meta property="og:image" content="http://zhou1317fe5.link/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course5-Week1/1%E5%BE%AA%E7%8E%AF%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B_12.png"><meta property="og:image" content="http://zhou1317fe5.link/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course5-Week1/1%E5%BE%AA%E7%8E%AF%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B_14.png"><meta property="og:image" content="http://zhou1317fe5.link/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course5-Week1/1%E5%BE%AA%E7%8E%AF%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B_15.png"><meta property="article:published_time" content="2024-02-10T01:46:34.000Z"><meta property="article:modified_time" content="2024-02-10T01:48:59.373Z"><meta property="article:author" content="Zhou1317fe5"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="深度学习"><meta property="article:tag" content="神经网络"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="http://zhou1317fe5.link/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course5-Week1/1%E5%BE%AA%E7%8E%AF%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B.png"><meta name="referrer" content="no-referrer-when-downgrade"><title>深度学习-Course5-Week1循环序列模型 - Zhou1317fe5</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="/css/iconfont_csdn/iconfont.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"zhou1317fe5.link",root:"/",version:"1.9.4",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1}},search_path:"/local-search.xml"};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><meta name="generator" content="Hexo 6.3.0"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>Zhou1317fe5</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/Post_banner_img.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="深度学习-Course5-Week1循环序列模型"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2024-02-10 09:46" pubdate>2024年2月10日 上午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 8.2k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 69 分钟 </span><span id="busuanzi_container_page_pv" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="busuanzi_value_page_pv"></span> 次</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 style="display:none">深度学习-Course5-Week1循环序列模型</h1><div class="markdown-body"><h1 id="序列模型">1 序列模型</h1><p>序列模型能够应用在许多领域，例如：</p><ul><li>语音识别</li><li>音乐发生器</li><li>情感分类</li><li>DNA序列分析</li><li>机器翻译</li><li>视频动作识别</li><li>名称实体识别</li></ul><p><img src="/img/深度学习-Course5-Week1/1循环序列模型.png" srcset="/img/loading.gif" lazyload></p><p>这些序列模型基本都属于监督式学习，但输入x和输出y不一定都是序列模型。</p><p>如果都是序列模型的话，模型输入长度，输出长度也不一定完全一致。</p><h1 id="名称实体识别">2 名称实体识别</h1><p>下面以名称实体识别为例，介绍序列模型的命名规则，示例语句为：</p><ul><li>Harry Potter and Hermione Granger invented a new spell.</li></ul><p>该句话包含9个单词，输出y即为1 x 9向量，每位表征对应单词是否为人名的一部分，1表示是，0表示否。</p><p>很明显，该句话中“Harry”， “Potter”， “Hermione”， “Granger”均是人名成分，所以，对应的输出y可表示为：</p><p><span class="math display">\[y=\begin{bmatrix}1&amp;1&amp;0&amp;1&amp;1&amp;0&amp;0&amp;0&amp;0\end{bmatrix}\]</span></p><p>一般约定使用<span class="math inline">\(y^{&lt;t&gt;}\)</span>表示序列对应位置的输出，使用<span class="math inline">\(T_y\)</span>表示输出序列长度，则<span class="math inline">\(1\leq t\leq T_y\)</span>。</p><p>对于输入x，表示为： <span class="math display">\[\begin{bmatrix}x^{&lt;1&gt;}&amp;x^{&lt;2&gt;}&amp;x^{&lt;3&gt;}&amp;x^{&lt;4&gt;}&amp;x^{&lt;5&gt;}&amp;x^{&lt;6&gt;}&amp;x^{&lt;7&gt;}&amp;x^{&lt;8&gt;}&amp;x^{&lt;9&gt;}\end{bmatrix}\]</span></p><p>同样，<span class="math inline">\(x^{&lt;t&gt;}\)</span>表示序列对应位置的输入，<span class="math inline">\(T_x\)</span>表示输入序列长度。注意，此例中，<span class="math inline">\(T_x=T_y\)</span>,但是也存在<span class="math inline">\(T_x\neq T_y\)</span>的情况。</p><p>如何来表示每个<span class="math inline">\(x^{&lt;t&gt;}\)</span>呢？方法是首先建立一个词汇库vocabulary，尽可能包含更多的词汇。例如一个包含10000个词汇的词汇库为： <img src="/img/深度学习-Course5-Week1/1循环序列模型_1.png" srcset="/img/loading.gif" lazyload></p><p>该词汇库可看成是 10000 x 1 的向量。</p><p>然后，使用one-hot编码，例句中的每个单词<span class="math inline">\(x^{&lt;t&gt;}\)</span>都可以表示成10000 x 1的向量，词汇表中与<span class="math inline">\(x^{&lt;t&gt;}\)</span>对应的位置为1，其它位置为0。该<span class="math inline">\(x^{&lt;t&gt;}\)</span>为one-hot向量。如果出现词汇表之外的单词，可以使用UNK或其他字符串来表示。</p><p>对于多样本，以上序列模型对应的命名规则可表示为：<span class="math inline">\(X^{(i)&lt;t&gt;}\)</span>，<span class="math inline">\(y^{(i)&lt;t&gt;}\)</span>，<span class="math inline">\(T_x^{(i)}\)</span>，<span class="math inline">\(T_y^{(i)}\)</span>。其中，<span class="math inline">\(i\)</span>表示第i个样本。不同样本的<span class="math inline">\(T_x^{(i)}\)</span>或<span class="math inline">\(T_y^{(i)}\)</span>都有可能不同。</p><h1 id="rnn">3 RNN</h1><p>标准的神经网络不适合解决序列模型问题，而循环神经网络（RNN）是专门用来解决序列模型问题的。</p><p>RNN模型结构如下：</p><p><img src="/img/深度学习-Course5-Week1/1循环序列模型_2.png" srcset="/img/loading.gif" lazyload></p><p>序列模型从左到右，依次传递，此例中，<span class="math inline">\(T_x=T_y\)</span>。<span class="math inline">\(x^{&lt;t&gt;}\)</span>到<span class="math inline">\(\hat{y}^{&lt;t&gt;}\)</span>之间是隐藏神经元。<span class="math inline">\(a^{&lt;t&gt;}\)</span>会传入到第<span class="math inline">\(t+1\)</span>个元素中，作为输入。其中，<span class="math inline">\(a^{&lt;0&gt;}\)</span>一般为零向量。</p><p>RNN模型包含三类权重系数，分别是<span class="math inline">\(Wax\)</span>，<span class="math inline">\(Waa\)</span>，<span class="math inline">\(Wya\)</span>。且不同元素之间同一位置共享同一权重系数。 <img src="/img/深度学习-Course5-Week1/1循环序列模型_3.png" srcset="/img/loading.gif" lazyload></p><p><img src="/img/深度学习-Course5-Week1/rnn_step_forward_figure2_v3a.png" srcset="/img/loading.gif" lazyload></p><p>RNN的正向传播（Forward Propagation）过程为：</p><p><span class="math display">\[\begin{gathered}a^{&lt;t&gt;}=g(W_{aa}\cdot a^{&lt;t-1&gt;}+W_{ax}\cdot x^{&lt;t&gt;}+ba)\\\hat{y}^{&lt;t&gt;}=g(W_{ya}\cdot a^{&lt;t&gt;}+b_y)\end{gathered}\]</span></p><p>其中，g(⋅)表示激活函数，不同的问题需要使用不同的激活函数。</p><p>为了简化表达式，可以对上式进行整合：</p><p><span class="math display">\[\left.W_{aa}\cdot a^{&lt;t-1&gt;}+W_{ax}\cdot x^{&lt;t&gt;}=[W_{aa}W_{ax}]\left[\begin{array}{c}a^{&lt;t-1&gt;}\\x^{&lt;t&gt;}\end{array}\right.\right]\to W_a[a^{&lt;t-1&gt;},x^{&lt;t&gt;}]\]</span></p><p>则正向传播可表示为： <span class="math display">\[\begin{gathered}a^{&lt;t&gt;}=g(W_a[a^{&lt;t-1&gt;},x^{&lt;t&gt;}]+b_a)\\\hat{y}^{&lt;t&gt;}=g(W_y\cdot a^{&lt;t&gt;}+b_y)\end{gathered}\]</span></p><p>针对上面识别人名的例子，经过RNN正向传播，单个元素的Loss function为：</p><p><span class="math display">\[L^{&lt;t&gt;}(\hat{y}^{&lt;t&gt;},y^{&lt;t&gt;})=-y^{&lt;t&gt;}log\hat{y}^{&lt;t&gt;}-(1-y^{&lt;t&gt;})log(1-\hat{y}^{&lt;t&gt;})\]</span></p><p>该样本所有元素的Loss function为：</p><p><span class="math display">\[L(\hat{y},y)=\sum_{t=1}^{T_y}L^{&lt;t&gt;}(\hat{y}^{&lt;t&gt;},y^{&lt;t&gt;})\]</span></p><p>然后，反向传播（Backpropagation）过程就是从右到左分别计算L(y^,y)对参数Wa，Wy，ba，by的偏导数。思路与做法与标准的神经网络是一样的。一般可以通过成熟的深度学习框架自动求导，例如PyTorch、Tensorflow等。</p><p>这种从右到左的求导过程被称为Backpropagation through time。</p><h1 id="不同种类rnn">4 不同种类RNN</h1><p>以上介绍的例子中，Tx=Ty。但是在很多RNN模型中，Tx是不等于Ty的。</p><p>根据Tx与Ty的关系，RNN模型包含以下几个类型：</p><ul><li>One to one: Tx=1,Ty=1Tx=1,Ty=1</li><li>One to many: Tx=1,Ty&gt;1Tx=1,Ty&gt;1</li><li>Many to one: Tx&gt;1,Ty=1Tx&gt;1,Ty=1</li><li>Many to many: Tx=TyTx=Ty<br></li><li>Many to many: Tx≠TyTx≠Ty</li></ul><p>不同类型相应的示例结构如下： <img src="/img/深度学习-Course5-Week1/1循环序列模型_4.png" srcset="/img/loading.gif" lazyload></p><p>Many to many: Tx≠TyTx≠Ty 有文本翻译等。</p><h1 id="语言模型和序列生成">5 语言模型和序列生成</h1><p>语言模型是自然语言处理（NLP）中最基本和最重要的任务之一。</p><p>使用RNN能够很好地建立需要的不同语言风格的语言模型。</p><p>什么是语言模型呢？举个例子，在语音识别中，某句语音有两种翻译：</p><ul><li><p>The apple and pair salad.</p></li><li><p>The apple and pear salad.</p></li></ul><p>很明显，第二句话更有可能是正确的翻译。</p><p>语言模型实际上会计算出这两句话各自的出现概率。比如第一句话概率为<span class="math inline">\(10^{−13}\)</span>，第二句话概率为<span class="math inline">\(10^{−10}\)</span>。</p><p>也就是说，利用语言模型得到各自语句的概率，选择概率最大的语句作为正确的翻译。</p><p>概率计算的表达式为： <span class="math display">\[P(y^{&lt;1&gt;},y^{&lt;2&gt;},\cdots,y^{&lt;Ty&gt;})\]</span> 如何使用RNN构建语言模型？</p><p>首先，我们需要一个足够大的训练集，训练集由大量的单词语句语料库（corpus）构成。</p><p>然后，对corpus的每句话进行切分词（tokenize）。做法就跟第2节介绍的一样，建立vocabulary，对每个单词进行one-hot编码。</p><p>例如下面这句话：</p><ul><li>The Egyptian Mau is a bread of cat.</li></ul><p>One-hot编码已经介绍过了，不再赘述。</p><p>还需注意的是，每句话结束末尾，需要加上&lt; EOS &gt;作为语句结束符。</p><p>另外，若语句中有词汇表中没有的单词，用&lt; UNK &gt;表示。假设单词“Mau”不在词汇表中，则上面这句话可表示为：</p><ul><li>The Egyptian &lt; UNK &gt; is a bread of cat. &lt; EOS &gt;</li></ul><p>准备好训练集并对语料库进行切分词等处理之后，接下来构建相应的RNN模型。 <img src="/img/深度学习-Course5-Week1/1循环序列模型_5.png" srcset="/img/loading.gif" lazyload></p><p>语言模型的RNN结构如上图所示，<span class="math inline">\(x^{&lt;1&gt;}\)</span>和<span class="math inline">\(a^{&lt;0&gt;}\)</span>均为零向量。Softmax输出层<span class="math inline">\(\hat{y}^{&lt;1&gt;}\)</span>表示出现该语句第一个单词的概率，softmax输出层<span class="math inline">\(\hat{y}^{&lt;2&gt;}\)</span>表示在第一个单词基础上出现第二个单词的概率，即条件概率，以此类推，最后是出现&lt; EOS &gt;的条件概率。</p><p>单个元素的softmax loss function为： <span class="math display">\[L^{&lt;t&gt;}(\hat{y}^{&lt;t&gt;},y^{&lt;t&gt;})=-\sum_iy_i^{&lt;t&gt;}log\hat{y}_i^{&lt;t&gt;}\]</span> 该样本所有元素的Loss function为： <span class="math display">\[L(\hat{y},y)=\sum_tL^{&lt;t&gt;}(\hat{y}^{&lt;t&gt;},y^{&lt;t&gt;})\]</span> 对语料库的每条语句进行RNN模型训练，最终得到的模型可以根据给出语句的前几个单词预测其余部分，将语句补充完整。</p><p>例如给出“Cats average 15”，RNN模型可能预测完整的语句是“Cats average 15 hours of sleep a day.”。</p><p>最后补充一点，整个语句出现的概率等于语句中所有元素出现的条件概率乘积。例如某个语句包含<span class="math inline">\(y^{&lt;1&gt;},y^{&lt;2&gt;},y^{&lt;3&gt;}\)</span>，则整个语句出现的概率为： <span class="math display">\[P(y^{&lt;1&gt;},y^{&lt;2&gt;},y^{&lt;3&gt;})=P(y^{&lt;1&gt;})\cdot P(y^{&lt;2&gt;}|y^{&lt;1&gt;})\cdot P(y^{&lt;3&gt;}|y^{&lt;1&gt;},y^{&lt;2&gt;})\]</span></p><h1 id="序列采样">6 序列采样</h1><p>利用训练好的RNN语言模型，可以进行新的序列采样，从而随机产生新的语句。</p><p>与上一节介绍的一样，相应的RNN模型如下所示： <img src="/img/深度学习-Course5-Week1/1循环序列模型_5.png" srcset="/img/loading.gif" lazyload></p><p>首先，从第一个元素输出<span class="math inline">\(\hat{y}^{&lt;1&gt;}\)</span>的softmax分布中随机选取一个word作为新语句的首单词。然后，<span class="math inline">\(y^{&lt;1&gt;}\)</span>作为<span class="math inline">\(x^{&lt;2&gt;}\)</span> ,得到<span class="math inline">\(\hat{y}^{&lt;1&gt;}\)</span>的softmax分布。从中选取概率最大的word作为<span class="math inline">\(y^{&lt;2&gt;}\)</span>,继续将<span class="math inline">\(y^{&lt;2&gt;}\)</span>作为<span class="math inline">\(x^{&lt;3&gt;}\)</span>,以此类推。直到产生&lt; EOS&gt;结束符，则标志语句生成完毕。当然，也可以设定语句长度上限，达到长度上限即停止生成新的单词。最终，根据随机选择的首单词，RNN模型会生成一条新的语句。</p><p>值得一提的是，如果不希望新的语句中包含&lt; UNK &gt;标志符，可以在每次产生&lt; UNK &gt;时重新采样，直到生成非&lt; UNK &gt;标志符为止。</p><p>以上介绍的是word level RNN，即每次生成单个word，语句由多个words构成。</p><p>另外一种情况是character level RNN，即词汇表由单个英文字母或字符组成，如下所示：</p><p><span class="math display">\[Vocabulay=[a,b,c,\cdots,z,.,;,,0,1,\cdots,9,A,B,\cdots,Z]\]</span> Character level RNN与word level RNN不同的是，<span class="math inline">\(y^{&lt;t&gt;}\)</span>由单个字符组成而不是word。训练集中的每句话都当成是由许多字符组成的。</p><p>character level RNN的优点是能有效避免遇到词汇表中不存在的单词&lt; UNK &gt;。</p><p>但是，character level RNN的缺点也很突出。由于是字符表征，每句话的字符数量很大，这种大的跨度不利于寻找语句前部分和后部分之间的依赖性。另外，character level RNN的在训练时的计算量也是庞大的。</p><p>基于这些缺点，目前character level RNN的应用并不广泛，但是在特定应用下仍然有发展的趋势。</p><h1 id="rnns-梯度消失和爆炸">7 RNNs 梯度消失和爆炸</h1><p>语句中可能存在跨度很大的依赖关系，即某个word可能与它距离较远的某个word具有强依赖关系。例如下面这两条语句：</p><ul><li><p>The cat, which already ate fish, was full.</p></li><li><p>The cats, which already ate fish, were full.</p></li></ul><p>第一句话中，was受cat影响；第二句话中，were受cats影响。它们之间都跨越了很多单词。</p><p>而一般的RNN模型每个元素受其周围附近的影响较大，难以建立跨度较大的依赖性。</p><p>上面两句话的这种依赖关系，由于跨度很大，普通的RNN网络容易出现梯度消失，捕捉不到它们之间的依赖，造成语法错误。</p><p>另一方面，RNN也可能出现梯度爆炸的问题，即gradient过大。梯度爆炸是指在训练过程中，大量误差梯度累积，导致对 NN 模型权重的更新非常大。这些权重可能会变得过大，导致溢出，识别为 NaN（"Not a Number"）</p><p>常用的解决办法是设定一个阈值，一旦梯度最大值达到这个阈值，就对整个梯度向量进行尺度缩小。这种做法被称为gradient clipping。</p><h1 id="gru">8 GRU</h1><p>RNN的隐藏层单元结构如下图所示：</p><p><img src="/img/深度学习-Course5-Week1/1循环序列模型_6.png" srcset="/img/loading.gif" lazyload></p><p><span class="math inline">\(a^{&lt;t&gt;}\)</span> 的表达式为: <span class="math display">\[a^{&lt;t&gt;}=tanh(W_a[a^{&lt;t-1&gt;},x^{&lt;t&gt;}]+b_a)\]</span></p><p>为了解决梯度消失问题，对上述单元进行修改，添加了记忆单元，构建GRU，如下图所示： <img src="/img/深度学习-Course5-Week1/1循环序列模型_7.png" srcset="/img/loading.gif" lazyload> 相应的表达式为： <span class="math display">\[\begin{aligned}\tilde{c}^{&lt;t&gt;} &amp; =tanh(W_{c}[c^{&lt;t-1&gt;},x^{&lt;t&gt;}]+b_{c})\\ \Gamma_{u} &amp; =\sigma(W_{u}[c^{&lt;t-1&gt;},x^{&lt;t&gt;}]+b_{u})\\ c^{&lt;t&gt;} &amp; =\Gamma*\tilde{c}^{&lt;t&gt;}+(1-\Gamma_{u})*c^{&lt;t-1&gt;}\end{aligned}\]</span> 其中，<span class="math inline">\(c^{&lt;t-1&gt;}=a^{&lt;t-1&gt;}\)</span>，<span class="math inline">\(c^{&lt;t&gt;}=a^{&lt;t&gt;}\)</span>。<span class="math inline">\(\Gamma_u\)</span>意为gate，记忆单元。当<span class="math inline">\(\Gamma_u=1\)</span>时，代表更新；当 <span class="math inline">\(\Gamma_u=0\)</span>时，代表记忆，保留之前的模块输出。这一点跟CNN中的ResNets的作用有点类似。因此，<span class="math inline">\(\Gamma_u\)</span>能够保证RNN模型中跨度很大的依赖关系不受影响，消除梯度消失问题。</p><p>上面介绍的是简化的GRU模型，完整的GRU添加了另外一个gate，即Γr，表达式如下：</p><p><span class="math display">\[\begin{gathered}\widetilde{c}^{&lt;t&gt;}=tanh(W_{c}[\Gamma_{r}*c^{&lt;t-1&gt;},x^{&lt;t&gt;}]+b_{c})\\ \Gamma_{u}=\sigma(W_{u}[c^{&lt;t-1&gt;},x^{&lt;t&gt;}]+b_{u})\\ \Gamma_{r}=\sigma(W_{r}[c^{&lt;t-1&gt;},x^{&lt;t&gt;}]+b_{r})\\ c^{&lt;t&gt;}=\Gamma_{}*\tilde{c}^{&lt;t&gt;}+\left(1-\Gamma_{u}\right)*c^{&lt;t-1&gt;}\\ a^{&lt;t&gt;}=c^{&lt;t&gt;}\\ \end{gathered}\]</span> 注意，以上表达式中的∗表示元素相乘，而非矩阵相乘。</p><p>GRU可以看成是简化的LSTM，两种方法都具有各自的优势。</p><h1 id="lstm">9 LSTM</h1><p>LSTM是另一种更强大的解决梯度消失问题的方法。它对应的RNN隐藏层单元结构如下图所示： <img src="/img/深度学习-Course5-Week1/1循环序列模型_8.png" srcset="/img/loading.gif" lazyload> 相应的表达式为： <span class="math display">\[\begin{gathered} \widetilde{c}^{&lt;t&gt;}=tanh(W_c[a^{&lt;t-1&gt;},x^{&lt;t&gt;}]+b_c) \\ \Gamma_u=\sigma(W_u[a^{&lt;t-1&gt;},x^{&lt;t&gt;}]+b_u) \\ \Gamma_f=\sigma(W_f[a^{&lt;t-1&gt;},x^{&lt;t&gt;}]+b_f) \\ \Gamma_o=\sigma(W_o[a^{&lt;t-1&gt;},x^{&lt;t&gt;}]+b_o) \\ c^{&lt;t&gt;}=\Gamma_{u}*\tilde{c}^{&lt;t&gt;}+\Gamma_{f}*c^{&lt;t-1&gt;} \\ a^{&lt;t&gt;}=\Gamma_o*c^{&lt;t&gt;} \end{gathered}\]</span> LSTM包含三个门：<span class="math inline">\(\Gamma_u\)</span>，<span class="math inline">\(\Gamma_f\)</span>，<span class="math inline">\(\Gamma_o\)</span>分别对应更新门（update gate），遗忘门（forget gate）和输出门（output gate）。</p><p>如果考虑<span class="math inline">\(c^{&lt;t−1&gt;}\)</span>对<span class="math inline">\(\Gamma_u\)</span>，<span class="math inline">\(\Gamma_f\)</span>，<span class="math inline">\(\Gamma_o\)</span>的影响，可加入peephole connection，对LSTM的表达式进行修改： <span class="math display">\[\begin{gathered}\tilde{c}^{&lt;t&gt;}=tanh(W_{c}[a^{&lt;t-1&gt;},x^{&lt;t&gt;}]+b_{c})\\ \Gamma_{u}=\sigma(W_{u}[a^{&lt;t-1&gt;},x^{&lt;t&gt;},c^{&lt;t-1&gt;}]+b_{u})\\ \Gamma_{f}=\sigma(W_{f}[a^{&lt;t-1&gt;},x^{&lt;t&gt;},c^{&lt;t-1&gt;}]+b_{f})\\ \Gamma_{o}=\sigma(W_{o}[a^{&lt;t-1&gt;},x^{&lt;t&gt;},c^{&lt;t-1&gt;}]+b_{o})\\ c^{&lt;t&gt;}=\Gamma_{u}*\tilde{c}^{&lt;t&gt;}+\Gamma_{f}*c^{&lt;t-1&gt;}\\ a^{&lt;t&gt;}=\Gamma_{o}*c^{&lt;t&gt;}\end{gathered}\]</span></p><p><span class="math inline">\(\Gamma_u\)</span>维度等于 LSTM 隐藏单元数量.</p><h1 id="双向rnn">10 双向RNN</h1><p>我们在前面简单提过Bidirectional RNN，它的结构如下图所示： <img src="/img/深度学习-Course5-Week1/1循环序列模型_12.png" srcset="/img/loading.gif" lazyload> BRNN对应的输出表达式为： <span class="math display">\[\hat{y}^{&lt;t&gt;}=g(W_y[a^{\to&lt;t&gt;},a^{\leftarrow&lt;t&gt;}]+b_y)\]</span> BRNN能够同时对序列进行双向处理，性能大大提高。</p><p>但是计算量较大，且在处理实时语音时，需要等到完整的一句话结束时才能进行分析。</p><h1 id="deep-rnns">11 Deep RNNs</h1><p>Deep RNNs由多层RNN组成，其结构如下图所示。 <img src="/img/深度学习-Course5-Week1/1循环序列模型_14.png" srcset="/img/loading.gif" lazyload> 与RNN一样，用上标<span class="math inline">\([l]\)</span>表示层数，Deep RNNs中 <span class="math inline">\(a^{[l]&lt;t&gt;}\)</span> 的表达式为：</p><p><span class="math display">\[a^{[l]&lt;t&gt;}=g(W_a^{[l]}[a^{[l]&lt;t-1&gt;},a^{[l-1]&lt;t&gt;}]+b_a^{[l]})\]</span> 我们知道DNN层数可达100多，而Deep RNNs一般没有那么多层，3层RNNs已经较复杂了。</p><p>另外一种Deep RNNs结构是每个输出层上还有一些垂直单元，如下图所示。 <img src="/img/深度学习-Course5-Week1/1循环序列模型_15.png" srcset="/img/loading.gif" lazyload></p></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="category-chain-item">深度学习</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">#机器学习</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">#深度学习</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">#神经网络</a></div></div><div class="license-box my-3"><div class="license-title"><div>深度学习-Course5-Week1循环序列模型</div><div>http://zhou1317fe5.link/2024/02/10/深度学习-Course5-Week1循环序列模型/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>Zhou1317fe5</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2024年2月10日</div></div><div class="license-meta-item"><div>许可协议</div><div><a target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2024/02/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course5-Week2%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%8E%E8%AF%8D%E5%B5%8C%E5%85%A5/" title="深度学习-Course5-Week2自然语言处理与词嵌入"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">深度学习-Course5-Week2自然语言处理与词嵌入</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2024/02/09/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Course3-Week2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AD%96%E7%95%A5-2/" title="深度学习-Course3-Week2机器学习策略(2)"><span class="hidden-mobile">深度学习-Course3-Week2机器学习策略(2)</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div><div class="statistics"><span id="busuanzi_container_site_pv" style="display:none">总访问量 <span id="busuanzi_value_site_pv"></span> 次 </span><span id="busuanzi_container_site_uv" style="display:none">总访客数 <span id="busuanzi_value_site_uv"></span> 人</span></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script>window.MathJax?(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise()):window.MathJax={tex:{inlineMath:{"[+]":[["$","$"]]}},loader:{load:["ui/lazy"]},options:{renderActions:{insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(t=>{let e=t.parentNode;"li"===e.nodeName.toLowerCase()&&e.parentNode.classList.add("has-jax")})},"",!1]}}},Fluid.events.registerRefreshCallback((function(){"MathJax"in window&&MathJax.startup.document&&"function"==typeof MathJax.startup.document.state&&(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise())}))</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script src="/js/local-search.js"></script><script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>