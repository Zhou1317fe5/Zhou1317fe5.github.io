<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png"><link rel="icon" href="/img/fluid.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="Zhou1317fe5"><meta name="keywords" content=""><meta name="description" content="天猫重复购买预测-04模型训练、验证和评测代码详解"><meta property="og:type" content="article"><meta property="og:title" content="天猫重复购买预测-04模型训练、验证和评测"><meta property="og:url" content="https://zhou1317fe5.github.io/2023/12/15/%E5%A4%A9%E7%8C%AB%E9%87%8D%E5%A4%8D%E8%B4%AD%E4%B9%B0%E9%A2%84%E6%B5%8B-04%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E3%80%81%E9%AA%8C%E8%AF%81%E5%92%8C%E8%AF%84%E6%B5%8B/index.html"><meta property="og:site_name" content="Zhou1317fe5"><meta property="og:description" content="天猫重复购买预测-04模型训练、验证和评测代码详解"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E5%A4%A9%E7%8C%AB%E9%87%8D%E5%A4%8D%E8%B4%AD%E4%B9%B0%E9%A2%84%E6%B5%8B-04/output_30_1.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E5%A4%A9%E7%8C%AB%E9%87%8D%E5%A4%8D%E8%B4%AD%E4%B9%B0%E9%A2%84%E6%B5%8B-04/output_30_2.png"><meta property="article:published_time" content="2023-12-15T10:06:45.000Z"><meta property="article:modified_time" content="2024-12-27T02:40:49.700Z"><meta property="article:author" content="Zhou1317fe5"><meta property="article:tag" content="机器学习"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://zhou1317fe5.github.io/img/%E5%A4%A9%E7%8C%AB%E9%87%8D%E5%A4%8D%E8%B4%AD%E4%B9%B0%E9%A2%84%E6%B5%8B-04/output_30_1.png"><meta name="referrer" content="no-referrer-when-downgrade"><title>天猫重复购买预测-04模型训练、验证和评测 - Zhou1317fe5</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="/css/iconfont_csdn/iconfont.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"zhou1317fe5.github.io",root:"/",version:"1.9.4",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1}},search_path:"/local-search.xml"};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><meta name="generator" content="Hexo 6.3.0"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>Zhou1317fe5</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/Post_banner_img.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="天猫重复购买预测-04模型训练、验证和评测"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2023-12-15 18:06" pubdate>2023年12月15日 晚上</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 30k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 250 分钟 </span><span id="busuanzi_container_page_pv" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="busuanzi_value_page_pv"></span> 次</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 style="display:none">天猫重复购买预测-04模型训练、验证和评测</h1><div class="markdown-body"><h1 id="基础代码">1基础代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">import</span> warnings<br>warnings.filterwarnings(<span class="hljs-string">&quot;ignore&quot;</span>) <br></code></pre></td></tr></table></figure><h2 id="读取数据">1.1 读取数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 训练数据前10000行，测试数据前100条</span><br>train_data = pd.read_csv(<span class="hljs-string">&#x27;./data/train_all.csv&#x27;</span>,nrows=<span class="hljs-number">10000</span>)<br>test_data = pd.read_csv(<span class="hljs-string">&#x27;./data/test_all.csv&#x27;</span>,nrows=<span class="hljs-number">100</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_data.head()<br></code></pre></td></tr></table></figure><div><style scoped>.dataframe tbody tr th:only-of-type{vertical-align:middle}.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</style><table border="1" class="dataframe"><thead><tr style="text-align:right"><th></th><th>user_id</th><th>merchant_id</th><th>age_range</th><th>gender</th><th>user_cnt</th><th>seller_nunique</th><th>cat_nunique</th><th>brand_nunique</th><th>item_nunique</th><th>time_stamp_nunique</th><th>...</th><th>embeeding_93</th><th>embeeding_94</th><th>embeeding_95</th><th>embeeding_96</th><th>embeeding_97</th><th>embeeding_98</th><th>embeeding_99</th><th>lgb_clf</th><th>xgb_clf</th><th>label</th></tr></thead><tbody><tr><th>0</th><td>105600.0</td><td>1487.0</td><td>6.0</td><td>1.0</td><td>310.0</td><td>96.0</td><td>37.0</td><td>88.0</td><td>217.0</td><td>29.0</td><td>...</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.942560</td><td>0.941660</td><td>0.0</td></tr><tr><th>1</th><td>110976.0</td><td>159.0</td><td>5.0</td><td>0.0</td><td>274.0</td><td>181.0</td><td>70.0</td><td>159.0</td><td>233.0</td><td>52.0</td><td>...</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.933391</td><td>0.927695</td><td>0.0</td></tr><tr><th>2</th><td>374400.0</td><td>302.0</td><td>5.0</td><td>1.0</td><td>278.0</td><td>57.0</td><td>59.0</td><td>62.0</td><td>148.0</td><td>35.0</td><td>...</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.923382</td><td>0.909905</td><td>0.0</td></tr><tr><th>3</th><td>189312.0</td><td>1760.0</td><td>4.0</td><td>0.0</td><td>237.0</td><td>49.0</td><td>35.0</td><td>45.0</td><td>170.0</td><td>9.0</td><td>...</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.940141</td><td>0.940140</td><td>0.0</td></tr><tr><th>4</th><td>189312.0</td><td>2511.0</td><td>4.0</td><td>0.0</td><td>237.0</td><td>49.0</td><td>35.0</td><td>45.0</td><td>170.0</td><td>9.0</td><td>...</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.930960</td><td>0.949015</td><td>0.0</td></tr></tbody></table><p>5 rows × 231 columns</p></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">test_data.head()<br></code></pre></td></tr></table></figure><div><style scoped>.dataframe tbody tr th:only-of-type{vertical-align:middle}.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</style><table border="1" class="dataframe"><thead><tr style="text-align:right"><th></th><th>user_id</th><th>merchant_id</th><th>age_range</th><th>gender</th><th>user_cnt</th><th>seller_nunique</th><th>cat_nunique</th><th>brand_nunique</th><th>item_nunique</th><th>time_stamp_nunique</th><th>...</th><th>embeeding_92</th><th>embeeding_93</th><th>embeeding_94</th><th>embeeding_95</th><th>embeeding_96</th><th>embeeding_97</th><th>embeeding_98</th><th>embeeding_99</th><th>lgb_clf</th><th>xgb_clf</th></tr></thead><tbody><tr><th>0</th><td>105600.0</td><td>1487.0</td><td>6.0</td><td>1.0</td><td>310.0</td><td>96.0</td><td>37.0</td><td>88.0</td><td>217.0</td><td>29.0</td><td>...</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.936660</td><td>0.931692</td></tr><tr><th>1</th><td>110976.0</td><td>159.0</td><td>5.0</td><td>0.0</td><td>274.0</td><td>181.0</td><td>70.0</td><td>159.0</td><td>233.0</td><td>52.0</td><td>...</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.941681</td><td>0.934976</td></tr><tr><th>2</th><td>374400.0</td><td>302.0</td><td>5.0</td><td>1.0</td><td>278.0</td><td>57.0</td><td>59.0</td><td>62.0</td><td>148.0</td><td>35.0</td><td>...</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.926634</td><td>0.922883</td></tr><tr><th>3</th><td>189312.0</td><td>1760.0</td><td>4.0</td><td>0.0</td><td>237.0</td><td>49.0</td><td>35.0</td><td>45.0</td><td>170.0</td><td>9.0</td><td>...</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.939729</td><td>0.945877</td></tr><tr><th>4</th><td>189312.0</td><td>2511.0</td><td>4.0</td><td>0.0</td><td>237.0</td><td>49.0</td><td>35.0</td><td>45.0</td><td>170.0</td><td>9.0</td><td>...</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.941506</td><td>0.947046</td></tr></tbody></table><p>5 rows × 230 columns</p></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 读取全部数据</span><br><span class="hljs-comment"># train_data = pd.read_csv(&#x27;train_all.csv&#x27;,nrows=None)</span><br><span class="hljs-comment"># test_data = pd.read_csv(&#x27;test_all.csv&#x27;,nrows=None)</span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_data.columns<br></code></pre></td></tr></table></figure><pre><code class="hljs">Index([&#39;user_id&#39;, &#39;merchant_id&#39;, &#39;age_range&#39;, &#39;gender&#39;, &#39;user_cnt&#39;,
       &#39;seller_nunique&#39;, &#39;cat_nunique&#39;, &#39;brand_nunique&#39;, &#39;item_nunique&#39;,
       &#39;time_stamp_nunique&#39;,
       ...
       &#39;embeeding_93&#39;, &#39;embeeding_94&#39;, &#39;embeeding_95&#39;, &#39;embeeding_96&#39;,
       &#39;embeeding_97&#39;, &#39;embeeding_98&#39;, &#39;embeeding_99&#39;, &#39;lgb_clf&#39;, &#39;xgb_clf&#39;,
       &#39;label&#39;],
      dtype=&#39;object&#39;, length=231)</code></pre><h2 id="获取训练和测试数据">1.2 获取训练和测试数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">features_columns = [col <span class="hljs-keyword">for</span> col <span class="hljs-keyword">in</span> train_data.columns <span class="hljs-keyword">if</span> col <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;user_id&#x27;</span>,<span class="hljs-string">&#x27;label&#x27;</span>]]<br>train = train_data[features_columns].values<br>test = test_data[features_columns].values<br>target =train_data[<span class="hljs-string">&#x27;label&#x27;</span>].values<br></code></pre></td></tr></table></figure><h2 id="切分40数据用于线下验证">1.3 切分40%数据用于线下验证</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br><br>clf = RandomForestClassifier(n_estimators=<span class="hljs-number">100</span>, max_depth=<span class="hljs-number">2</span>, random_state=<span class="hljs-number">0</span>, n_jobs=-<span class="hljs-number">1</span>)<br>X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=<span class="hljs-number">0.4</span>, random_state=<span class="hljs-number">0</span>)<br><br><span class="hljs-built_in">print</span>(X_train.shape, y_train.shape)<br><span class="hljs-built_in">print</span>(X_test.shape, y_test.shape)<br><br>clf = clf.fit(X_train, y_train)<br>clf.score(X_test, y_test)  <br></code></pre></td></tr></table></figure><pre><code class="hljs">(5073, 229) (5073,)
(3382, 229) (3382,)





0.936428149024246</code></pre><h1 id="简单验证">2 简单验证</h1><h2 id="交叉验证">2.1 交叉验证</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> cross_val_score<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br><br>clf = RandomForestClassifier(n_estimators=<span class="hljs-number">100</span>, max_depth=<span class="hljs-number">2</span>, random_state=<span class="hljs-number">0</span>, n_jobs=-<span class="hljs-number">1</span>)<br>scores = cross_val_score(clf, train, target, cv=<span class="hljs-number">5</span>)<br><span class="hljs-built_in">print</span>(scores)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Accuracy: %0.2f (+/- %0.2f)&quot;</span> % (scores.mean(), scores.std() * <span class="hljs-number">2</span>)) <br></code></pre></td></tr></table></figure><pre><code class="hljs">[0.93 0.93 0.93 0.93 0.93]
Accuracy: 0.93 (+/- 0.00)</code></pre><h2 id="f1验证">2.2 F1验证</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> cross_val_score<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br><br>clf = RandomForestClassifier(n_estimators=<span class="hljs-number">100</span>, max_depth=<span class="hljs-number">2</span>, random_state=<span class="hljs-number">0</span>, n_jobs=-<span class="hljs-number">1</span>)<br>scores = cross_val_score(clf, train, target, cv=<span class="hljs-number">5</span>, scoring=<span class="hljs-string">&#x27;f1_macro&#x27;</span>)<br><span class="hljs-built_in">print</span>(scores)  <br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;F1: %0.2f (+/- %0.2f)&quot;</span> % (scores.mean(), scores.std() * <span class="hljs-number">2</span>))<br></code></pre></td></tr></table></figure><pre><code class="hljs">[0.48 0.48 0.48 0.48 0.48]
F1: 0.48 (+/- 0.00)</code></pre><h1 id="设置交叉验证方法">3 设置交叉验证方法</h1><h2 id="使用shufflesplit切分数据">3.1 使用ShuffleSplit切分数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> ShuffleSplit<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> cross_val_score<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br><br>clf = RandomForestClassifier(n_estimators=<span class="hljs-number">100</span>, max_depth=<span class="hljs-number">2</span>, random_state=<span class="hljs-number">0</span>, n_jobs=-<span class="hljs-number">1</span>)<br>cv = ShuffleSplit(n_splits=<span class="hljs-number">5</span>, test_size=<span class="hljs-number">0.3</span>, random_state=<span class="hljs-number">0</span>)<br>cross_val_score(clf, train, target, cv=cv)  <br></code></pre></td></tr></table></figure><pre><code class="hljs">array([0.94, 0.93, 0.93, 0.93, 0.93])</code></pre><p>ShuffleSplit 类是 scikit-learn 中用于生成交叉验证划分的工具。它根据给定的随机种子生成随机抽样，从而实现数据集的随机分割。这在分类问题和特征选择中非常有用，因为它可以帮助我们评估不同特征组合的性能。</p><p>ShuffleSplit 迭代器将会生成一个用户给定数量的独立的训练/测试数据划分。样例首先被打散然后划分为一对训练测试集合。</p><p>ShuffleSplit 类的参数如下：</p><ol type="1"><li>n_splits：表示交叉验证的次数。默认值为 10。</li><li>test_size：表示测试集的大小。默认值为 0.1。</li><li>train_size：表示训练集的大小。默认为原始数据集的大小减去测试集的大小。</li><li>random_state：表示随机种子。用于确保结果的可重复性。</li></ol><h2 id="使用kflod切分数据">3.2 使用KFlod切分数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> KFold<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br><br>clf = RandomForestClassifier(n_estimators=<span class="hljs-number">100</span>, max_depth=<span class="hljs-number">2</span>, random_state=<span class="hljs-number">0</span>, n_jobs=-<span class="hljs-number">1</span>)<br>kf = KFold(n_splits=<span class="hljs-number">5</span>)<br><span class="hljs-keyword">for</span> k, (train_index, test_index) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(kf.split(train)):<br>    X_train, X_test, y_train, y_test = train[train_index], train[test_index], target[train_index], target[test_index]<br>    clf = clf.fit(X_train, y_train)<br>    <span class="hljs-built_in">print</span>(k, clf.score(X_test, y_test))<br></code></pre></td></tr></table></figure><pre><code class="hljs">0 0.9319929036073329
1 0.9331756357185098
2 0.9302188054405677
3 0.9331756357185098
4 0.9325842696629213</code></pre><h2 id="stratifiedkfold切分数据label均分">3.3 StratifiedKFold切分数据(label均分)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> StratifiedKFold<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br><br>clf = RandomForestClassifier(n_estimators=<span class="hljs-number">100</span>, max_depth=<span class="hljs-number">2</span>, random_state=<span class="hljs-number">0</span>, n_jobs=-<span class="hljs-number">1</span>)<br>skf = StratifiedKFold(n_splits=<span class="hljs-number">5</span>)<br><span class="hljs-keyword">for</span> k, (train_index, test_index) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(skf.split(train, target)):<br>    X_train, X_test, y_train, y_test = train[train_index], train[test_index], target[train_index], target[test_index]<br>    clf = clf.fit(X_train, y_train)<br>    <span class="hljs-built_in">print</span>(k, clf.score(X_test, y_test))<br></code></pre></td></tr></table></figure><pre><code class="hljs">0 0.9325842696629213
1 0.9325842696629213
2 0.9319929036073329
3 0.9319929036073329
4 0.9319929036073329</code></pre><p>StratifiedKFold 交叉验证是一种用于评估分类模型的方法。它根据数据的离散性（或称分布）来划分训练集和测试集，以确保测试集的变化与整体数据分布一致。多用于样本正负比例不平衡的分类问题中。</p><p>StratifiedKFold 的参数如下：</p><p>n_splits：表示交叉验证的次数。默认值为 5。 shuffle：表示是否随机打乱数据集。默认为 True。 random_state：表示随机种子。用于确保结果的可重复性。</p><h1 id="模型调参">4 模型调参</h1><p>对模型调参，然后预测并评估模型性能。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> GridSearchCV<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> classification_report<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br><br><br><span class="hljs-comment"># Split the dataset in two equal parts</span><br>X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=<span class="hljs-number">0.5</span>, random_state=<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># model </span><br>clf = RandomForestClassifier(n_jobs=-<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># Set the parameters by cross-validation</span><br><br>tuned_parameters = &#123;<br>                    <span class="hljs-string">&#x27;n_estimators&#x27;</span>: [<span class="hljs-number">50</span>, <span class="hljs-number">100</span>, <span class="hljs-number">200</span>]<br><span class="hljs-comment">#                     ,&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;]</span><br><span class="hljs-comment">#                     ,&#x27;max_depth&#x27;: [2, 5]</span><br><span class="hljs-comment">#                     ,&#x27;max_features&#x27;: [&#x27;log2&#x27;, &#x27;sqrt&#x27;, &#x27;int&#x27;]</span><br><span class="hljs-comment">#                     ,&#x27;bootstrap&#x27;: [True, False]</span><br><span class="hljs-comment">#                     ,&#x27;warm_start&#x27;: [True, False]</span><br>                    &#125;<br><br>scores = [<span class="hljs-string">&#x27;precision&#x27;</span>]<br><br><span class="hljs-keyword">for</span> score <span class="hljs-keyword">in</span> scores:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;# Tuning hyper-parameters for %s&quot;</span> % score)<br>    <span class="hljs-built_in">print</span>()<br><br>    clf = GridSearchCV(clf, tuned_parameters, cv=<span class="hljs-number">5</span>,<br>                       scoring=<span class="hljs-string">&#x27;%s_macro&#x27;</span> % score)<br>    clf.fit(X_train, y_train)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Best parameters set found on development set:&quot;</span>)<br>    <span class="hljs-built_in">print</span>()<br>    <span class="hljs-built_in">print</span>(clf.best_params_)<br>    <span class="hljs-built_in">print</span>()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Grid scores on development set:&quot;</span>)<br>    <span class="hljs-built_in">print</span>()<br>    means = clf.cv_results_[<span class="hljs-string">&#x27;mean_test_score&#x27;</span>]<br>    stds = clf.cv_results_[<span class="hljs-string">&#x27;std_test_score&#x27;</span>]<br>    <span class="hljs-keyword">for</span> mean, std, params <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(means, stds, clf.cv_results_[<span class="hljs-string">&#x27;params&#x27;</span>]):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;%0.3f (+/-%0.03f) for %r&quot;</span><br>              % (mean, std * <span class="hljs-number">2</span>, params))<br>    <span class="hljs-built_in">print</span>()<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Detailed classification report:&quot;</span>)<br>    <span class="hljs-built_in">print</span>()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;The model is trained on the full development set.&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;The scores are computed on the full evaluation set.&quot;</span>)<br>    <span class="hljs-built_in">print</span>()<br>    y_true, y_pred = y_test, clf.predict(X_test)<br>    <span class="hljs-built_in">print</span>(classification_report(y_true, y_pred))<br>    <span class="hljs-built_in">print</span>()<br></code></pre></td></tr></table></figure><pre><code class="hljs"># Tuning hyper-parameters for precision

Best parameters set found on development set:

&#123;&#39;n_estimators&#39;: 100&#125;

Grid scores on development set:

0.463 (+/-0.000) for &#123;&#39;n_estimators&#39;: 50&#125;
0.463 (+/-0.001) for &#123;&#39;n_estimators&#39;: 100&#125;
0.463 (+/-0.001) for &#123;&#39;n_estimators&#39;: 200&#125;

Detailed classification report:

The model is trained on the full development set.
The scores are computed on the full evaluation set.

              precision    recall  f1-score   support

         0.0       0.94      0.99      0.96      3963
         1.0       0.17      0.02      0.04       265

    accuracy                           0.93      4228
   macro avg       0.55      0.51      0.50      4228
weighted avg       0.89      0.93      0.91      4228</code></pre><p>​<br>​</p><h4 id="代码解释">代码解释</h4><p>·clf = GridSearchCV(clf, tuned_parameters, cv=5,scoring='%s_macro' % score)<code>这行代码中，</code>'%s_macro' % score<code>是一个Python字符串格式化表达式，用来构造</code>GridSearchCV<code>中</code>scoring`参数的值。</p><ul><li><code>%s</code> 是一个占位符，它用来指示一个字符串将被插入到这个位置。</li><li><code>score</code> 是一个变量，其值在之前的代码中由循环 <code>for score in scores:</code> 定义。代码样例中，<code>scores</code>列表包含一个元素，即 <code>'precision'</code>。</li><li><code>'%s_macro' % score</code> 这个表达式的作用是把 <code>scores</code> 列表里的字符串元素插入到 <code>%s</code> 的位置，生成一个新的字符串。</li></ul><p>实际上，当 <code>score</code> 变量的值是 <code>'precision'</code> 时，表达式<code>'%s_macro' % score</code>就会生成字符串 <code>'precision_macro'</code>。这表明 <code>GridSearchCV</code> 实例将使用预测的宏平均精确度作为模型评估的得分标准。</p><p>宏平均精确度（macro average precision）是一种评分方法，在多类分类问题中非常有用，它会计算每个类的精确度，然后计算这些精确度的平均值，不考虑每个类的样本量。这与加权平均（weighted average）相反，加权平均会根据每个类中的样本数量给予不同的权重。</p><h1 id="混淆矩阵">5混淆矩阵</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> itertools<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> confusion_matrix<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br><br><span class="hljs-comment"># label name</span><br>class_names = [<span class="hljs-string">&#x27;no-repeat&#x27;</span>, <span class="hljs-string">&#x27;repeat&#x27;</span>]<br><br><span class="hljs-comment"># Split the data into a training set and a test set</span><br>X_train, X_test, y_train, y_test = train_test_split(train, target, random_state=<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># Run classifier, using a model that is too regularized (C too low) to see</span><br><span class="hljs-comment"># the impact on the results</span><br>clf = RandomForestClassifier(n_jobs=-<span class="hljs-number">1</span>)<br>y_pred = clf.fit(X_train, y_train).predict(X_test)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_confusion_matrix</span>(<span class="hljs-params">cm, classes,</span><br><span class="hljs-params">                          normalize=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">                          title=<span class="hljs-string">&#x27;Confusion matrix&#x27;</span>,</span><br><span class="hljs-params">                          cmap=plt.cm.Blues</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    This function prints and plots the confusion matrix.</span><br><span class="hljs-string">    Normalization can be applied by setting `normalize=True`.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> normalize:<br>        cm = cm.astype(<span class="hljs-string">&#x27;float&#x27;</span>) / cm.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">1</span>)[:, np.newaxis]<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Normalized confusion matrix&quot;</span>)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Confusion matrix, without normalization&#x27;</span>)<br><br>    <span class="hljs-built_in">print</span>(cm)<br><br>    plt.imshow(cm, interpolation=<span class="hljs-string">&#x27;nearest&#x27;</span>, cmap=cmap)<br>    plt.title(title)<br>    plt.colorbar()<br>    tick_marks = np.arange(<span class="hljs-built_in">len</span>(classes))<br>    plt.xticks(tick_marks, classes, rotation=<span class="hljs-number">45</span>)<br>    plt.yticks(tick_marks, classes)<br><br>    fmt = <span class="hljs-string">&#x27;.2f&#x27;</span> <span class="hljs-keyword">if</span> normalize <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;d&#x27;</span><br>    thresh = cm.<span class="hljs-built_in">max</span>() / <span class="hljs-number">2.</span><br>    <span class="hljs-keyword">for</span> i, j <span class="hljs-keyword">in</span> itertools.product(<span class="hljs-built_in">range</span>(cm.shape[<span class="hljs-number">0</span>]), <span class="hljs-built_in">range</span>(cm.shape[<span class="hljs-number">1</span>])):<br>        plt.text(j, i, <span class="hljs-built_in">format</span>(cm[i, j], fmt),<br>                 horizontalalignment=<span class="hljs-string">&quot;center&quot;</span>,<br>                 color=<span class="hljs-string">&quot;white&quot;</span> <span class="hljs-keyword">if</span> cm[i, j] &gt; thresh <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;black&quot;</span>)<br><br>    plt.ylabel(<span class="hljs-string">&#x27;True label&#x27;</span>)<br>    plt.xlabel(<span class="hljs-string">&#x27;Predicted label&#x27;</span>)<br>    plt.tight_layout()<br><br><br><span class="hljs-comment"># Compute confusion matrix</span><br>cnf_matrix = confusion_matrix(y_test, y_pred)<br>np.set_printoptions(precision=<span class="hljs-number">2</span>) <span class="hljs-comment"># 设置了打印混淆矩阵时的浮点数精度为两位小数</span><br><br><span class="hljs-comment"># Plot non-normalized confusion matrix</span><br>plt.figure()<br>plot_confusion_matrix(cnf_matrix, classes=class_names,<br>                      title=<span class="hljs-string">&#x27;Confusion matrix, without normalization&#x27;</span>)<br><br><span class="hljs-comment"># Plot normalized confusion matrix</span><br>plt.figure()<br>plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=<span class="hljs-literal">True</span>,<br>                      title=<span class="hljs-string">&#x27;Normalized confusion matrix&#x27;</span>)<br><br>plt.show()<br></code></pre></td></tr></table></figure><pre><code class="hljs">Confusion matrix, without normalization
[[1973    9]
 [ 128    4]]
Normalized confusion matrix
[[1.   0.  ]
 [0.97 0.03]]</code></pre><p><img src="/img/天猫重复购买预测-04/output_30_1.png" srcset="/img/loading.gif" lazyload></p><p><img src="/img/天猫重复购买预测-04/output_30_2.png" srcset="/img/loading.gif" lazyload></p><h4 id="代码解释-1">代码解释</h4><p><strong>函数<code>plot_confusion_matrix</code>解释</strong>：</p><ul><li>形参<code>cm</code>是混淆矩阵的数据，<code>classes</code>是分类标签的列表。</li><li><code>normalize</code>参数指示是否应该归一化混淆矩阵。如果设为<code>True</code>，函数则会将每一行的值除以其总和，结果是每一行的数值和为1，这有助于理解每个真实标签的预测分布。</li><li><code>title</code>是图表的标题，默认为'Confusion matrix'。</li><li><code>cmap=plt.cm.Blues</code>定义了图表使用的颜色映射，默认为蓝色调。</li></ul><p>函数内部的步骤包括：</p><ol type="1"><li>若进行标准化，则对混淆矩阵的每个元素进行规范化，将每一行的值除以该行的总和，并打印“Normalized confusion matrix”（标准化混淆矩阵）。</li><li>若不标准化，则打印“Confusion matrix, without normalization”（未标准化的混淆矩阵）。</li><li>打印混淆矩阵<code>cm</code>的数值。</li><li>使用<code>plt.imshow</code>绘制混淆矩阵的热图，并根据标准化与否设置适当的标题、颜色条卷及颜色映射。</li><li><code>plt.xticks</code>和<code>plt.yticks</code>设置x轴和y轴的刻度标签，旋转x轴标签以便于阅读。</li><li>使用一个循环在热图上标记每个格子的数值。颜色根据数值与混淆矩阵最大值的一半相比是否较大而决定，以便数值在视觉上更容易区分。</li><li>设定x轴和y轴的标签分别为'Predicted label'和'True label'，并调用<code>plt.tight_layout()</code>确保布局正确。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> classification_report<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br><br><span class="hljs-comment"># label name</span><br>class_names = [<span class="hljs-string">&#x27;no-repeat&#x27;</span>, <span class="hljs-string">&#x27;repeat&#x27;</span>]<br><br><span class="hljs-comment"># Split the data into a training set and a test set</span><br>X_train, X_test, y_train, y_test = train_test_split(train, target, random_state=<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># Run classifier, using a model that is too regularized (C too low) to see</span><br><span class="hljs-comment"># the impact on the results</span><br>clf = RandomForestClassifier(n_jobs=-<span class="hljs-number">1</span>)<br>y_pred = clf.fit(X_train, y_train).predict(X_test)<br><br><span class="hljs-built_in">print</span>(classification_report(y_test, y_pred, target_names=class_names))<br></code></pre></td></tr></table></figure><pre><code class="hljs">              precision    recall  f1-score   support

   no-repeat       0.94      1.00      0.97      1982
      repeat       0.30      0.02      0.04       132

    accuracy                           0.94      2114
   macro avg       0.62      0.51      0.50      2114
weighted avg       0.90      0.94      0.91      2114</code></pre><p>​</p><h1 id="不同的分类模型">6 不同的分类模型</h1><h2 id="逻辑回归模型">1 逻辑回归模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<br><br>stdScaler = StandardScaler()<br>X = stdScaler.fit_transform(train) <span class="hljs-comment"># 标准化</span><br><br><span class="hljs-comment"># Split the data into a training set and a test set</span><br>X_train, X_test, y_train, y_test = train_test_split(X, target, random_state=<span class="hljs-number">0</span>)<br><br>clf = LogisticRegression(random_state=<span class="hljs-number">0</span>, solver=<span class="hljs-string">&#x27;lbfgs&#x27;</span>, multi_class=<span class="hljs-string">&#x27;multinomial&#x27;</span>).fit(X_train, y_train)<br>clf.score(X_test, y_test)<br></code></pre></td></tr></table></figure><pre><code class="hljs">0.9380321665089877</code></pre><h4 id="代码解释-2">代码解释</h4><p><code>LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')</code>参数解释</p><ol type="1"><li><p><code>random_state=0</code>：设置随机数种子为0，这样可以在重复运行代码时获得相同的结果。这对于模型的训练和测试是非常有用的。</p></li><li><p><code>solver='lbfgs'</code>：选择优化算法为L-BFGS。L-BFGS是一种收敛速度较快的优化算法，适用于大规模问题。在这个例子中，我们使用L-BFGS来寻找最优的模型参数。</p></li><li><p><code>multi_class='multinomial'</code>：设置多分类方式为多类分类器。多类分类器是一种常用的分类方式，它可以处理多类分类问题，例如按钮 clicks、广告点击率预测等。在这个例子中，我们使用多类分类器来预测不同类型的标签。</p></li></ol><h2 id="knn-模型">2.KNN 模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KNeighborsClassifier<br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<br><br>stdScaler = StandardScaler()<br>X = stdScaler.fit_transform(train)<br><br><span class="hljs-comment"># Split the data into a training set and a test set</span><br>X_train, X_test, y_train, y_test = train_test_split(X, target, random_state=<span class="hljs-number">0</span>)<br><br>clf = KNeighborsClassifier(n_neighbors=<span class="hljs-number">3</span>).fit(X_train, y_train)<br>clf.score(X_test, y_test)<br></code></pre></td></tr></table></figure><pre><code class="hljs">0.9252601702932829</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># clf.predict(X_test)</span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># clf.predict_proba(X_test)</span><br></code></pre></td></tr></table></figure><h2 id="高斯贝叶斯模型">3.高斯贝叶斯模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.naive_bayes <span class="hljs-keyword">import</span> GaussianNB<br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<br><br>stdScaler = StandardScaler()<br>X = stdScaler.fit_transform(train)<br><br><span class="hljs-comment"># Split the data into a training set and a test set</span><br>X_train, X_test, y_train, y_test = train_test_split(X, target, random_state=<span class="hljs-number">0</span>)<br><br>clf = GaussianNB().fit(X_train, y_train)<br>clf.score(X_test, y_test)<br></code></pre></td></tr></table></figure><pre><code class="hljs">0.3793755912961211</code></pre><h2 id="决策树模型">4.决策树模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> tree<br><br><span class="hljs-comment"># Split the data into a training set and a test set</span><br>X_train, X_test, y_train, y_test = train_test_split(train, target, random_state=<span class="hljs-number">0</span>)<br><br>clf = tree.DecisionTreeClassifier()<br>clf = clf.fit(X_train, y_train)<br>clf.score(X_test, y_test)<br></code></pre></td></tr></table></figure><pre><code class="hljs">0.8684957426679281</code></pre><h2 id="bagging模型">5.Bagging模型</h2><p>基模型为KNN</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> BaggingClassifier<br><span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KNeighborsClassifier<br><br><span class="hljs-comment"># Split the data into a training set and a test set</span><br>X_train, X_test, y_train, y_test = train_test_split(train, target, random_state=<span class="hljs-number">0</span>)<br>clf = BaggingClassifier(KNeighborsClassifier(), max_samples=<span class="hljs-number">0.5</span>, max_features=<span class="hljs-number">0.5</span>)<br><br>clf = clf.fit(X_train, y_train)<br>clf.score(X_test, y_test)<br></code></pre></td></tr></table></figure><pre><code class="hljs">0.9375591296121097</code></pre><h2 id="随机森林模型">6.随机森林模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br><br><span class="hljs-comment"># Split the data into a training set and a test set</span><br>X_train, X_test, y_train, y_test = train_test_split(train, target, random_state=<span class="hljs-number">0</span>)<br>clf = clf = RandomForestClassifier(n_estimators=<span class="hljs-number">10</span>, max_depth=<span class="hljs-number">3</span>, min_samples_split=<span class="hljs-number">12</span>, random_state=<span class="hljs-number">0</span>)<br><br>clf = clf.fit(X_train, y_train)<br>clf.score(X_test, y_test)<br></code></pre></td></tr></table></figure><pre><code class="hljs">0.9375591296121097</code></pre><h2 id="极端随机树模型">7.极端随机树模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> ExtraTreesClassifier<br><br><span class="hljs-comment"># Split the data into a training set and a test set</span><br>X_train, X_test, y_train, y_test = train_test_split(train, target, random_state=<span class="hljs-number">0</span>)<br>clf = ExtraTreesClassifier(n_estimators=<span class="hljs-number">10</span>, max_depth=<span class="hljs-literal">None</span>, min_samples_split=<span class="hljs-number">2</span>, random_state=<span class="hljs-number">0</span>)<br><br>clf = clf.fit(X_train, y_train)<br>clf.score(X_test, y_test)<br></code></pre></td></tr></table></figure><pre><code class="hljs">0.9309366130558183</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">clf.n_features_<br></code></pre></td></tr></table></figure><pre><code class="hljs">229</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">clf.feature_importances_[:<span class="hljs-number">10</span>]<br></code></pre></td></tr></table></figure><pre><code class="hljs">array([0.08, 0.02, 0.01, 0.02, 0.02, 0.01, 0.01, 0.02, 0.02, 0.01])</code></pre><h2 id="adaboost模型">8.AdaBoost模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> AdaBoostClassifier<br><br><span class="hljs-comment"># Split the data into a training set and a test set</span><br>X_train, X_test, y_train, y_test = train_test_split(train, target, random_state=<span class="hljs-number">0</span>)<br>clf = AdaBoostClassifier(n_estimators=<span class="hljs-number">10</span>)<br><br>clf = clf.fit(X_train, y_train)<br>clf.score(X_test, y_test)<br></code></pre></td></tr></table></figure><pre><code class="hljs">0.9375591296121097</code></pre><h2 id="gbdt模型">9.GBDT模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> GradientBoostingClassifier<br><br><span class="hljs-comment"># Split the data into a training set and a test set</span><br>X_train, X_test, y_train, y_test = train_test_split(train, target, random_state=<span class="hljs-number">0</span>)<br>clf = GradientBoostingClassifier(n_estimators=<span class="hljs-number">10</span>, learning_rate=<span class="hljs-number">1.0</span>, max_depth=<span class="hljs-number">1</span>, random_state=<span class="hljs-number">0</span>)<br><br>clf = clf.fit(X_train, y_train)<br>clf.score(X_test, y_test)<br></code></pre></td></tr></table></figure><pre><code class="hljs">0.9375591296121097</code></pre><h2 id="集成学习">10.集成学习</h2><p>VOTE模型投票</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> cross_val_score<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><span class="hljs-keyword">from</span> sklearn.naive_bayes <span class="hljs-keyword">import</span> GaussianNB<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> VotingClassifier<br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<br><br>stdScaler = StandardScaler()<br>X = stdScaler.fit_transform(train)<br>y = target<br><br><br>clf1 = LogisticRegression(solver=<span class="hljs-string">&#x27;lbfgs&#x27;</span>, multi_class=<span class="hljs-string">&#x27;multinomial&#x27;</span>, random_state=<span class="hljs-number">1</span>)<br>clf2 = RandomForestClassifier(n_estimators=<span class="hljs-number">50</span>, random_state=<span class="hljs-number">1</span>)<br>clf3 = GaussianNB()<br><br>eclf = VotingClassifier(estimators=[(<span class="hljs-string">&#x27;lr&#x27;</span>, clf1), (<span class="hljs-string">&#x27;rf&#x27;</span>, clf2), (<span class="hljs-string">&#x27;gnb&#x27;</span>, clf3)], voting=<span class="hljs-string">&#x27;hard&#x27;</span>)<br><br><span class="hljs-keyword">for</span> clf, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>([clf1, clf2, clf3, eclf], [<span class="hljs-string">&#x27;Logistic Regression&#x27;</span>, <span class="hljs-string">&#x27;Random Forest&#x27;</span>, <span class="hljs-string">&#x27;naive Bayes&#x27;</span>, <span class="hljs-string">&#x27;Ensemble&#x27;</span>]):<br>    scores = cross_val_score(clf, X, y, cv=<span class="hljs-number">5</span>, scoring=<span class="hljs-string">&#x27;accuracy&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Accuracy: %0.2f (+/- %0.2f) [%s]&quot;</span> % (scores.mean(), scores.std(), label))<br></code></pre></td></tr></table></figure><pre><code class="hljs">Accuracy: 0.93 (+/- 0.00) [Logistic Regression]
Accuracy: 0.93 (+/- 0.00) [Random Forest]
Accuracy: 0.39 (+/- 0.02) [naive Bayes]
Accuracy: 0.93 (+/- 0.00) [Ensemble]</code></pre><h2 id="lgb-模型">11.LGB 模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> lightgbm<br><br>X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=<span class="hljs-number">0.4</span>, random_state=<span class="hljs-number">0</span>)<br>X_test, X_valid, y_test, y_valid = train_test_split(X_test, y_test, test_size=<span class="hljs-number">0.5</span>, random_state=<span class="hljs-number">0</span>)<br><br>clf = lightgbm<br><br>train_matrix = clf.Dataset(X_train, label=y_train)<br>test_matrix = clf.Dataset(X_test, label=y_test)<br>params = &#123;<br>          <span class="hljs-string">&#x27;boosting_type&#x27;</span>: <span class="hljs-string">&#x27;gbdt&#x27;</span>,<br>          <span class="hljs-comment">#&#x27;boosting_type&#x27;: &#x27;dart&#x27;,</span><br>          <span class="hljs-string">&#x27;objective&#x27;</span>: <span class="hljs-string">&#x27;multiclass&#x27;</span>,<br>          <span class="hljs-string">&#x27;metric&#x27;</span>: <span class="hljs-string">&#x27;multi_logloss&#x27;</span>,<br>          <span class="hljs-string">&#x27;min_child_weight&#x27;</span>: <span class="hljs-number">1.5</span>,<br>          <span class="hljs-string">&#x27;num_leaves&#x27;</span>: <span class="hljs-number">2</span>**<span class="hljs-number">5</span>,<br>          <span class="hljs-string">&#x27;lambda_l2&#x27;</span>: <span class="hljs-number">10</span>,<br>          <span class="hljs-string">&#x27;subsample&#x27;</span>: <span class="hljs-number">0.7</span>,<br>          <span class="hljs-string">&#x27;colsample_bytree&#x27;</span>: <span class="hljs-number">0.7</span>,<br>          <span class="hljs-string">&#x27;colsample_bylevel&#x27;</span>: <span class="hljs-number">0.7</span>,<br>          <span class="hljs-string">&#x27;learning_rate&#x27;</span>: <span class="hljs-number">0.03</span>,<br>          <span class="hljs-string">&#x27;tree_method&#x27;</span>: <span class="hljs-string">&#x27;exact&#x27;</span>,<br>          <span class="hljs-string">&#x27;seed&#x27;</span>: <span class="hljs-number">2017</span>,<br>          <span class="hljs-string">&quot;num_class&quot;</span>: <span class="hljs-number">2</span>,<br>          <span class="hljs-string">&#x27;silent&#x27;</span>: <span class="hljs-literal">True</span>,<br>          &#125;<br>num_round = <span class="hljs-number">10000</span><br><span class="hljs-comment">#early_stopping_rounds = 100</span><br><span class="hljs-comment">#callbacks=[lightgbm.log_evaluation(period=100), lightgbm.early_stopping(stopping_rounds=100)]</span><br>callbacks=[lightgbm.early_stopping(stopping_rounds=<span class="hljs-number">100</span>)]<br>model = clf.train(params, <br>                  train_matrix,<br>                  num_round,<br>                  valid_sets=test_matrix,<br>                  <span class="hljs-comment">#early_stopping_rounds=early_stopping_rounds</span><br>                  callbacks=callbacks <br>                  )<br>pre= model.predict(X_valid,num_iteration=model.best_iteration)<br></code></pre></td></tr></table></figure><pre><code class="hljs">Early stopping, best iteration is:
[52]    valid_0&#39;s multi_logloss: 0.237993</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;score : &#x27;</span>, np.mean((pre[:,<span class="hljs-number">1</span>]&gt;<span class="hljs-number">0.5</span>)==y_valid))<br></code></pre></td></tr></table></figure><pre><code class="hljs">score :  0.937906564163217</code></pre><h2 id="xgb-模型">12.XGB 模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> xgboost<br><br>X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=<span class="hljs-number">0.4</span>, random_state=<span class="hljs-number">0</span>)<br>X_test, X_valid, y_test, y_valid = train_test_split(X_test, y_test, test_size=<span class="hljs-number">0.5</span>, random_state=<span class="hljs-number">0</span>)<br><br>clf = xgboost<br><br>train_matrix = clf.DMatrix(X_train, label=y_train, missing=-<span class="hljs-number">1</span>)<br>test_matrix = clf.DMatrix(X_test, label=y_test, missing=-<span class="hljs-number">1</span>)<br>z = clf.DMatrix(X_valid, label=y_valid, missing=-<span class="hljs-number">1</span>)<br>params = &#123;<span class="hljs-string">&#x27;booster&#x27;</span>: <span class="hljs-string">&#x27;gbtree&#x27;</span>,<br>          <span class="hljs-string">&#x27;objective&#x27;</span>: <span class="hljs-string">&#x27;multi:softprob&#x27;</span>,<br>          <span class="hljs-string">&#x27;eval_metric&#x27;</span>: <span class="hljs-string">&#x27;mlogloss&#x27;</span>,<br>          <span class="hljs-string">&#x27;gamma&#x27;</span>: <span class="hljs-number">1</span>,<br>          <span class="hljs-string">&#x27;min_child_weight&#x27;</span>: <span class="hljs-number">1.5</span>,<br>          <span class="hljs-string">&#x27;max_depth&#x27;</span>: <span class="hljs-number">5</span>,<br>          <span class="hljs-string">&#x27;lambda&#x27;</span>: <span class="hljs-number">100</span>,<br>          <span class="hljs-string">&#x27;subsample&#x27;</span>: <span class="hljs-number">0.7</span>,<br>          <span class="hljs-string">&#x27;colsample_bytree&#x27;</span>: <span class="hljs-number">0.7</span>,<br>          <span class="hljs-string">&#x27;colsample_bylevel&#x27;</span>: <span class="hljs-number">0.7</span>,<br>          <span class="hljs-string">&#x27;eta&#x27;</span>: <span class="hljs-number">0.03</span>,<br>          <span class="hljs-string">&#x27;tree_method&#x27;</span>: <span class="hljs-string">&#x27;exact&#x27;</span>,<br>          <span class="hljs-string">&#x27;seed&#x27;</span>: <span class="hljs-number">2017</span>,<br>          <span class="hljs-string">&quot;num_class&quot;</span>: <span class="hljs-number">2</span><br>          &#125;<br><br>num_round = <span class="hljs-number">10000</span><br>early_stopping_rounds = <span class="hljs-number">100</span><br>watchlist = [(train_matrix, <span class="hljs-string">&#x27;train&#x27;</span>),<br>             (test_matrix, <span class="hljs-string">&#x27;eval&#x27;</span>)<br>             ]<br><br>model = clf.train(params,<br>                  train_matrix,<br>                  num_boost_round=num_round,<br>                  evals=watchlist,<br>                  early_stopping_rounds=early_stopping_rounds<br>                  )<br>pre = model.predict(z,ntree_limit=model.best_ntree_limit)<br></code></pre></td></tr></table></figure><pre><code class="hljs">[295]   train-mlogloss:0.22932  eval-mlogloss:0.23988
[296]   train-mlogloss:0.22929  eval-mlogloss:0.23988
[297]   train-mlogloss:0.22925  eval-mlogloss:0.23985
[298]   train-mlogloss:0.22922  eval-mlogloss:0.23984
[299]   train-mlogloss:0.22915  eval-mlogloss:0.23986
[300]   train-mlogloss:0.22907  eval-mlogloss:0.23985</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;score : &#x27;</span>, np.mean((pre[:,<span class="hljs-number">1</span>]&gt;<span class="hljs-number">0.3</span>)==y_valid))<br></code></pre></td></tr></table></figure><pre><code class="hljs">score :  0.937906564163217</code></pre><h1 id="自己封装模型">7 自己封装模型</h1><h2 id="stackingbootstrapbagging技术实践">7.1 Stacking,Bootstrap,Bagging技术实践</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    导入相关包</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> lightgbm <span class="hljs-keyword">as</span> lgb<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> f1_score<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> KFold<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> StratifiedKFold<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SBBTree</span>():<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        SBBTree</span><br><span class="hljs-string">        Stacking,Bootstap,Bagging</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">                    self, </span><br><span class="hljs-params">                    params,</span><br><span class="hljs-params">                    stacking_num,</span><br><span class="hljs-params">                    bagging_num,</span><br><span class="hljs-params">                    bagging_test_size,</span><br><span class="hljs-params">                    num_boost_round,</span><br><span class="hljs-params">                    callbacks</span><br><span class="hljs-params">                </span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">            Initializes the SBBTree.</span><br><span class="hljs-string">            Args:</span><br><span class="hljs-string">              params : lgb params.</span><br><span class="hljs-string">              stacking_num : k_flod stacking.</span><br><span class="hljs-string">              bagging_num : bootstrap num.</span><br><span class="hljs-string">              bagging_test_size : bootstrap sample rate.</span><br><span class="hljs-string">              num_boost_round : boost num.</span><br><span class="hljs-string">              callbacks: callbacks=[lightgbm.early_stopping(stopping_rounds=200)</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.params = params<br>        self.stacking_num = stacking_num<br>        self.bagging_num = bagging_num<br>        self.bagging_test_size = bagging_test_size<br>        self.num_boost_round = num_boost_round<br>        self.callbacks = callbacks<br><br>        self.model = lgb<br>        self.stacking_model = []<br>        self.bagging_model = []<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">fit</span>(<span class="hljs-params">self, X, y</span>):<br>        <span class="hljs-string">&quot;&quot;&quot; fit model. &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> self.stacking_num &gt; <span class="hljs-number">1</span>:<br>            layer_train = np.zeros((X.shape[<span class="hljs-number">0</span>], <span class="hljs-number">2</span>))<br>            self.SK = StratifiedKFold(n_splits=self.stacking_num, shuffle=<span class="hljs-literal">True</span>, random_state=<span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">for</span> k,(train_index, test_index) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(self.SK.split(X, y)):<br>                X_train = X[train_index]<br>                y_train = y[train_index]<br>                X_test = X[test_index]<br>                y_test = y[test_index]<br><br>                lgb_train = lgb.Dataset(X_train, y_train)<br>                lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)<br><br>                gbm = lgb.train(self.params,<br>                            lgb_train,<br>                            num_boost_round=self.num_boost_round,<br>                            valid_sets=lgb_eval,<br>                            callbacks=self.callbacks<br>                            )<br><br>                self.stacking_model.append(gbm)<br><br>                pred_y = gbm.predict(X_test, num_iteration=gbm.best_iteration)<br>                layer_train[test_index, <span class="hljs-number">1</span>] = pred_y<br><br>            X = np.hstack((X, layer_train[:,<span class="hljs-number">1</span>].reshape((-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)))) <br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">pass</span><br>        <span class="hljs-keyword">for</span> bn <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.bagging_num):<br>            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=self.bagging_test_size, random_state=bn)<br><br>            lgb_train = lgb.Dataset(X_train, y_train)<br>            lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)<br><br><br>            gbm = lgb.train(self.params,<br>                        lgb_train,<br>                        num_boost_round=<span class="hljs-number">10000</span>,<br>                        valid_sets=lgb_eval,<br>                        callbacks=[lightgbm.early_stopping(stopping_rounds=<span class="hljs-number">200</span>)]<br>                        )<br><br>            self.bagging_model.append(gbm)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, X_pred</span>):<br>        <span class="hljs-string">&quot;&quot;&quot; predict test data. &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> self.stacking_num &gt; <span class="hljs-number">1</span>:<br>            test_pred = np.zeros((X_pred.shape[<span class="hljs-number">0</span>], self.stacking_num))<br>            <span class="hljs-keyword">for</span> sn,gbm <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(self.stacking_model):<br>                pred = gbm.predict(X_pred, num_iteration=gbm.best_iteration)<br>                test_pred[:, sn] = pred<br>            X_pred = np.hstack((X_pred, test_pred.mean(axis=<span class="hljs-number">1</span>).reshape((-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))))  <br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">pass</span> <br>        <span class="hljs-keyword">for</span> bn,gbm <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(self.bagging_model):<br>            pred = gbm.predict(X_pred, num_iteration=gbm.best_iteration)<br>            <span class="hljs-keyword">if</span> bn == <span class="hljs-number">0</span>:<br>                pred_out=pred<br>            <span class="hljs-keyword">else</span>:<br>                pred_out+=pred<br>        <span class="hljs-keyword">return</span> pred_out/self.bagging_num<br></code></pre></td></tr></table></figure><h4 id="代码解释-3">代码解释</h4><p>这段代码定义了一个名为<code>SBBTree</code>类，表示Stacking, Bootstrap, Bagging这三种集成学习算法的组合。代码中使用了LightGBM框架来训练模型。</p><p>详细解释：</p><p><strong>类 <code>SBBTree</code>：</strong></p><ul><li><p><code>__init__</code> 方法：</p><p>类的构造函数用于初始化<code>SBBTree</code>对象，它接受以下参数：</p><ul><li><code>params</code>：LightGBM模型的参数。</li><li><code>stacking_num</code>：堆叠的次数，用于k折交叉验证中。</li><li><code>bagging_num</code>：Bagging的次数，即bootstrapping的样本集的数量。</li><li><code>bagging_test_size</code>：bootstrapping时测试集的比例。</li><li><code>num_boost_round</code>：boosting迭代次数。</li><li><code>callbacks</code>：回调函数，用于训练过程中提供一些操作，例如早停止。</li></ul><p>构造函数内部将传入的参数赋值给类的属性，并初始化了两个列表，<code>stacking_model</code>和<code>bagging_model</code>，分别用于存储来自堆叠的模型和bagging的模型。</p></li><li><p><code>fit</code> 方法：</p><p>用来训练SBBTree模型。这个方法首先检查如果<code>stacking_num</code>大于1，就会执行k折堆叠，每次迭代都训练一个新的模型并将其添加到<code>stacking_model</code>列表，并且保留预测结果。所有折的预测结果平均后，作为新的特征添加到原有的特征集中。如果<code>stacking_num</code>等于1，预测结果作为新的特征是不被添加的。</p><p>接下来的Bagging部分，代码通过<code>train_test_split</code>函数进行多次划分，每一次划分都训练一个新的LightGBM模型，并将每个模型添加到<code>bagging_model</code>列表中。</p></li><li><p><code>predict</code> 方法：</p><p>用来预测输入的数据<code>X_pred</code>。如果<code>stacking_num</code>大于1，就会使用堆叠过程中得到的模型来对数据进行预测，并将结果平均后作为新特征，再和原始特征进行叠加。然后利用Bagging过程中得到的所有模型对叠加后的特征数据集进行预测。最后，所有Bagging模型的预测结果将求平均值返回最终的预测结果。如果<code>stacking_num</code>等于1，则没有对原始特征叠加新特征的步骤。</p></li></ul><p>该类结合了三种集成算法来提升模型的泛化能力和性能。堆叠（Stacking）是通过将模型的预测结果作为新的特征来训练上层模型；Bootstrap是通过从原数据集中随机抽样来生成新的数据集；Bagging是通过并行地训练多个模型并组合它们的预测结果来减小方差。</p><p>这种组合方式通常可以让模型在多样性和稳定性上取得更好的平衡，从而提高预测的准确率。</p><h2 id="测试自己封装的模型类">7.2 测试自己封装的模型类</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    TEST CODE</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_gaussian_quantiles<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics<br>X, y = make_gaussian_quantiles(mean=<span class="hljs-literal">None</span>, cov=<span class="hljs-number">1.0</span>, n_samples=<span class="hljs-number">1000</span>, n_features=<span class="hljs-number">50</span>, n_classes=<span class="hljs-number">2</span>, shuffle=<span class="hljs-literal">True</span>, random_state=<span class="hljs-number">2</span>)<br>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="hljs-number">0.33</span>, random_state=<span class="hljs-number">1</span>)<br>params = &#123;<br>        <span class="hljs-string">&#x27;task&#x27;</span>: <span class="hljs-string">&#x27;train&#x27;</span>,<br>        <span class="hljs-string">&#x27;boosting_type&#x27;</span>: <span class="hljs-string">&#x27;gbdt&#x27;</span>,<br>        <span class="hljs-string">&#x27;objective&#x27;</span>: <span class="hljs-string">&#x27;binary&#x27;</span>,<br>        <span class="hljs-string">&#x27;metric&#x27;</span>: <span class="hljs-string">&#x27;auc&#x27;</span>,<br>        <span class="hljs-string">&#x27;num_leaves&#x27;</span>: <span class="hljs-number">9</span>,<br>        <span class="hljs-string">&#x27;learning_rate&#x27;</span>: <span class="hljs-number">0.03</span>,<br>        <span class="hljs-string">&#x27;feature_fraction_seed&#x27;</span>: <span class="hljs-number">2</span>,<br>        <span class="hljs-string">&#x27;feature_fraction&#x27;</span>: <span class="hljs-number">0.9</span>,<br>        <span class="hljs-string">&#x27;bagging_fraction&#x27;</span>: <span class="hljs-number">0.8</span>,<br>        <span class="hljs-string">&#x27;bagging_freq&#x27;</span>: <span class="hljs-number">5</span>,<br>        <span class="hljs-string">&#x27;min_data&#x27;</span>: <span class="hljs-number">20</span>,<br>        <span class="hljs-string">&#x27;min_hessian&#x27;</span>: <span class="hljs-number">1</span>,<br>        <span class="hljs-string">&#x27;verbose&#x27;</span>: -<span class="hljs-number">1</span>,<br>        <span class="hljs-string">&#x27;silent&#x27;</span>: <span class="hljs-number">0</span><br>        &#125;<br><span class="hljs-comment"># test 1</span><br>model = SBBTree(params, stacking_num=<span class="hljs-number">2</span>, bagging_num=<span class="hljs-number">1</span>,  bagging_test_size=<span class="hljs-number">0.33</span>, num_boost_round=<span class="hljs-number">10000</span>, callbacks=[lightgbm.early_stopping(stopping_rounds=<span class="hljs-number">200</span>)])<br>model.fit(X,y)<br>X_pred = X[<span class="hljs-number">0</span>].reshape((<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>))<br>pred=model.predict(X_pred)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;pred&#x27;</span>)<br><span class="hljs-built_in">print</span>(pred)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;TEST 1 ok&#x27;</span>)<br><br><br><span class="hljs-comment"># test 1</span><br>model = SBBTree(params, stacking_num=<span class="hljs-number">1</span>, bagging_num=<span class="hljs-number">1</span>, bagging_test_size=<span class="hljs-number">0.33</span>, num_boost_round=<span class="hljs-number">10000</span>, callbacks=[lightgbm.early_stopping(stopping_rounds=<span class="hljs-number">200</span>)])<br>model.fit(X_train,y_train)<br>pred1=model.predict(X_test)<br><br><span class="hljs-comment"># test 2 </span><br>model = SBBTree(params, stacking_num=<span class="hljs-number">1</span>, bagging_num=<span class="hljs-number">3</span>, bagging_test_size=<span class="hljs-number">0.33</span>, num_boost_round=<span class="hljs-number">10000</span>, callbacks=[lightgbm.early_stopping(stopping_rounds=<span class="hljs-number">200</span>)])<br>model.fit(X_train,y_train)<br>pred2=model.predict(X_test)<br><br><span class="hljs-comment"># test 3 </span><br>model = SBBTree(params, stacking_num=<span class="hljs-number">5</span>, bagging_num=<span class="hljs-number">1</span>, bagging_test_size=<span class="hljs-number">0.33</span>, num_boost_round=<span class="hljs-number">10000</span>, callbacks=[lightgbm.early_stopping(stopping_rounds=<span class="hljs-number">200</span>)])<br>model.fit(X_train,y_train)<br>pred3=model.predict(X_test)<br><br><span class="hljs-comment"># test 4 </span><br>model = SBBTree(params, stacking_num=<span class="hljs-number">5</span>, bagging_num=<span class="hljs-number">3</span>, bagging_test_size=<span class="hljs-number">0.33</span>, num_boost_round=<span class="hljs-number">10000</span>, callbacks=[lightgbm.early_stopping(stopping_rounds=<span class="hljs-number">200</span>)])<br>model.fit(X_train,y_train)<br>pred4=model.predict(X_test)<br><br>fpr, tpr, thresholds = metrics.roc_curve(y_test+<span class="hljs-number">1</span>, pred1, pos_label=<span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;auc: &#x27;</span>,metrics.auc(fpr, tpr))<br><br>fpr, tpr, thresholds = metrics.roc_curve(y_test+<span class="hljs-number">1</span>, pred2, pos_label=<span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;auc: &#x27;</span>,metrics.auc(fpr, tpr))<br><br>fpr, tpr, thresholds = metrics.roc_curve(y_test+<span class="hljs-number">1</span>, pred3, pos_label=<span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;auc: &#x27;</span>,metrics.auc(fpr, tpr))<br><br>fpr, tpr, thresholds = metrics.roc_curve(y_test+<span class="hljs-number">1</span>, pred4, pos_label=<span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;auc: &#x27;</span>,metrics.auc(fpr, tpr))<br><br><br><span class="hljs-comment"># auc:  0.7281621243885396</span><br><span class="hljs-comment"># auc:  0.7710471146419509</span><br><span class="hljs-comment"># auc:  0.7894369046305492</span><br><span class="hljs-comment"># auc:  0.8084519474787597</span><br></code></pre></td></tr></table></figure><pre><code class="hljs">Early stopping, best iteration is:
[26]    valid_0&#39;s auc: 0.764161
Training until validation scores don&#39;t improve for 200 rounds
Early stopping, best iteration is:
[49]    valid_0&#39;s auc: 0.806934
auc:  0.7107286034793483
auc:  0.7791754018169113
auc:  0.7681783074037294
auc:  0.7900989370701387</code></pre><h4 id="代码解释-4">代码解释</h4><p>这段测试代码使用了之前定义的<code>SBBTree</code>类，该类是用于构建、训练和预测一个集成学习模型，结合了Stacking、Bootstrap和Bagging策略。</p><p><strong>测试代码的步骤包括：</strong></p><ol type="1"><li><p>导入<code>make_gaussian_quantiles</code>函数用于生成一个高斯分布的合成数据集，和<code>metrics</code>模块用于评估模型性能。</p></li><li><p>生成一个含1000个样本、每个样本具有50个特征的数据集，并且标签类别为2。</p></li><li><p>使用<code>train_test_split</code>把数据集分割成训练集和测试集，测试集大小占33%。</p></li><li><p>设置LightGBM模型相关的参数<code>params</code>的字典，包括任务类型、梯度提升类型、目标函数、评价指标等。</p><ul><li><code>task</code>：指定当前的任务类型，可以是<code>train</code>（训练）或<code>predict</code>（预测）。</li><li><code>boosting_type</code>：指定提升类型，可以是<code>gbdt</code>（梯度提升树）。</li><li><code>objective</code>：指定目标函数，可以是<code>binary</code>（二分类）。</li><li><code>metric</code>：指定评估指标，可以是<code>auc</code>（Area Under the Curve）。</li><li><code>num_leaves</code>：指定叶子节点数。</li><li><code>learning_rate</code>：指定学习率。</li><li><code>feature_fraction_seed</code>：指定特征fraction的随机种子。</li><li><code>feature_fraction</code>：指定特征fraction。</li><li><code>bagging_fraction</code>：指定baggingfraction。</li><li><code>bagging_freq</code>：指定bagging频率。</li><li><code>min_data</code>：指定每个叶子节点上最少的数据样本数。</li><li><code>min_hessian</code>：指定每个叶子节点上最小的Hessian值。</li><li><code>verbose</code>：指定是否显示日志信息，可以是<code>-1</code>（不显示）或<code>0</code>（显示）。</li><li><code>silent</code>：指定是否静默模式，可以是<code>0</code>（不静默）或<code>1</code>（静默）。</li></ul></li><li><p>进行四个不同设置的测试：</p><ul><li><code>test 1</code>: <code>stacking_num=2</code>, <code>bagging_num=1</code></li><li><code>test 1</code>: <code>stacking_num=1</code>, <code>bagging_num=1</code></li><li><code>test 2</code>: <code>stacking_num=1</code>, <code>bagging_num=3</code></li><li><code>test 3</code>: <code>stacking_num=5</code>, <code>bagging_num=1</code></li><li><code>test 4</code>: <code>stacking_num=5</code>, <code>bagging_num=3</code></li></ul><p>每个测试都实例化了一个<code>SBBTree</code>模型，使用提供的参数进行训练，并对测试集进行预测。</p></li><li><p>对每个测试的预测结果，使用<code>roc_curve</code>计算真正率（TPR）和假正率（FPR），再使用<code>metrics.auc</code>计算AUC值，并打印出来。<code>y_test+1</code>和<code>pos_label=2</code>的部分是为了使得二元分类的标签与<code>metrics</code>模块的默认正类标签匹配。</p></li></ol><p>这段代码的目标是评估<code>SBBTree</code>不同的堆叠和装袋配置对模型性能（特别是AUC分数）的影响，并打印出模型预测的示例。通过改变堆叠数和装袋数，能看到模型在不同设置下的性能如何变化。</p><h1 id="天猫复购场景实战">8 天猫复购场景实战</h1><h2 id="读取特征数据">8.1读取特征数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> lightgbm <span class="hljs-keyword">as</span> lgb<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> f1_score<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> KFold<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> StratifiedKFold<br><br>train_data = pd.read_csv(<span class="hljs-string">&#x27;./data/train_all.csv&#x27;</span>,nrows=<span class="hljs-number">10000</span>)<br>test_data = pd.read_csv(<span class="hljs-string">&#x27;./data/test_all.csv&#x27;</span>,nrows=<span class="hljs-number">100</span>)<br><br>features_columns = [col <span class="hljs-keyword">for</span> col <span class="hljs-keyword">in</span> train_data.columns <span class="hljs-keyword">if</span> col <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;user_id&#x27;</span>,<span class="hljs-string">&#x27;label&#x27;</span>]]<br>train = train_data[features_columns].values<br>test = test_data[features_columns].values<br>target =train_data[<span class="hljs-string">&#x27;label&#x27;</span>].values<br></code></pre></td></tr></table></figure><h2 id="设置模型参数">8.2设置模型参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python">params = &#123;<br>        <span class="hljs-string">&#x27;task&#x27;</span>: <span class="hljs-string">&#x27;train&#x27;</span>,<br>        <span class="hljs-string">&#x27;boosting_type&#x27;</span>: <span class="hljs-string">&#x27;gbdt&#x27;</span>,<br>        <span class="hljs-string">&#x27;objective&#x27;</span>: <span class="hljs-string">&#x27;binary&#x27;</span>,<br>        <span class="hljs-string">&#x27;metric&#x27;</span>: <span class="hljs-string">&#x27;auc&#x27;</span>,<br>        <span class="hljs-string">&#x27;num_leaves&#x27;</span>: <span class="hljs-number">9</span>,<br>        <span class="hljs-string">&#x27;learning_rate&#x27;</span>: <span class="hljs-number">0.03</span>,<br>        <span class="hljs-string">&#x27;feature_fraction_seed&#x27;</span>: <span class="hljs-number">2</span>,<br>        <span class="hljs-string">&#x27;feature_fraction&#x27;</span>: <span class="hljs-number">0.9</span>,<br>        <span class="hljs-string">&#x27;bagging_fraction&#x27;</span>: <span class="hljs-number">0.8</span>,<br>        <span class="hljs-string">&#x27;bagging_freq&#x27;</span>: <span class="hljs-number">5</span>,<br>        <span class="hljs-string">&#x27;min_data&#x27;</span>: <span class="hljs-number">20</span>,<br>        <span class="hljs-string">&#x27;min_hessian&#x27;</span>: <span class="hljs-number">1</span>,<br>        <span class="hljs-string">&#x27;verbose&#x27;</span>: -<span class="hljs-number">1</span>,<br>        <span class="hljs-string">&#x27;silent&#x27;</span>: <span class="hljs-number">0</span><br>        &#125;<br><br>model = SBBTree(params=params,<br>                stacking_num=<span class="hljs-number">5</span>,<br>                bagging_num=<span class="hljs-number">3</span>,<br>                bagging_test_size=<span class="hljs-number">0.33</span>,<br>                num_boost_round=<span class="hljs-number">10000</span>,<br>                callbacks=[lightgbm.early_stopping(stopping_rounds=<span class="hljs-number">200</span>)]<br>                )<br></code></pre></td></tr></table></figure><h2 id="模型训练">8.3模型训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.fit(train, target)<br></code></pre></td></tr></table></figure><pre><code class="hljs">Training until validation scores don&#39;t improve for 200 rounds
Early stopping, best iteration is:
[27]    valid_0&#39;s auc: 0.562125</code></pre><h2 id="预测结果">8.4预测结果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">pred = model.predict(test)<br>df_out = pd.DataFrame()<br>df_out[<span class="hljs-string">&#x27;user_id&#x27;</span>] = test_data[<span class="hljs-string">&#x27;user_id&#x27;</span>].astype(<span class="hljs-built_in">int</span>)<br>df_out[<span class="hljs-string">&#x27;predict_prob&#x27;</span>] = pred<br>df_out.head()<br></code></pre></td></tr></table></figure><div><style scoped>.dataframe tbody tr th:only-of-type{vertical-align:middle}.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</style><table border="1" class="dataframe"><thead><tr style="text-align:right"><th></th><th>user_id</th><th>predict_prob</th></tr></thead><tbody><tr><th>0</th><td>105600</td><td>0.064700</td></tr><tr><th>1</th><td>110976</td><td>0.069873</td></tr><tr><th>2</th><td>374400</td><td>0.069219</td></tr><tr><th>3</th><td>189312</td><td>0.064055</td></tr><tr><th>4</th><td>189312</td><td>0.063837</td></tr></tbody></table></div><h2 id="保存结果">8.5保存结果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    保留数据头，不保存index</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>df_out.to_csv(<span class="hljs-string">&#x27;./data/df_out.csv&#x27;</span>,header=<span class="hljs-literal">True</span>,index=<span class="hljs-literal">False</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;save OK!&#x27;</span>)<br></code></pre></td></tr></table></figure><pre><code class="hljs">save OK!</code></pre></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E5%85%A5%E9%97%A8/" class="category-chain-item">机器学习实战入门</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">#机器学习</a></div></div><div class="license-box my-3"><div class="license-title"><div>天猫重复购买预测-04模型训练、验证和评测</div><div>https://zhou1317fe5.github.io/2023/12/15/天猫重复购买预测-04模型训练、验证和评测/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>Zhou1317fe5</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2023年12月15日</div></div><div class="license-meta-item"><div>许可协议</div><div><a target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2023/12/22/%E5%A4%A9%E7%8C%AB%E9%87%8D%E5%A4%8D%E8%B4%AD%E4%B9%B0%E9%A2%84%E6%B5%8B-05%E7%89%B9%E5%BE%81%E4%BC%98%E5%8C%96%E5%92%8C%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/" title="天猫重复购买预测-05特征优化和特征选择"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">天猫重复购买预测-05特征优化和特征选择</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2023/12/07/%E5%A4%A9%E7%8C%AB%E9%87%8D%E5%A4%8D%E8%B4%AD%E4%B9%B0%E9%A2%84%E6%B5%8B-03%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" title="天猫重复购买预测-03特征工程"><span class="hidden-mobile">天猫重复购买预测-03特征工程</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div><div class="statistics"><span id="busuanzi_container_site_pv" style="display:none">总访问量 <span id="busuanzi_value_site_pv"></span> 次 </span><span id="busuanzi_container_site_uv" style="display:none">总访客数 <span id="busuanzi_value_site_uv"></span> 人</span></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script>window.MathJax?(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise()):window.MathJax={tex:{inlineMath:{"[+]":[["$","$"]]}},loader:{load:["ui/lazy"]},options:{renderActions:{insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(t=>{let e=t.parentNode;"li"===e.nodeName.toLowerCase()&&e.parentNode.classList.add("has-jax")})},"",!1]}}},Fluid.events.registerRefreshCallback((function(){"MathJax"in window&&MathJax.startup.document&&"function"==typeof MathJax.startup.document.state&&(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise())}))</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script src="/js/local-search.js"></script><script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>