<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png"><link rel="icon" href="/img/fluid.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="Zhou1317fe5"><meta name="keywords" content=""><meta name="description" content="强化学习 强化学习是一种机器学习方法，它通过代理与环境的交互来学习如何做出最优决策。代理在不断尝试中获得奖励信号，并通过调整策略来最大化累积奖励。这种学习方式类似于人类通过试错来学习，帮助机器代理逐步提高在特定任务上的表现能力。 强化学习算法的思路非常简单，以游戏为例，如果在游戏中采取某种策略可以取得较高的得分，那么就进一步“强化”这种策略，以期继续取得较好的结果。这种策略与日常生活中的各种“"><meta property="og:type" content="article"><meta property="og:title" content="机器学习07-强化学习"><meta property="og:url" content="https://zhou1317fe5.github.io/2023/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A007-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/index.html"><meta property="og:site_name" content="Zhou1317fe5"><meta property="og:description" content="强化学习 强化学习是一种机器学习方法，它通过代理与环境的交互来学习如何做出最优决策。代理在不断尝试中获得奖励信号，并通过调整策略来最大化累积奖励。这种学习方式类似于人类通过试错来学习，帮助机器代理逐步提高在特定任务上的表现能力。 强化学习算法的思路非常简单，以游戏为例，如果在游戏中采取某种策略可以取得较高的得分，那么就进一步“强化”这种策略，以期继续取得较好的结果。这种策略与日常生活中的各种“"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A007-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230721182005.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A007-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230721182247.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A007-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230721184441.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A007-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230721185214.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A007-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230721194933.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A007-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230721202150.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A007-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230721224150.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A007-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230721224959.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A007-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230722101410.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A007-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230722110235.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A007-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230722112131.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A007-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230722113031.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A007-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230722113530.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A007-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230722183834.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A007-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230722185112.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A007-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230722185443.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A007-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230722190456.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A007-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230722191729.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A007-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230722193800.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A007-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230722203836.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A007-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230722203905.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A007-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230722204559.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A007-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230722210052.png"><meta property="article:published_time" content="2023-07-22T13:21:00.000Z"><meta property="article:modified_time" content="2023-07-22T13:25:32.300Z"><meta property="article:author" content="Zhou1317fe5"><meta property="article:tag" content="机器学习"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A007-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230721182005.png"><meta name="referrer" content="no-referrer-when-downgrade"><title>机器学习07-强化学习 - Zhou1317fe5</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="/css/iconfont_csdn/iconfont.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"zhou1317fe5.github.io",root:"/",version:"1.9.4",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1}},search_path:"/local-search.xml"};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><meta name="generator" content="Hexo 6.3.0"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>Zhou1317fe5</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/Post_banner_img.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="机器学习07-强化学习"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2023-07-22 21:21" pubdate>2023年7月22日 晚上</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 6.3k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 53 分钟 </span><span id="busuanzi_container_page_pv" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="busuanzi_value_page_pv"></span> 次</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 style="display:none">机器学习07-强化学习</h1><div class="markdown-body"><h1 id="强化学习">强化学习</h1><p>强化学习是一种机器学习方法，它通过代理与环境的交互来学习如何做出最优决策。代理在不断尝试中获得奖励信号，并通过调整策略来最大化累积奖励。这种学习方式类似于人类通过试错来学习，帮助机器代理逐步提高在特定任务上的表现能力。</p><p>强化学习算法的思路非常简单，以游戏为例，如果在游戏中采取某种策略可以取得较高的得分，那么就进一步“强化”这种策略，以期继续取得较好的结果。这种策略与日常生活中的各种“绩效奖励”非常类似。</p><h1 id="离散状态空间">离散状态空间</h1><h2 id="示例火星探测器">示例：火星探测器</h2><p>探测器在火星执行探测任务，共有6个位置。在强化学习中，探测器的位置被称为状态，初始时，探测器位于状态4。 现在，位置1和位置6都有一些有趣的表面。科学家希望探测器对位置1进行采样，因为它比位置6更重要。但是位置1很远，所以我们通过奖励函数来反映状态的价值，在每个状态可以获得对应的奖励。状态1的奖励是100，状态6的奖励是40，其他状态的奖励都是0。在每一步中，探测器可以选择向左或向右移动。 <img src="/img/机器学习07-强化学习/Pasted%20image%2020230721182005.png" srcset="/img/loading.gif" lazyload> 在每个位置，机器人都处于某种状态s，它可以选择一个动作a，并从而得到奖励R(s)，状态随动作变为新的状态s'。注意，奖励R(s)是与状态s相关，而不是下一个状态s'相关。 <img src="/img/机器学习07-强化学习/Pasted%20image%2020230721182247.png" srcset="/img/loading.gif" lazyload> 例如，当探测器处于状态4，并采取了向左走的动作，状态4奖励为0，变为新的状态3。</p><p>强化学习四要素：<strong>状态、动作、奖励、下一个状态</strong></p><p>探测器在不同的位置可以走不同的路线，起点为位置4可以走4-3-2-1，也可以走4-5-6。较远的位置虽然奖励多，但是它所花费的时间和路程值得这些奖励吗？怎么知道一组特定的奖励比另一组不同的奖励更好还是更差?</p><p>我们用强化学习中的<strong>回报（Return）</strong> 来解决这件事。</p><h2 id="回报">回报</h2><p>基于折扣因子和奖励值，计算一系列动作下所获得的总效用值。折扣因子 <span class="math inline">\(\gamma\)</span> (Gamma)：取值范围在0到1之间，一般是一个非常接近1的数值。 <img src="/img/机器学习07-强化学习/Pasted%20image%2020230721184441.png" srcset="/img/loading.gif" lazyload></p><p>得到的回报取决于奖励，而奖励取决于你采取的行动，因此回报取决于你采取的行动。 <img src="/img/机器学习07-强化学习/Pasted%20image%2020230721185214.png" srcset="/img/loading.gif" lazyload></p><h2 id="策略">策略</h2><p>策略（policy）是指代理在特定状态下选择动作的方式或规则。策略定义了代理对于给定状态所做出的行为，就是一个从状态到行为的映射。我们可以通过策略来确定每个状态下的行为。</p><p>强化学习的目标是训练出一个策略函数，它以任意状态s为输入，并将其映射到采取的某个动作a。</p><p>例如可以有4种策略，以最后一种策略为例，右边是策略函数 <img src="/img/机器学习07-强化学习/Pasted%20image%2020230721194933.png" srcset="/img/loading.gif" lazyload></p><p>强化学习的目标是找到一个策略 <span class="math inline">\(\pi\)</span> ，告诉你在每个状态下采取什么行动以最大化回报。</p><h2 id="马尔可夫决策过程">马尔可夫决策过程</h2><p><strong>马尔可夫决策过程</strong>（Markov Decision Process, MDP），指未来仅取决于当前状态，而不取决于当前状态之前发生的任何事情。换句话说，在马尔可夫决策过程中，未来只取决于你现在所处的位置，而不取决于你是如何到达这里的。 <img src="/img/机器学习07-强化学习/Pasted%20image%2020230721202150.png" srcset="/img/loading.gif" lazyload></p><p>根据目标当前状态 s，基于策略选择动作a，世界或环境会发生变化，然后观察基于世界状态，目标的状态s及得到的奖励R</p><h2 id="状态动作价值函数qsa">状态动作价值函数Q(s,a)</h2><p>状态动作价值函数 Q(s, a)（也称为 Q-value 函数）是在强化学习中用于评估在给定状态 s 下采取动作 a 的价值的函数。它表示了代理在特定状态下采取某个动作所能获得的预期累积回报。</p><p>具体而言，对于给定的状态动作对 (s, a)，Q(s, a) 表示从状态 s 开始，采取某一行动，之后一直按照最佳的策略行动，最终获得的预期累积回报。这个预期累积回报通常是通过累加未来奖励的折扣值来计算的。（至于为何能在获得最优策略前计算Q，后面会讲，会基于循环）。 <img src="/img/机器学习07-强化学习/Pasted%20image%2020230721224150.png" srcset="/img/loading.gif" lazyload></p><p>例如： 计算Q（2，→），则为以状态2开始，采取向右的行动，到状态3，那后面最优策略是一直向左走，行动轨迹为2-3-2-1，所以计算Q为12.5。 Q（2，←）则为以状态2开始，采取某一行动为向左，行动轨迹为2-1，此时Q为50。</p><p>对每个状态，每个动作，都可计算得到Q。而且我们可以发现，Q(s,a)的最大值即为从状态s获得的最佳可能回报。因此当我们可以计算Q(s,a)，即可知道最佳的动作。 <img src="/img/机器学习07-强化学习/Pasted%20image%2020230721224959.png" srcset="/img/loading.gif" lazyload></p><p>如果你可以计算每一个状态每一个动作的Q(s,a)，然后找出最大的Q(s,a)，这个Q(s,a)中的a即为最佳的行动，所以该状态s的 <span class="math inline">\(\pi(s)=a\)</span> 。</p><p>因此，Q计算和策略相对应，如果能基于s和a计算Q函数，就能得到策略。</p><h2 id="贝尔曼方程">贝尔曼方程</h2><p>贝尔曼方程（Bellman equation）是强化学习中的关键方程，用来描述状态值函数或者状态动作值函数之间的递归关系。 <strong>贝尔曼方程</strong>：<span class="math inline">\(Q(s,a)=R(s)+\gamma\max_{a^{\prime}}Q(s^{\prime},a^{\prime})\)</span></p><p>贝尔曼方程通过将<strong>当前</strong>状态和动作的价值与<strong>下一个</strong>状态的价值联系起来，帮助我们计算最优策略。也就是说，贝尔曼方程告诉我们一个状态的价值如何与其后续状态的价值相关联。</p><p>贝尔曼方程的本质是，当前状态s的总回报包含两部分，一部分是马上得到的奖励R(s)，第二部分是 <span class="math inline">\(\gamma\)</span> 乘以下一状态s'的最佳总回报 <img src="/img/机器学习07-强化学习/Pasted%20image%2020230722101410.png" srcset="/img/loading.gif" lazyload> 例如从状态4开始，向左走： <span class="math display">\[ \begin{aligned}&amp;Q(4,\leftarrow)\\&amp;=0+(0.5)0+(0.5)^20+(0.5)^3100\\&amp;=R(4)+(0.5)\begin{bmatrix}0+(0.5)0+(0.5)^2100\end{bmatrix} \\&amp;=R(4)+(0.5)\max_{a^{\prime}}Q(3,a^{\prime})\end{aligned} \]</span></p><p>实际上，贝尔曼方程告诉我们，在当前状态下采取某个动作的价值与下一个状态的价值之间存在一个关系。通过不断迭代更新这个关系，最终将能够找到最佳策略，即在每个状态下都选择可以获得最大预期回报的动作。 ## 随机马尔可夫决策</p><p>在随机马尔可夫过程中，执行动作a时，存在一定概率无法按照预期进行。以一个随机环境为例，我们向一个探测器发送左转指令，但由于未知的环境因素，左侧地面可能很滑，导致探测器滑向相反的方向，无法按照我们的指令执行。因此，我们获得的奖励也是随机的。</p><p>在随机强化学习问题中，目标的不是最大化回报，而是折扣奖励总和的平均值，即期望回报 <span class="math display">\[\begin{aligned}\text{Expected Return}&amp;=\text{Average}(R_1+\gamma R_2+\gamma^2R_3+\gamma^3R_4+\cdots)\\&amp;=\text{E}[R_1+\gamma R_2+\gamma^2R_3+\gamma^3R_4+\cdots]\end{aligned}\]</span></p><p>随机马尔可夫决策中，贝尔曼方程改为： <img src="/img/机器学习07-强化学习/Pasted%20image%2020230722110235.png" srcset="/img/loading.gif" lazyload> 前面TensorFlow实例中，misstep_prob参数就表示失误概率。</p><h1 id="连续状态空间应用">连续状态空间应用</h1><p>例如在自动控制汽车、飞机中，状态s都是连续状态空间，包含x,y,z轴坐标，及各轴方向上速度、角速度等，这都是连续状态空间</p><h2 id="示例登月器">示例：登月器</h2><p>登月器的任务是在适当的时候启动火力推进，将其安全降落到着陆台上。</p><p><strong>状态：</strong> <img src="/img/机器学习07-强化学习/Pasted%20image%2020230722112131.png" srcset="/img/loading.gif" lazyload> <span class="math inline">\(x\)</span> ：水平位置 <span class="math inline">\(y\)</span> ：垂直高度 <span class="math inline">\(\dot{x}\)</span> ：水平方向速度 <span class="math inline">\(\dot{y}\)</span> ：垂直方向速度 <span class="math inline">\(\theta\)</span> ：倾斜角，向左/右倾斜多少 <span class="math inline">\(\dot{\theta}\)</span> ：角速度 <span class="math inline">\(l\)</span> ：左脚落地 <span class="math inline">\(r\)</span> ：右脚落地</p><p><strong>动作：</strong> 什么都不做，向下加速，向左加速，向右加速</p><p><strong>奖励函数：</strong> <img src="/img/机器学习07-强化学习/Pasted%20image%2020230722113031.png" srcset="/img/loading.gif" lazyload> 到达着陆台：100-140 靠近/远离着陆台的额外奖励 坠毁：-100 软着陆：+100 腿着陆：+10 启动主推进器：-0.3 启动侧推进器：-0.03</p><p><strong>目标</strong>是找到策略函数： <img src="/img/机器学习07-强化学习/Pasted%20image%2020230722113530.png" srcset="/img/loading.gif" lazyload></p><h2 id="学习状态值函数强化学习中使用神经网络">学习状态值函数（强化学习中使用神经网络）</h2><h2 id="总1">总1</h2><p>关键思想是我们要训练一个神经网络来计算或近似S,a的状态动作值函数Q,然后上我们洗择好的动作</p><p>学习算法的核心是我们将训练一个神经网络，该网络输入当前状态和当前动作并计算或近似的Q 。 我们将状态和动作放在一起作为输入，这个12个数字的列表，8个数字用于状态，然后是个数字是动作的单热编码。这是我们对神经网络的输入，称之为X。神经网络的工作就是输出Q。</p><p>因为我们稍后会使用神经网络训练算法，所以我还将Q作为训练神经网络逼近的目标值Y。强化学习与监督学习不同，但我们要做的不是输入状态并让它来输出动作。我们要做的是输入一个状态动作对，让它尝试输出Q,并在强化学习算法中使用神经网络。</p><p><img src="/img/机器学习07-强化学习/Pasted%20image%2020230722183834.png" srcset="/img/loading.gif" lazyload> 如果您可以在隐藏层和上层中选择适当的参数来训练神经网络，以便为您提供对Q的良好估计，那么每当您在月球着陆器处于某种状态s时，您就可以使用神经网络计算Q。对于所有四个动作，都可以计算Q，Q(s,nothing),Q(s,left),Q(s,main),Q(s,right)，最后，哪个具有最高值，就选择那个相应的动作。 例如，如果在这四个值中，Q(s,main)最大，那么将启动着陆器的主推进器。</p><p>所以问题就变成了，如何训练一个神经网络来输出Q?</p><p>方法是使用贝尔曼方程来创建包含大量示例×和y的训练集，然后我们使用监督学习，就像神经网络时所学的一样。使用监督学习，利用神经网络学习从x到y的映射，即从状态动作对到目标值Q的映射。</p><p>但是，如何获得具有x和y值的训练集，可以在其上训练神经网络？</p><p>这是贝尔曼方程：<span class="math inline">\(Q(s,a)=R(s)+\gamma\max_{a^{\prime}}Q(s^{\prime},a^{\prime})\)</span> 。 <img src="/img/机器学习07-强化学习/Pasted%20image%2020230722185112.png" srcset="/img/loading.gif" lazyload>我们将等式右边的值为神经网络的输出y，等式左边即为输入x。神经网络的工作是输入x，即输入状态动作对，并尝试准确预测右边的值。</p><p>在监督学习中，我们训练一个神经网络来学习一个函数f，它取决于参数W和B（神经网络各个层的参数），神经网络的最左端输入X，最右端放一些接近目标值y的东西。 <img src="/img/机器学习07-强化学习/Pasted%20image%2020230722185443.png" srcset="/img/loading.gif" lazyload> 问题是，我们如何才能为新网络提供一个具有x和y的训练集来学习？</p><p>我们将使用着陆器，并尝试在其中随机执行不同的操作。通过在月球着陆器中尝试做不同的动作，我们将会得到很多例子，这些例子说明我们何时处于哪种状态并采取了哪些动作，可能是一个好的动作，也可能是一个坏的动作。然后，由于处于该状态，我们获得了一些奖励R(s)，接着，我们进入了某个新状态S'。<span class="math inline">\((s,a,R(s),s^{\prime})\)</span> 我们称为元组。</p><p><img src="/img/机器学习07-强化学习/Pasted%20image%2020230722190456.png" srcset="/img/loading.gif" lazyload></p><p>例如，也许有一次处于某个状态 <span class="math inline">\(S^{(1)}\)</span>，采取了动作 <span class="math inline">\(a^{(1)}\)</span> ，得到了奖励 <span class="math inline">\(R(S^{(1)})\)</span> ，达到了个新状态 <span class="math inline">\(S^{\prime(1)}\)</span> ； 也许在不同的时间处于某个状态 <span class="math inline">\(S^{(2)}\)</span>，采取了动作 <span class="math inline">\(a^{(2)}\)</span> ，得到了奖励 <span class="math inline">\(R(S^{(2)})\)</span> ，达到了个新状态 <span class="math inline">\(S^{\prime(2)}\)</span> ；以此类推 。 也许你已经这样做了10,000次甚至超过10,000次。这10,000个元组都是训练示例x，y。 <img src="/img/机器学习07-强化学习/Pasted%20image%2020230722191729.png" srcset="/img/loading.gif" lazyload> 元组中的前两个将用于计算x，后两个将用于计算y。 例如，<span class="math inline">\(x^1\)</span> 就是 <span class="math inline">\(x^{(1)}=(s^{(1)},a^{(1)})\)</span> ；<span class="math inline">\(y^1\)</span> 将使用贝尔曼方程的右侧计算。 贝尔曼方程表示，当您输入 <span class="math inline">\(s^{(1)},a^{(1)}\)</span> 时，您希望<span class="math inline">\(Q(s^{(1)},a^{(1)})=R(s^{(1)})+\gamma\max_{a^{\prime}}Q(s^{\prime(1)},a^{\prime})\)</span> 。<span class="math inline">\(y^1\)</span> 即等于 <span class="math inline">\(R(s^{(1)})+\gamma\max_{a^{\prime}}Q(s^{\prime(1)},a^{\prime})\)</span> 。注意，元组最后的两个元素提供了足够的信息来计算右侧，计算后这将是数字，比如12.5或17。 我们将数字保存为 <span class="math inline">\(y^1\)</span> 。这对 <span class="math inline">\(x^1,y^1\)</span> 成为了第一个训练示例。 <img src="/img/机器学习07-强化学习/Pasted%20image%2020230722193800.png" srcset="/img/loading.gif" lazyload> 现在你可能想要知道 <span class="math inline">\(Q(s^{\prime(1)},a^{\prime})\)</span> 从哪里来，其实我们最初并不知道 <span class="math inline">\(Q(s^{\prime(1)},a^{\prime})\)</span> 是什么，但是当不知道Q函数是什么时，可以随机猜测一个初始值。刚开始这里的每一步Q都只是一些猜测，随着时间的推移，他们会变得更好，慢慢的逼近实际的Q函数值。</p><p>依此类推，直到你最终得到10,000个包含这些x,y对的训练示例。稍后，我们将采用这个训练集，其中x是具有12个特征的输入，而y只是数字。 我们将使用均方误差损失来训练一个新网络，以尝试将y预测为输入x的函数。</p><p>以上描述的只是我们将使用的算法的一部分，接下来看看它们是如何组合成一个学习Q函数的完整算法的。</p><p>首先，我们将采用我们的神经网络并随机初始化神经网络的所有参数。最初我们不知道Q函数是什么，我们只是完全随机的赋值。我们假设这个神经网络是我们对Q函数的初始随机猜测。这有点像你在训川练线性回归时随机初始化所有参数，然后使用梯度下降来改进参数。现在随机初始化没关系。重要的是算法是否可以慢慢改进参数以获得更好的估计。 <img src="/img/机器学习07-强化学习/Pasted%20image%2020230722203836.png" srcset="/img/loading.gif" lazyload> 接下来，我们将重复执行以下操作；我们将对着陆器中采取动作，然后得到众多元组 <span class="math inline">\((s,a,R(s),s^{\prime})\)</span> ， 我们要做的是存储这些元组的10,000个最新示例。当你运行这个算法时，你会看到着陆器有很多步骤，可能有几十万个步骤。为了确保我们最终不会使用过多的计算机内存，通常的做法是只记住我们在 MDP中看到的10,000个最近的此类元组。这种仅存储最近示例的技术在强化学习算法中称为回放缓冲区。 <img src="/img/机器学习07-强化学习/Pasted%20image%2020230722203905.png" srcset="/img/loading.gif" lazyload> 日前，我们只是让月球着陆器随机飞行，有时会坠毁，有时不会坠毁，并根据学习算法经验获取这些元组。</p><p>有时我们会训练神经网络，为了训练神经网络，下面是我们要做的。</p><p>我们将查看我们保存的这10,000个最近的元组，并创建一个包含10,000个示例的训练集。 训练集需要很多对x和y。对于我们的训练示例，x将是s来自元组的 <span class="math inline">\((s,a)\)</span> 这一部分。我们希望神经网络尝试预测的目标值是<span class="math inline">\(\mathbf{y}=R(s)+\:\gamma\max_{a^{\prime}}Q(s^{\prime},a^{\prime})\)</span> 。我们如何获得Q的这个值？最初是我们随机初始化的这个神经网络，是一个猜测值。 <img src="/img/机器学习07-强化学习/Pasted%20image%2020230722204559.png" srcset="/img/loading.gif" lazyload> 创建这10,000个训练示例后，我们将拥有训练示例 <span class="math inline">\(x^1\)</span> 、<span class="math inline">\(y^1\)</span> 到 <span class="math inline">\(x^{10,000}\)</span> 、<span class="math inline">\(y^{10,000}\)</span> 。我们将训练一个神经网络，我将把新的神经网络称为Qnew,这样 <span class="math inline">\(Q_{new}(s,a)\approx y\)</span> 。这正是训练那个神经网络 <span class="math inline">\(f_{W,B}(x)\approx y\)</span> 。现在，这个神经网络应该稍微更好地估计Q函数是什么，接着我们要做的是将Q设置为我们刚刚学习的新神经网络 <img src="/img/机器学习07-强化学习/Pasted%20image%2020230722210052.png" srcset="/img/loading.gif" lazyload> 事实证明，如果您从真正随机猜测Q函数开始运行此算法，然后使用Bellman方 Set-new.程反复尝试改进Q函数的估计值。,9然后通过反复执行此操作，采取大量操作，训川练模型，这将改进您对Q函数的猜 Set-new. Jw,BX)XU测.,9对于您训川练的下一个模型，您现在对什么是Q函数有了更好的估计。那么你训练的下一个模型会更好当您更新Q等于Qnew时。然后下次训练 <span class="math inline">\(Q(s^{\prime},a^{\prime})\)</span> 时，将是一个更好的估计值。</p><p>当你在每次迭代中运行这个算法时，<span class="math inline">\(Q(s^{\prime},a^{\prime})\)</span> 有望成为Q函数的更好估计，这样当你运行算法足够长的时间时，这个估计值会越来越接近真实值，这样你就可以用它来挑选，希望是好的动作或 MDP。</p><p>这个算法称为DQN算法（Deep Q-Network），因为使用深度学习和神经网络训练模型来学习Q函数。</p></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="category-chain-item">机器学习</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">#机器学习</a></div></div><div class="license-box my-3"><div class="license-title"><div>机器学习07-强化学习</div><div>https://zhou1317fe5.github.io/2023/07/22/机器学习07-强化学习/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>Zhou1317fe5</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2023年7月22日</div></div><div class="license-meta-item"><div>许可协议</div><div><a target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"></article><article class="post-next col-6"><a href="/2023/07/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A006-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/" title="机器学习06-推荐系统"><span class="hidden-mobile">机器学习06-推荐系统</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div><div class="statistics"><span id="busuanzi_container_site_pv" style="display:none">总访问量 <span id="busuanzi_value_site_pv"></span> 次 </span><span id="busuanzi_container_site_uv" style="display:none">总访客数 <span id="busuanzi_value_site_uv"></span> 人</span></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script>window.MathJax?(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise()):window.MathJax={tex:{inlineMath:{"[+]":[["$","$"]]}},loader:{load:["ui/lazy"]},options:{renderActions:{insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(t=>{let e=t.parentNode;"li"===e.nodeName.toLowerCase()&&e.parentNode.classList.add("has-jax")})},"",!1]}}},Fluid.events.registerRefreshCallback((function(){"MathJax"in window&&MathJax.startup.document&&"function"==typeof MathJax.startup.document.state&&(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise())}))</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script src="/js/local-search.js"></script><script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>