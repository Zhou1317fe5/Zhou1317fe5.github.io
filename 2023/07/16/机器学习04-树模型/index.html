<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png"><link rel="icon" href="/img/fluid.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="Zhou1317fe5"><meta name="keywords" content=""><meta name="description" content="分类决策树   在每个节点上如何选择特征进行拆分？   选择要拆分的特征以尝试最大化纯度    何时停止分支？    分出类别 达到最大深度 当纯度低于阈值 节点的示例数量低于阈值  纯度 熵（entropy）的定义，它是衡量一组数据不纯度（混乱程度）的指标。 熵越小，纯度越高  熵函数  在决策树构建的过程中，我们希望通过选择特征使得熵最小化，即寻找能够最"><meta property="og:type" content="article"><meta property="og:title" content="机器学习04-树模型"><meta property="og:url" content="http://example.com/2023/07/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A004-%E6%A0%91%E6%A8%A1%E5%9E%8B/index.html"><meta property="og:site_name" content="Zhou1317fe5"><meta property="og:description" content="分类决策树   在每个节点上如何选择特征进行拆分？   选择要拆分的特征以尝试最大化纯度    何时停止分支？    分出类别 达到最大深度 当纯度低于阈值 节点的示例数量低于阈值  纯度 熵（entropy）的定义，它是衡量一组数据不纯度（混乱程度）的指标。 熵越小，纯度越高  熵函数  在决策树构建的过程中，我们希望通过选择特征使得熵最小化，即寻找能够最"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://example.com/img/4%E6%A0%91%E6%A8%A1%E5%9E%8B/Pasted%20image%2020230715193726.png"><meta property="og:image" content="http://example.com/img/4%E6%A0%91%E6%A8%A1%E5%9E%8B/Pasted%20image%2020230715170822.png"><meta property="og:image" content="http://example.com/img/4%E6%A0%91%E6%A8%A1%E5%9E%8B/Pasted%20image%2020230715171547.png"><meta property="og:image" content="http://example.com/img/4%E6%A0%91%E6%A8%A1%E5%9E%8B/Pasted%20image%2020230715172451.png"><meta property="og:image" content="http://example.com/img/4%E6%A0%91%E6%A8%A1%E5%9E%8B/Pasted%20image%2020230715172601.png"><meta property="og:image" content="http://example.com/img/4%E6%A0%91%E6%A8%A1%E5%9E%8B/Pasted%20image%2020230715183454.png"><meta property="og:image" content="http://example.com/img/4%E6%A0%91%E6%A8%A1%E5%9E%8B/Pasted%20image%2020230715191714.png"><meta property="og:image" content="http://example.com/img/4%E6%A0%91%E6%A8%A1%E5%9E%8B/Pasted%20image%2020230715193336.png"><meta property="og:image" content="http://example.com/img/4%E6%A0%91%E6%A8%A1%E5%9E%8B/Pasted%20image%2020230715195850.png"><meta property="og:image" content="http://example.com/img/4%E6%A0%91%E6%A8%A1%E5%9E%8B/Pasted%20image%2020230715200031.png"><meta property="og:image" content="http://example.com/img/4%E6%A0%91%E6%A8%A1%E5%9E%8B/Pasted%20image%2020230715201645.png"><meta property="og:image" content="http://example.com/img/4%E6%A0%91%E6%A8%A1%E5%9E%8B/Pasted%20image%2020230715201734.png"><meta property="og:image" content="http://example.com/img/4%E6%A0%91%E6%A8%A1%E5%9E%8B/Pasted%20image%2020230715201805.png"><meta property="og:image" content="http://example.com/img/4%E6%A0%91%E6%A8%A1%E5%9E%8B/Pasted%20image%2020230715212515.png"><meta property="og:image" content="http://example.com/img/4%E6%A0%91%E6%A8%A1%E5%9E%8B/Pasted%20image%2020230715222743.png"><meta property="og:image" content="http://example.com/img/4%E6%A0%91%E6%A8%A1%E5%9E%8B/Pasted%20image%2020230716093526.png"><meta property="article:published_time" content="2023-07-16T15:08:11.000Z"><meta property="article:modified_time" content="2023-07-17T13:22:30.261Z"><meta property="article:author" content="Zhou1317fe5"><meta property="article:tag" content="机器学习"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="http://example.com/img/4%E6%A0%91%E6%A8%A1%E5%9E%8B/Pasted%20image%2020230715193726.png"><meta name="referrer" content="no-referrer-when-downgrade"><title>机器学习04-树模型 - Zhou1317fe5</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"example.com",root:"/",version:"1.9.4",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1}},search_path:"/local-search.xml"};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><meta name="generator" content="Hexo 6.3.0"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>Zhou1317fe5</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/Post_banner_img.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="机器学习04-树模型"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2023-07-16 23:08" pubdate>2023年7月16日 晚上</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 2.7k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 23 分钟 </span><span id="busuanzi_container_page_pv" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="busuanzi_value_page_pv"></span> 次</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 style="display:none">机器学习04-树模型</h1><div class="markdown-body"><h1 id="分类决策树">分类决策树</h1><p><img src="/img/4树模型/Pasted%20image%2020230715193726.png" srcset="/img/loading.gif" lazyload></p><ol type="1"><li>在每个节点上如何选择特征进行拆分？</li></ol><ul><li>选择要拆分的特征以尝试最大化纯度 <img src="/img/4树模型/Pasted%20image%2020230715170822.png" srcset="/img/loading.gif" lazyload></li></ul><ol start="2" type="1"><li>何时停止分支？ <img src="/img/4树模型/Pasted%20image%2020230715171547.png" srcset="/img/loading.gif" lazyload></li></ol><ul><li>分出类别</li><li>达到最大深度</li><li>当纯度低于阈值</li><li>节点的示例数量低于阈值</li></ul><h1 id="纯度">纯度</h1><p>熵（entropy）的定义，它是衡量一组数据不纯度（混乱程度）的指标。 熵越小，纯度越高</p><p><img src="/img/4树模型/Pasted%20image%2020230715172451.png" srcset="/img/loading.gif" lazyload></p><p>熵函数 <img src="/img/4树模型/Pasted%20image%2020230715172601.png" srcset="/img/loading.gif" lazyload> 在决策树构建的过程中，我们希望通过选择特征使得熵最小化，即寻找能够最大程度减少数据集的纯度。通过计算每个特征的熵，并对其进行比较，我们可以选择最佳的划分特征来构建决策树的节点。</p><h1 id="选择拆分信息增益">选择拆分信息增益</h1><p>在决策树中，熵的减少称为信息增益（information gain） 信息增益是决策树算法中用于选择最佳划分特征的指标之一。它表示通过使用某个特征对数据集进行划分所能获得的整体熵减少量。</p><p>在计算信息增益时，首先需要计算划分前的数据集的熵（也称为初始熵）。然后，对于每个候选划分特征，并计算每个划分子集的熵。最后，通过计算划分前的熵与所有划分子集熵的加权平均来计算信息增益。</p><p><img src="/img/4树模型/Pasted%20image%2020230715183454.png" srcset="/img/loading.gif" lazyload> 其中，初始熵是对整个数据集进行计算得到的熵，子集熵是对每个划分子集进行计算得到的熵，子集样本数是每个划分子集中的样本数量，总样本数是整个数据集的样本总数。</p><p>未分支前有5个猫，5个狗，因此初始熵熵 <span class="math inline">\(H(0.5)=1\)</span> 。 分支后加权平均的熵为上图红色方框，未分支前的熵减去分支后加权平均的熵即为信息增益。</p><p>具体计算公式如下：</p><p>信息增益 = 初始熵 - ∑(子集熵 * 子集样本数 / 总样本数) <img src="/img/4树模型/Pasted%20image%2020230715191714.png" srcset="/img/loading.gif" lazyload></p><p>信息增益表示通过使用某个特征划分数据集，分支后，<strong>整体熵相对于初始熵的减少量</strong>。我们希望选择具有<strong>最大</strong>信息增益的特征作为划分依据，因为它能够最大程度地<strong>减少数据集的不确定性（混乱程度）</strong>，提供更多有用的信息。</p><p>总结：熵代表混乱程度，熵越高，混乱程度越高；信息增益越大，熵减少的越大，说明从高熵减低到低熵，进而说明从高混乱程度到低混乱程度的变化。</p><h1 id="决策树步骤">决策树步骤</h1><p><img src="/img/4树模型/Pasted%20image%2020230715193336.png" srcset="/img/loading.gif" lazyload> 从根节点开始，将所有的示例放置在根节点上。 计算所有可能特征的信息增益，并选择具有最高信息增益的特征。 根据所选特征划分数据集，并创建决策树的左右分支。 不断重复划分过程，直到满足停止准则为止： - 当一个节点是100%属于某一类别时 - 当划分一个节点会导致树超过最大深度时 - 额外划分带来的信息增益低于阈值时。</p><h1 id="如何处理连续值特征">如何处理连续值特征</h1><p>在决策树中处理连续性数值的特征通常有两种方法：二元切分和多元切分。</p><ol type="1"><li><p>二元切分（Binary Splitting）：通过选择一个阈值来将连续性数值特征进行二元切分。对于给定的连续性特征，可以选择一个合适的阈值将数据集划分为两个子集。例如，如果特征是年龄，可以选择一个年龄阈值，将数据分为小于等于阈值和大于阈值的两个子集。然后，根据这个划分继续构建决策树。</p></li><li><p>多元切分（Multi-way Splitting）：与二元切分不同，多元切分将连续性数值特征划分为多个范围或区间。通过指定多个阈值或范围，将连续性特征划分为多个子集。例如，对于年龄特征，可以指定年龄范围(0-10岁, 11-20岁, 21-30岁, 等等)作为划分依据。然后，根据这个划分继续构建决策树。</p></li></ol><p>无论是二元切分还是多元切分，决策树都会根据划分结果计算信息增益或其他评估指标，选择最佳的划分。</p><p>在示例中增加一列体重特征： <img src="/img/4树模型/Pasted%20image%2020230715195850.png" srcset="/img/loading.gif" lazyload> 对于体重，我们选取不同的阈值来分割，然后计算对应的信息增益 <img src="/img/4树模型/Pasted%20image%2020230715200031.png" srcset="/img/loading.gif" lazyload> 当选择9时，信息增益最大，因此我们用是否大于9磅进行分支。</p><h1 id="回归决策树">回归决策树</h1><p>预测体重： <img src="/img/4树模型/Pasted%20image%2020230715201645.png" srcset="/img/loading.gif" lazyload></p><p><img src="/img/4树模型/Pasted%20image%2020230715201734.png" srcset="/img/loading.gif" lazyload></p><p><img src="/img/4树模型/Pasted%20image%2020230715201805.png" srcset="/img/loading.gif" lazyload> 方差 方差减小最大</p><h1 id="有放回抽样样本">有放回抽样样本</h1><p>共十个样本，五个猫 五个狗，有放回的抽取10次，组成一个新的训练集。 <img src="/img/4树模型/Pasted%20image%2020230715212515.png" srcset="/img/loading.gif" lazyload></p><h1 id="装袋决策树">装袋决策树</h1><p>有一个大小为m的训练集。</p><p>b等于1到B，我们将训练集放入袋子中，利用有放回抽样选出B次大小为m的新的训练集，我们在这些新的训练集中训练一个个决策树。 <img src="/img/4树模型/Pasted%20image%2020230715222743.png" srcset="/img/loading.gif" lazyload> 选出一个新的训练集，训练一个决策树；选出另外一个新的训练集，在训练一个决策树，就这样选出B个训练集，训练出B个决策树。 B则代表决策树的数量， 已经构建了一个包含B棵不同树的集合，然后让这些树对预测的结果进行投票。</p><p>当B远大于100时，算法的性能并不会有显著的增加，只会降低计算速度。</p><p>这种从袋子中选出新的训练集创建的决策树称为装袋决策时，所以B代表的是bag。</p><p>弊端是，这种抽取训练集的方法，会使许多树在根节点或者根节点附近的某些节点使用相同的特征进行分支。因此对算法进行改进，尝试随机化每个节点的特征选择，使数之间变得更加不同这就是随机森林。 # 随机森林</p><p>在每个节点上，在选择用于划分的特征时，如果有n个可用特征，则从中随机选择一个大小为k（k &lt; n）的特征子集，并只允许算法从该特征子集中进行选择，从中选择出具有最高信息增益的特征进行分支。 当n很大时，K值的典型选择是它的平方根 <span class="math inline">\({\mathrm{k=\sqrt{n}}}\)</span> 。</p><p>因此，这意味着训练集的任何微小变化都不太可能对整个随机森林算法的整体输出产生巨大影响。</p><h1 id="xgboost">XGBoost</h1><p>依然使用有放回的抽样方式，创建一个大小为m的新训练集。但不是以相等的（1/m）概率从所有示例中抽取，而是更有可能抽取那些先前训练树分类错误的示例。</p><p><strong>优势：</strong> 助推树的开源实现 快速高效的实现 可选择良好的默认拆分标准和何时停止拆分的标准 内置正则化，防止过拟合 机器学习竞赛（如Kaggle竞赛）中极具竞争力的算法 # 决策树VS神经网络 <img src="/img/4树模型/Pasted%20image%2020230716093526.png" srcset="/img/loading.gif" lazyload></p><ol type="1"><li><p>数据集的特征：决策树在处理具有离散特征和类别特征的数据集时表现较好，而神经网络在处理具有连续特征和大量样本的数据集时通常更有效。</p></li><li><p>数据集的大小：如果数据集非常大，神经网络通常能够通过深层次的学习来发现复杂的模式和关联，因此神经网络可能更适合用于大规模数据集。</p></li><li><p>解释性要求：决策树提供了较好的可解释性，可以根据节点分裂规则和特征重要性进行解释。而神经网络通常被认为是“黑盒”模型，其内部权重和参数难以解释。</p></li><li><p>预测性能需求：神经网络在某些高度复杂的问题上可能具有更强的预测性能，尤其是当数据集具有深层次的非线性关系时。集成树模型如随机森林和梯度提升树则在处理噪声和异常值方面表现较好。</p></li><li><p>训练效率：决策树通常更容易训练和调优，而神经网络需要更多的计算资源和时间来训练和优化。</p></li><li><p>预测速度：决策树是一种基于规则的模型，可以快速进行预测。相比之下，神经网络需要通过多层次的计算才能进行预测，因此在实时性要求较高的场景中可能不太适用。</p></li></ol><p>最终的选择应该根据具体问题的需求和数据集特征来决定。在某些情况下，使用决策树和集成树（如随机森林、梯度提升树）能够提供较好的性能和解释性；而在其他情况下，神经网络可能会更适合。还可以尝试将两种算法结合起来，如使用神经网络进行特征提取，然后使用决策树进行分类或回归。</p></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="category-chain-item">机器学习</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">#机器学习</a></div></div><div class="license-box my-3"><div class="license-title"><div>机器学习04-树模型</div><div>http://example.com/2023/07/16/机器学习04-树模型/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>Zhou1317fe5</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2023年7月16日</div></div><div class="license-meta-item"><div>许可协议</div><div><a target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2023/07/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A005-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" title="机器学习05-无监督学习"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">机器学习05-无监督学习</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2023/07/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A003-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/" title="机器学习03-模型评估"><span class="hidden-mobile">机器学习03-模型评估</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div><div class="statistics"><span id="busuanzi_container_site_pv" style="display:none">总访问量 <span id="busuanzi_value_site_pv"></span> 次 </span><span id="busuanzi_container_site_uv" style="display:none">总访客数 <span id="busuanzi_value_site_uv"></span> 人</span></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script>window.MathJax?(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise()):window.MathJax={tex:{inlineMath:{"[+]":[["$","$"]]}},loader:{load:["ui/lazy"]},options:{renderActions:{insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(t=>{let e=t.parentNode;"li"===e.nodeName.toLowerCase()&&e.parentNode.classList.add("has-jax")})},"",!1]}}},Fluid.events.registerRefreshCallback((function(){"MathJax"in window&&MathJax.startup.document&&"function"==typeof MathJax.startup.document.state&&(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise())}))</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script src="/js/local-search.js"></script><script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>