<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png"><link rel="icon" href="/img/fluid.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="Zhou1317fe5"><meta name="keywords" content=""><meta name="description" content="线性回归模型（Linear Regression） 假设函数 \[f_{w,b}(x)&#x3D;w_{1}x_{1}+w_{2}x_{2}+...+w_{n}x_{n}+b\] 如何确定模型中的参数取什么值? 用代价函数 代价函数 代价函数（Cost Function）是用来衡量预测值与实际值之间的误差。它的目的是找到一组参数，使得预测值与实际值之间的误差最小（确定最优参数）。评价模型是否拟合的"><meta property="og:type" content="article"><meta property="og:title" content="机器学习00-监督学习"><meta property="og:url" content="https://zhou1317fe5.github.io/2023/07/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A000-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/index.html"><meta property="og:site_name" content="Zhou1317fe5"><meta property="og:description" content="线性回归模型（Linear Regression） 假设函数 \[f_{w,b}(x)&#x3D;w_{1}x_{1}+w_{2}x_{2}+...+w_{n}x_{n}+b\] 如何确定模型中的参数取什么值? 用代价函数 代价函数 代价函数（Cost Function）是用来衡量预测值与实际值之间的误差。它的目的是找到一组参数，使得预测值与实际值之间的误差最小（确定最优参数）。评价模型是否拟合的"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Snipaste_2023-06-09_11-22-50.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Snipaste_2023-06-12_19-02-10.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Snipaste_2023-05-27_11-58-53.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Snipaste_2023-06-08_16-08-53.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Snipaste_2023-06-08_17-32-14.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Snipaste_2023-06-08_17-54-18.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Snipaste_2023-06-08_18-10-02.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Snipaste_2023-06-09_08-48-39.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Snipaste_2023-06-12_17-13-32.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Snipaste_2023-06-13_16-57-47.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Snipaste_2023-06-14_09-11-09.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Snipaste_2023-06-14_09-37-48.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Snipaste_2023-06-14_09-59-48.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Snipaste_2023-06-14_10-00-42.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Snipaste_2023-06-14_10-13-49.png"><meta property="og:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Snipaste_2023-06-14_10-18-04.png"><meta property="article:published_time" content="2023-07-07T11:31:09.000Z"><meta property="article:modified_time" content="2023-09-06T03:34:15.607Z"><meta property="article:author" content="Zhou1317fe5"><meta property="article:tag" content="机器学习"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://zhou1317fe5.github.io/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Snipaste_2023-06-09_11-22-50.png"><meta name="referrer" content="no-referrer-when-downgrade"><title>机器学习00-监督学习 - Zhou1317fe5</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="/css/iconfont_csdn/iconfont.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"zhou1317fe5.github.io",root:"/",version:"1.9.4",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1}},search_path:"/local-search.xml"};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><meta name="generator" content="Hexo 6.3.0"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>Zhou1317fe5</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/Post_banner_img.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="机器学习00-监督学习"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2023-07-07 19:31" pubdate>2023年7月7日 晚上</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 6.3k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 53 分钟 </span><span id="busuanzi_container_page_pv" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="busuanzi_value_page_pv"></span> 次</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 style="display:none">机器学习00-监督学习</h1><div class="markdown-body"><h1 id="线性回归模型linear-regression">线性回归模型（Linear Regression）</h1><h2 id="假设函数">假设函数</h2><p><span class="math display">\[f_{w,b}(x)=w_{1}x_{1}+w_{2}x_{2}+...+w_{n}x_{n}+b\]</span></p><p>如何确定模型中的参数取什么值? 用代价函数</p><h2 id="代价函数">代价函数</h2><p><a href="如何最简单、通俗地理解代价函数？.md">代价函数</a>（Cost Function）是用来衡量预测值与实际值之间的误差。它的目的是找到一组参数，使得预测值与实际值之间的误差最小（确定最优参数）。评价模型是否拟合的准确，值越小，拟合的越准确。</p><p><strong>线性回归的代价函数</strong>：最小二乘法。所谓“二乘”就是平方的意思。</p><p><span class="math display">\[J(w,b)=\frac{1}{2m}\sum_{i=1}^m\bigl(f_{w,b}\bigl(x^{(i)}\bigr)-y^{(i)}\bigr)^2\]</span></p><blockquote><p>此处1/2m中的2仅为了后续求导计算时，简化计算步骤</p></blockquote><p><strong>注意：</strong></p><p>代价函数中的 <span class="math display">\[(f_{w,b}\bigl(x^{(i)}\bigr)-y^{(i)}\bigr)^2\]</span> 部分叫损失函数（Loss Function）用L表示 <span class="math display">\[L=(f_{w,b}\bigl(x^{(i)}\bigr)-y^{(i)}\bigr)^2\]</span> 损失函数衡量的是你在一个训练样例上的表现如何，它是通过总结你随后获得的所有训练样列的损失；而代价函数衡量你在整个训练集上的表现。</p><p>因此，代价函数值是在损失函数值求和后除以训练量。</p><p>线性回归代价函数图：</p><p><img src="/img/机器学习/Snipaste_2023-06-09_11-22-50.png" srcset="/img/loading.gif" lazyload></p><p>代价函数图自变量为参数，因变量为代价函数值。有代价函数图了，如何自动到达代价函数图的最小值点，即模型拟合最优的点？</p><h2 id="梯度下降gradient-descent">梯度下降（Gradient descent ）</h2><p>从代价函数图中找到最小值所在的点。有没有一种算法可以自动地、快速求出使得代价函数最小的点呢？有，那就是<strong>梯度下降</strong></p><ul><li><p>梯度下降的工作是找到希望最小化代价函数J的参数w和b</p></li><li><p>如何快速到达最优点：从山顶一步一步走到山谷，先在原地转一圈，选最陡的地方走一步，在转一圈，在选最陡的一步。这一步一步就是多个梯度下降的步骤。</p></li><li><p>不同的w，b值决定你在那个位置</p></li><li><p>梯度下降算法 <span class="math display">\[ \begin{aligned} w &amp;= w - \alpha \frac{\partial}{d\omega}J(\omega.b) \\ b &amp;= b - \alpha \frac{\partial}{d\omega}J(\omega.b) \end{aligned} \]</span></p></li><li><p>梯度下降（Gradient descent ）同时更新w，b。同时更新目的在于，能够确保是在原点寻找最陡方向。赋值就像迈出了一步，先迈出左脚寻找最陡的地方和在原地寻找最陡的地方不一样。</p></li><li><p>α学习率决定你迈出的一步有多大</p></li><li><p>导数决定梯度下降方向，学习率决定步长</p></li></ul><p><img src="/img/机器学习/Snipaste_2023-06-12_19-02-10.png" srcset="/img/loading.gif" lazyload></p><p><strong>线性回归的梯度下降：</strong> <span class="math display">\[\begin{aligned} w&amp;=w-\alpha\frac{1}{m}\sum_{i=1}^{m}(f_{w,b}\big(x^{(i)}\big)-y^{(i)})x^{(i)}\\ b &amp;=b-\alpha\frac{1}{m}\sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)}) \end{aligned}\]</span></p><h2 id="多维特征">多维特征</h2><ul><li>矢量化使代码更简短 运行更高效</li></ul><h2 id="特征缩放feature-scaling">特征缩放（<strong>Feature Scaling</strong>）</h2><p>房价</p><p><span class="math display">\[ pr i c e=w_{1}x_{1}+w_{2}x_{2}+b \]</span> <span class="math inline">\(x_{1}\)</span> : size（feet^2）range :300-2000 <span class="math inline">\(x_{2}\)</span> : # bedrooms range :0-5</p><p>当特征的可能值较大时，如房间大小(<span class="math inline">\(x_{1}\)</span>)，则其参数(<span class="math inline">\(w_{1}\)</span>)的合理值将相对较小 当特征的可能值较小时，如卧室数量(<span class="math inline">\(x_{2}\)</span>)，则其参数(<span class="math inline">\(w_{2}\)</span>)的合理值将相对较大</p><p><img src="/img/机器学习/Snipaste_2023-05-27_11-58-53.png" srcset="/img/loading.gif" lazyload></p><p>特征散点图如左上时，其代价函数在等高线图中的样子如右上所示，为椭圆形，梯度下降可能会在它最终找到全局最小值之前来回弹跳很长的时间。</p><p>所以当你有不同的特征，它们的取值范围非常不同时，它可能会导致梯度下降运行缓慢，但重新缩放不同的特征，使它们都具有可比较的取值范围，让梯度下降法运行得更快。</p><h3 id="如何实现特征缩放">如何实现特征缩放？</h3><p>将数据标准化、归一化</p><p><strong>平均值归一化方法（Mean normalization）</strong></p><p>如果要将该值映射到[-1,1]区间内，则需要计算特征的平均值<span class="math inline">\(X_{Mean}\)</span>，使用平均值归一化方法公式为</p><p><span class="math display">\[x=\frac{X-X_{Mean}}{X_{Max}-X_{Min}}\]</span></p><p><strong>Z-score标准化方法（Z-score Normalization）</strong></p><p>标准化法需要提前计算特征的均值<span class="math inline">\(X_{Mean}\)</span>和标准差<span class="math inline">\(\sigma\)</span>，标准化后的变量值围绕0上下波动，大于0说明高于平均水平，小于0说明低于平均水平。 <span class="math display">\[x={\frac{X-X_{M e a n}}{\sigma}}\]</span></p><h3 id="缩放的范围是否有固定的要求">缩放的范围是否有固定的要求？</h3><p><img src="/img/机器学习/Snipaste_2023-06-08_16-08-53.png" srcset="/img/loading.gif" lazyload></p><p>最好是-1到1，实在不行，最大是-3到3，最小是-0.3到0.3，都能接受。</p><p>基本意思是，保证各个特征的数量级一致，通过缩放尽量让所有特征的取值在差不多范围，在一个数量级以内的特征可以不考虑缩放，这样它们的变化对预测值的影响都是接近的。</p><h2 id="判断梯度下降是否收敛">判断梯度下降是否收敛？</h2><p>在运行梯度下降时，怎样才能知道梯度下降是否有效，是否正在找到全局最小值或接近它的值？为了能够选择更好的学习率。</p><p>判断梯度下降运行是否良好，用<strong>学习曲线</strong>（learning curve），横轴为梯度下降迭代次数，纵轴为代价函数J。</p><p><img src="/img/机器学习/Snipaste_2023-06-08_17-32-14.png" srcset="/img/loading.gif" lazyload> 当运行良好时，学习曲线是一直下降的，直到变平（即收敛)；当出现先下降后又上升的情况时，可能是学习率错误，或者程序错误。查看此学习曲线，您可以尝试发现梯度下降是否收敛。</p><p>另一种方法是<strong>自动收敛测试</strong>（Automatic convergence test） 令<span class="math inline">\(\epsilon\)</span> ”epsilon“等于一个非常小的数，如果代价J在一次迭代中减少的幅度小于这个数字epsilon,那么很可能位于学习曲线的平坦部分，可以宣布收敛。但是选择正确的阈值epsilon非常困难。</p><h2 id="如何设置学习率">如何设置学习率？</h2><p>当梯度下降函数运行有错误时，可能是学习率太大了或者程序有错误。</p><ul><li>学习率过大，调小学习率</li><li>程序有错误，查看公式是否写错，比如把减号写成加号</li></ul><p><strong>尝试不同的学习率并画对应的学习曲线</strong></p><p>……0.001，0.003，0.01，0.03，0.1，0.3，1 ……</p><p><img src="/img/机器学习/Snipaste_2023-06-08_17-54-18.png" srcset="/img/loading.gif" lazyload></p><p>在太小，太大的学习率之间调试。</p><h2 id="特征工程feature-engineering">特征工程（feature engineering）</h2><p>利用领域知识和现有数据，创造出新的特征，用于机器学习算法；可以手动（manual）或自动（automated）。</p><p>如果新特征是原始特征的次方，那么特征缩放会非常重要</p><p><img src="/img/机器学习/Snipaste_2023-06-08_18-10-02.png" srcset="/img/loading.gif" lazyload></p><h1 id="分类classification">分类（Classification）</h1><h2 id="逻辑回归logistic-regression">逻辑回归（logistic regression）</h2><h3 id="假设函数-1">假设函数</h3><p>首先介绍sigmoid函数，又称logistic函数</p><p><img src="/img/机器学习/Snipaste_2023-06-09_08-48-39.png" srcset="/img/loading.gif" lazyload></p><p><span class="math display">\[g(z)=\frac{1}{1+e^{-z}}\quad0&lt;g(z)&lt;1\]</span></p><p>当z取无限大时，函数趋向于1；当取无限小时，趋向于0。该函数表示结果输出为1的概率。</p><p>令 <span class="math display">\[\text{Z}=\overrightarrow{W}\cdot\overrightarrow{X}+b\]</span> 则逻辑回归的假设函数为 <span class="math display">\[f_{\overrightarrow{W},b}(\overrightarrow{X})=g(\vec{W}\cdot\vec{X}+b)=\frac{1}{1+e^{-(\overrightarrow{W}\cdot\overrightarrow{X}+b)}}\]</span></p><h3 id="代价函数-1">代价函数</h3><p>首先我们回顾一下之前学过的一个代价函数，线性回归的代价函数： <span class="math display">\[J(w,b)=\frac{1}{2m}\sum_{i=1}^m\bigl(f_{w,b}\bigl(x^{(i)}\bigr)-y^{(i)}\bigr)^2\]</span> 如果把此代价函数用在逻辑回归中会怎么样？我们看一下应用该代价函数的代价函数图：</p><p><img src="/img/机器学习/Snipaste_2023-06-12_17-13-32.png" srcset="/img/loading.gif" lazyload></p><p>我们发现，，此函数不是凸函数，如果用梯度下降的话，找到的是局部最优，不是整体最优。</p><pre><code class="hljs">凸函数有个很好的性质，只要能证明是凸函数，最终解一定是全局最优解，即局部最小值是全局最小值。</code></pre><p>因此，最小二乘法代价函数不适合逻辑回归。</p><p>我们定义<strong>逻辑回归的损失函数</strong>如下（后续说明为何定义该函数）: $$L\left(f_{\overrightarrow{{{w}}},b}\left(\overrightarrow{{{x}}}^{(i)}\right),y^{(i)}\right)=\left\{\begin{array}{rl}{{-\log\left(f_{\overrightarrow{{{w}}},b}\left(\overrightarrow{{{x}}}^{(i)}\right)\right)}}&{{\mathrm{if~}y^{(i)}=1}}\\ {{-\log\left(1-f_{\overrightarrow{{{w}}},b}\left(\overrightarrow{{{x}}}^{(i)}\right)\right)}}&{{\mathrm{if~}y^{(i)}=0}}\end{array}\right. $$</p><p>因此，逻辑回归的代价函数如下：</p><p><span class="math display">\[J(\overrightarrow{W},b)=\frac{1}{m}\sum_{i=1}^{m}L\bigl(f_{\overrightarrow{W},b}\bigl(\overrightarrow{x}^{(i)}\bigr),y^{(i)}\bigr)\]</span></p><p>由于y的取值只能是0或1，所以<strong>简化</strong>后的损失函数及代价函数如下： $$\begin{aligned} &L{\big(}f_{\overrightarrow{{{w}}},b}{\big(}\overrightarrow{{{x}}}^{(i)}{\big)},y^{(i)}{\big)}=-y^{(i)}\mathrm{log}{\Big(}f_{\overrightarrow{{{w}}},b}{\big(}\overrightarrow{{{x}}}^{(i)}{\big)}{\Big)}-{\big(}1-y^{(i)}\big){\mathrm{log}}{\Big(}1-f_{\overrightarrow{{{w}}},b}{\big(}\overrightarrow{{{x}}}^{(i)}{\big)}{\Big)} \\ &J(\overrightarrow{w},b)=\frac{1}{m}\sum_{i=1}^{m}[L\big(f_{\overrightarrow{w},b}\big(\overrightarrow{x}^{(i)}\big),y^{(i)}\big)] \end{aligned}$$</p><p>那么为什么选择了这个函数作为代价函数呢？</p><p>这个特定的代价函数是使用称为<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/148968222?utm_id=0">最大似然估计</a>的统计原理从统计中推导出来的。最大似然估计是一种常用的参数估计方法，它基于样本数据，通过寻找最有可能产生这些数据的参数值，来确定模型的参数。</p><h3 id="梯度下降">梯度下降</h3><p><strong>==逻辑回归的梯度下降==</strong> <span class="math display">\[\begin{aligned} w_{j}&amp;=w_{j}-\alpha\frac{1}{m}\sum_{i=1}^{m}(f_{w,b}\big(x^{(i)}\big)-y^{(i)})x_{j}^{(i)}\\ b &amp;=b-\alpha\frac{1}{m}\sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)}) \end{aligned}\]</span></p><p>注意：形式与线性回归一样，但是<span class="math inline">\(f_{w,b}(x)\)</span> 表达式不同。</p><h1 id="过拟合">过拟合</h1><h2 id="欠拟合过拟合">欠拟合、过拟合</h2><p><img src="/img/机器学习/Snipaste_2023-06-13_16-57-47.png" srcset="/img/loading.gif" lazyload></p><p>左图，欠拟合（underfit），也称作高偏差（High-bias），即对于当前数据集的拟合程度不够，欠拟合的特征是在训练集和测试集上的准确率都不好；</p><p>中图，拟合刚好的状态，具有泛化能力；</p><p>右图，过拟合（overfit），也称作高方差（High variance），过拟合对于当前训练数据拟合得太好了，以至于模型只在当前的训练集上表现很好，而在其他数据集上表现就不是那么好，所以过拟合的特征是在训练集上准确率很高而在测试集上表现一般。</p><h2 id="解决过拟合">解决过拟合</h2><p>方法： 1. 收集更多的数据 2. 减少特征数量 3. 正则化</p><h3 id="特征工程">特征工程</h3><h3 id="正则化">正则化</h3><p>将正则化应用在代价函数中，使用正则化来减小参数的大小</p><p><img src="/img/机器学习/Snipaste_2023-06-14_09-11-09.png" srcset="/img/loading.gif" lazyload></p><p>如何理解上式？</p><p>因为<span class="math inline">\(w_3^2\)</span>和<span class="math inline">\(w_4^2\)</span>前的系数非常大，要想使整个式子最小化，只能令<span class="math inline">\(w_3^2\)</span>和<span class="math inline">\(w_4^2\)</span> 非常小（<span class="math inline">\(\approx0\)</span>），<span class="math inline">\(w_3\)</span>和<span class="math inline">\(w_4\)</span> 即为惩罚项。 对于右图过拟合状态，此时<span class="math inline">\(w_3\)</span>和<span class="math inline">\(w_4\)</span> 权重衰减，这样就会降低高阶项对整个函数的影响，使得拟合的函数变得比较平滑。</p><p>如果有很多的特征，那如何选择惩罚项呢？</p><p>如果有非常多的特征，你可能不知道那些特征重要，以及需要惩罚的特征。通常实现正则化的方式是惩罚<strong>所有</strong>的特征。（==为什么惩罚有利特征==）</p><p>对于预测房价实例，比如有100个特征，正则化后的代价函数如下： <span class="math display">\[J(\vec{w},b)=\frac{1}{2m}\sum_{i=1}^{m}(f_{\vec{w},b}\big(\vec{x}^{(i)}\big)-y^{(i)}\big)^2+\frac{\lambda}{2m}\sum_{j=1}^{n}\omega_{j}^2\]</span> 把上式分为两部分，左边部分即为原始的代价函数，右边部分为正则化项。λ为超参数，通常会取一个较大的数。</p><p>为了最小化整个代价函数，当λ是固定的，那么就要减小<span class="math inline">\(w_1\)</span>到<span class="math inline">\(w_n\)</span>的值。加入正则项后，<span class="math inline">\(w_1\)</span>到<span class="math inline">\(w_n\)</span>均会减小，也就是使得权重衰减，这样就会降低高阶项对于整个函数的影响，使得估计函数变得比较平滑。</p><p>我们还将λ除以2m,这样这里的第一项和第二项都在2m上按比例缩放，会更改容易选择λ的值。按照惯例我们不会因为参数b太大而惩罚它，在实践中，做与不做几乎没有什么区别。</p><p>因此，总结一下这个修改后的代价函数： <img src="/img/机器学习/Snipaste_2023-06-14_09-37-48.png" srcset="/img/loading.gif" lazyload></p><p>我们想要最小化[原始代价函数即均方误差项+第二项即正则化项]</p><p>λ : 可以控制两个不同目标之间的取舍。</p><p>此函数有两个目的，目的一：最小化预测值与真实值之间的误差，更好的拟合训练集。目的二：试图减小<span class="math inline">\(w_j\)</span> ，使假设函数变得“简单”，防止过度拟合。</p><p><strong>两者相互平衡，从而达到一种相互制约的关系，最终找到一个平衡点，从而更好地拟合训练集并且具有良好的泛化能力。</strong></p><p>不同的λ值有什么影响？</p><p>使用线性回归的房价预测示例。</p><p>如果，λ等于0，那么正则项等于零，即根本没有使用正则化，会过度拟合。 <img src="/img/机器学习/Snipaste_2023-06-14_09-59-48.png" srcset="/img/loading.gif" lazyload></p><p>当λ非常非常大时，例如<span class="math inline">\(\lambda=10^{10}\)</span>,那么<span class="math inline">\(w_1到w_4\)</span>几乎等于0，只剩常数b项，此时会欠拟合。</p><p><img src="/img/机器学习/Snipaste_2023-06-14_10-00-42.png" srcset="/img/loading.gif" lazyload></p><p>线性回归的正则化方法： <img src="/img/机器学习/Snipaste_2023-06-14_10-13-49.png" srcset="/img/loading.gif" lazyload></p><p>$$\begin{aligned} & w_{j}=w_{j}-\alpha\left[\frac{1}{m}\sum_{i=1}^{m}\left[(f_{\vec{{{w}}},b}\big(\vec{x}^{(i)}\big)-y^{(i)}\big)x_{j}^{(i)}\right]+\frac{\lambda}{m}w_{j}\right] \\ &b=b-\alpha\frac{1}{m}\sum_{i=1}^m(f_{\vec{\mathbf{w}},b}(\vec{\mathbf{x}}^{(i)})-y^{(i)}) \end{aligned}$$</p><p>逻辑回归的正则化方法： <img src="/img/机器学习/Snipaste_2023-06-14_10-18-04.png" srcset="/img/loading.gif" lazyload></p><p>参考资料：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/75364861">吴恩达机器学习笔记（三）正则化</a></p></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="category-chain-item">机器学习</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">#机器学习</a></div></div><div class="license-box my-3"><div class="license-title"><div>机器学习00-监督学习</div><div>https://zhou1317fe5.github.io/2023/07/07/机器学习00-监督学习/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>Zhou1317fe5</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2023年7月7日</div></div><div class="license-meta-item"><div>许可协议</div><div><a target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2023/07/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A001-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="机器学习01-神经网络"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">机器学习01-神经网络</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2023/06/29/Java13-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E8%BF%9B%E9%98%B6-%E6%8A%BD%E8%B1%A1%E7%B1%BB&amp;%E6%8E%A5%E5%8F%A3&amp;%E5%86%85%E9%83%A8%E7%B1%BB/" title="Java13-面向对象进阶-抽象类&amp;接口&amp;内部类"><span class="hidden-mobile">Java13-面向对象进阶-抽象类&amp;接口&amp;内部类</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div><div class="statistics"><span id="busuanzi_container_site_pv" style="display:none">总访问量 <span id="busuanzi_value_site_pv"></span> 次 </span><span id="busuanzi_container_site_uv" style="display:none">总访客数 <span id="busuanzi_value_site_uv"></span> 人</span></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script>window.MathJax?(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise()):window.MathJax={tex:{inlineMath:{"[+]":[["$","$"]]}},loader:{load:["ui/lazy"]},options:{renderActions:{insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(t=>{let e=t.parentNode;"li"===e.nodeName.toLowerCase()&&e.parentNode.classList.add("has-jax")})},"",!1]}}},Fluid.events.registerRefreshCallback((function(){"MathJax"in window&&MathJax.startup.document&&"function"==typeof MathJax.startup.document.state&&(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset(),MathJax.typesetPromise())}))</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script src="/js/local-search.js"></script><script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>